---
title: "Algorithme des K-means"
author: "Olivier Malahel"
format: html
from: markdown+emoji
code-block-bg: true
code-block-border-left: "#31BAE9"
---
## Introduction {.unnumbered}

L'algorithme des K-means appelÃ©es aussi mÃ©thode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On considÃ¨re que les observations x= {ğ‘¥â‚, .., ğ‘¥â‚™} sont dÃ©crites par p variables (ğ‘¥â‚™ appartient Ã  â„áµ–) et que l'espace est munie est de la mÃ©trique (gÃ©nÃ©ralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le mÃªme poids.

## Algorithme de Lloyd (k-means) {.unnumbered}

### Algorithme des *k*-means

La mÃ©thode des centres mobiles a pour but de fournir une partition de **x= {ğ‘¥â‚, .., ğ‘¥â‚™}**.

> **Principe** : Pour lâ€™ensemble dâ€™observations ğ‘¥ = {ğ‘¥â‚, .., ğ‘¥â‚™}, on suppose quâ€™il existe  
> ğ¶á´· = {ğ¶â‚, .., ğ¶â‚–} tel que ğ‘¥ = â‹ƒâ‚–â‚Œâ‚á´· ğ¶â‚– est disjoint

* Etape 1. On commence par choisir **K** observations diffÃ©rentes : {Î¼â‚, .., Î¼â‚–}
* Etape 2. On rÃ©alise itÃ©rativement une succession ces actions: 
    - Pour chaque observation **x= {ğ‘¥â‚, .., ğ‘¥â‚™}** on trouve son centre (barycentre) le plus proche pour crÃ©er ğ¶â‚– = {ensemble dâ€™observations les plus proches du centre Î¼â‚–}
    - Dans chaque nouvelle classe ğ¶â‚– on dÃ©finit le nouveau centre de classe Î¼â‚– comme Ã©tant le barycentre de ğ¶â‚–.
* Etape 3. on rÃ©itÃ¨re l'Ã©tape 1. 

### Remarques {.unnumbered}

- Lâ€™algorithme sâ€™arrÃªte lorsque la partition des classes ne change plus (critÃ¨re de convergence)

- l'algorithme ne s'applique que sur des donnÃ©es quantitatives 

- si les barycentres sont donnÃ©s, on commence l'algorithme avec ces barycentres.

### avantage, limite et spÃ©cificitÃ© de l'algorithme 

- avantage : algorithme rapide 

- limites: besoin de spÃ©cifier le nombre de classes K

- spÃ©cificitÃ© : minimisation de l'inertie intra-classe

## Exemple 1

Voici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters)
des donnÃ©es *x={1, 2, 9, 12, 20}*  avec les barycentres *Âµâ‚=1 et Î¼â‚‚=7* 

<u>ItÃ©ration 1</u> 

Etape 1 : *Âµâ‚=1 et Î¼â‚‚=7*  sont donnÃ©s par hypothÃ¨se 

Etape 2 : CommenÃ§ons par calculer la distance de chaque Ã©lÃ©ment de x par rapport Ã  Âµâ‚=1 et Î¼â‚‚=7

```{R}
# Fonction calculant les distances Ã  un barycentre
distance_au_centre <- function(x, mu) {
  # x  : vecteur numÃ©rique
  # mu : valeur numÃ©rique du barycentre
  abs(x - mu)
}
x   <- c(1, 2, 9, 12, 20)
mu1 <- 1
mu2 <- 7
distances1 <- distance_au_centre(x, mu1)
df1<- data.frame(points = x, distance = distances1)
distances2 <- distance_au_centre(x, mu2)
df2<- data.frame(points = x, distance = distances2)
df_merged <- merge(df1, df2, by= "points", suffixes = c("_mu1", "_mu2"))
print(df_merged)
```

Etape 2 : les classes sont telles que la distance d(x,Âµâ‚) d(x,Î¼â‚‚) soit minimale. Donc les classes sont {1,2} et {9,12,20}. 

<u>ItÃ©ration 2 </u> : 

Etape 1 : on dÃ©termine les centres de chaque classe

```{R}
mu3 <- mean(c(1, 2))
mu4 <- mean(c(9, 12, 20))
print(mu3)
print(mu4)
```

Etape 2 : CommenÃ§ons par calculer la distance de chaque Ã©lÃ©ment de x par rapport Ã  Âµâ‚=1,5 et Î¼â‚‚=13,67

```{R}
# Fonction calculant les distances Ã  un barycentre
distance_au_centre <- function(x, mu) {
  # x  : vecteur numÃ©rique
  # mu : valeur numÃ©rique du barycentre
  abs(x - mu)
}
x   <- c(1, 2, 9, 12, 20)
mu3 <-1.5
mu4 <-13.67

distances1 <- distance_au_centre(x, mu3)
df1<- data.frame(points = x, distance = distances1)
distances2 <- distance_au_centre(x, mu4)
df2<- data.frame(points = x, distance = distances2)
df_merged <- merge(df1, df2, by= "points", suffixes = c("_mu3", "_mu4"))
print(df_merged)
```

Etape 2 : les classes sont telles que la distance d(x,Âµ3) d(x,Î¼4) soit minimale. Le regroupement ne change pas de la 1Ã¨re Ã  la 2Ã¨me itÃ©ration (critÃ¨re de convergence) donc les classes sont {1,2} et {9,12,20}. 

Sous R, la fonction kmeans permet directement de rÃ©aliser le regroupement selon l'algorithme des k-means.

```{R}
# donnÃ©es
k1 <- 2
x1    <- c(1, 2, 9, 12, 20)
init1 <- matrix(c(1, 7), ncol = 1)  # centres initiaux

# k-means
res1  <- kmeans(x1, centers = init1, iter.max = 10)

# regroupement des donnÃ©es par classe
classes1 <- split(x1, res1$cluster)

# affichage des classes et de leurs Ã©lÃ©ments
cat("\nÃ‰lÃ©ments par classe :\n")
for (i in seq_along(classes1)) {
  cat(sprintf("Classe %d : %s\n", i, paste(classes1[[i]], collapse = ", ")))
}
# affichage des centres finaux
cat("Centres finaux :\n")
print(res1$centers)
```

l'inertie intra classe est Ã©gale Ã  : 
```{R}
total_inertia_alt1 <- sum(res1$withinss)
print(total_inertia_alt1)
```

## Exemple 2

Voici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters)
des donnÃ©es *x={1, 2, 9, 12, 20}*  avec les barycentres *Âµ1 =1, Âµ2 =12 et Âµ3 =20* 


```{R}
# donnÃ©es
k2 <- 3
x2 <- c(1, 2, 9, 12, 20)
init2 <- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux

# k-means
res2  <- kmeans(x2, centers = init2, iter.max = 10)

# regroupement des donnÃ©es par classe
classes2 <- split(x2, res2$cluster)

# affichage des classes et de leurs Ã©lÃ©ments
cat("\nÃ‰lÃ©ments par classe :\n")
for (i in seq_along(classes2)) {
  cat(sprintf("Classe %d : %s\n", i, paste(classes2[[i]], collapse = ", ")))
}
# affichage des centres finaux
cat("Centres finaux :\n")
print(res2$centers)
```

## Inertie intra-classe 

### DÃ©finition

> Lâ€™inertie intra-classe de x = {x1,..,xn} par rapport Ã  la partition
  $$C_K = \{\,C_1,\,C_2,\,\dots,\,C_K\} $$ et les centres des classes dans Âµ = {Âµ1,..,ÂµK} sâ€™Ã©crit:
> IW =$\sum_{k=1}^K dÂ²(xi, Âµk)$
 Notons-la IW(C,Âµ) Ã  partir de maintenant.

l'inertie intra classe est Ã©gale Ã  : 
```{R}
total_inertia_alt2 <- sum(res2$withinss)
print(total_inertia_alt2)
```
### PropriÃ©tÃ©s

- Lâ€™algorithme de Lloyd permet de diminuer l'inertie intra-classe IW Ã  chaque itÃ©ration. Ainsi : 
$\mathrm{IW}(C^{(m)}, \mu^{(m)}) \ge \mathrm{IW}(C^{(m+1)}, \mu^{(m)})$

-  La suite numÃ©rique $\mathrm{IW}(C^{(m)}, \mu^{(m)})$ est stationnaire(i.e.,Ã  partir dâ€™un certain nombre dâ€™itÃ©rations,elle prend toujours la mÃªme valeur).

Voici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters)
des donnÃ©es *x={1, 2, 9, 12, 20}*  avec les barycentres *Âµ1 =1, Âµ2 =9, Âµ3 =12 et Âµ4 =20.* 


```{R}
# donnÃ©es
x3 <- c(1, 2, 9, 12, 20)
init3 <- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux

# k-means
res3 <- kmeans(x3, centers = init3, iter.max = 10)

# regroupement des donnÃ©es par classe
classes3 <- split(x3, res3$cluster)

# affichage des classes et de leurs Ã©lÃ©ments
cat("\nÃ‰lÃ©ments par classe :\n")
for (i in seq_along(classes3)) {
  cat(sprintf("Classe %d : %s\n", i, paste(classes3[[i]], collapse = ", ")))
}
# affichage des centres finaux
cat("Centres finaux :\n")
print(res3$centers)
```

l'inertie intra classe est Ã©gale Ã  : 
```{R}
total_inertia_alt3 <- sum(res3$withinss)
print(total_inertia_alt3)
```

## MÃ©thode du coude

Le critÃ¨re du coude est une mÃ©thode visuelle consistant Ã  tracer lâ€™inertie intra de k-means en fonction de k et Ã  choisir le nombre de clusters correspondant au point dâ€™inflexion de la courbe.

```{R}
# vecteur de valeurs de k Ã  tester
ks    <- 2:4

# prÃ©-allocation dâ€™un vecteur pour stocker lâ€™inertie totale
inertias <- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)

# tracÃ© de la courbe inertie vs k
plot(ks, inertias,
     type = "b",                             # points reliÃ©s par des segments
     pch  = 19,                              # symbole plein pour les points
     xlab = "Nombre de clusters (k)",
     ylab = "Inertie intra-classe totale",
     main = "MÃ©thode du coude pour le choix de k")

```
Conclusion : La mÃ©thode du coude privilÃ©gie donc 3 clusters qui permettent de capturer la plus grande partie de l'inertie intra-classe. En effet, ajouter une 4Ã¨me classe, n'apporte qu'un faible gain d'inertie intra-classe. 
 