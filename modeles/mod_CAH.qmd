---
title: "Classification Ascendante Hiérarchique (CAH)"
author: "Olivier Malahel"
format: html
from: markdown+emoji
code-block-bg: true
code-block-border-left: "#31BAE9"
---
## Introduction {.unnumbered}

La classification hiérarchique ascendante est une méthode de partitionnement dont l’objectif est de construire une hiérarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant itérativement les groupes deux à deux.  

Dans la pratique, la CAH, c’est comme si tu voulais ranger des objets (observations) qui se ressemblent dans des boîtes (clusters ou groupes ou classes)

Au début, tu mets chaque objet dans sa propre boîte. 
Ensuite, tu regardes quelles boîtes contiennent les objets les plus proches, les plus similaires, et tu les rassembles à l'aide d'une distance qu'on appelle distance entre observations (généralement la distance euclidienne)
Puis tu continues : à chaque étape, tu regroupes deux boîtes qui se ressemblent le plus, à l'aide de la distance entre groupes. 

À la fin, tu obtiens une grande pyramide de regroupements, un peu comme un arbre qu’on appelle un dendrogramme. Et tu peux choisir de couper l’arbre à un certain niveau (grace à la plus longue distance verticale) pour former des groupes.

## Principe de la méthode 

Soit **x** = {x₁, …, xₙ} l’ensemble d’observations à classer :

1. **Étape initiale**  
   Les n individus x constituent chacun une classe singleton.

2. **Fusion des plus proches**  
   On calcule la distance deux à deux entre tous les individus, puis on réunit dans la même classe les deux individus les plus proches.

3. **Itération**  
   On recalcule la distance entre la nouvelle classe formée et les n–2 individus restants.  
   On fusionne à nouveau les deux éléments (classes ou individus) les plus proches.

   Ce processus est itéré jusqu’à ce qu’il ne reste plus qu’une unique classe: x.

<u>Remarque</u> : 


**Critères d’agrégation**  
Pour pouvoir regrouper des parties de Ω :

- **Dissimilarité entre observations**  
d(xi,xj) ∈ R+, xi,xj ∈ x,

::: {.callout-important}
La distance entre observations le plus utilisée est la distance euclidienne. 

- **Dissimilarité entre classes**  
D(A,B) ∈ R+, où A,B ∈ P(x) avec A∩B =∅.

## Algorithme de CAH

**Data** : x₁, …, xₙ  
**Initialisation** : définir la partition **C**⁽⁰⁾ constituée des singletons  

**for** m = 1, …, K–1 **do**  
  - Calculer les distances deux à deux entre les classes de la partition **C**⁽ᵐ⁻¹⁾ à l’aide de **D**  
  - Former la partition **C**⁽ᵐ⁾ en regroupant les deux classes les plus proches selon **D**  
**end**  

**Result** : Hiérarchie indicée  

L’algorithme de la CAH fournit une hiérarchie indicée où D est l’indice si D
 est croissante.

## Distance (ou dissimilarité) entre les classes 

Soit A et B deux classes de Ω, on a les **critères d’agrégation (linkages)** suivants :

- **Critère du saut minimum (single linkage)**  
    $D_m$(A,B) = min{d(x,y); x ∈ A, y ∈ B}.

- **Critère du saut maximum (complete linkage)**  
    $D_M$(A,B) = max{d(x,y); x ∈ A, y ∈ B}.

- **Critère de la distance moyenne (average linkage)**  
    $D_a(A,B) = \frac{\sum_{x\in A}\sum_{y\in B} d(x,y)}{n_A\,n_B}$


    où $n_A$=card(A) et $n_B$=card(B)

- **Critère de Ward (Ward linkage)**  
    $D_W(A, B) = \frac{n_A\,n_B}{n_A + n_B}\,d^2(\mu_A, \mu_B)$


    où $µ_A=\frac{\sum_{x∈A}x}{n_A}$ et $µ_B=\frac{\sum_{y∈B}y}{n_B}$

## Dendrogramme

Un dendrogramme est un type de diagramme arborescent utilisé pour représenter visuellement le résultat d’une analyse de regroupement hiérarchique (clustering hiérarchique). 

Les branches représentent les fusions successives de groupes d’objets selon leur similarité ou leur distance.

### Règle de la plus grande distance verticale

La règle de la plus grande distance verticale (aussi appelée "règle de l'élagage maximal") est une méthode visuelle simple pour déterminer le nombre optimal de clusters à partir d’un dendrogramme.

Etapes de la règle : 

- Tracer le dendrogramme.

- Repérer les hauteurs des lignes verticales.

- Identifier la plus grande distance verticale sans ligne horizontale qui la coupe : C’est-à-dire la plus grande "barre verticale libre".

- Tracer une ligne horizontale qui coupe cette barre.

- Le nombre optimal de clusters est alors égal au nombre de branches coupées par cette ligne horizontale.

### avantages, limites et spécificité de l'algorithme CAH

- avantages : pas besoin de spécifier K à l’avance, visualisation hiérarchique.

- limites: complexité quadratique, sensible à l’échelle des variables.

- spécificité : Structure arborescente 


### Application directe
```{R}
# Étape 1 : Données originales
data <- data.frame(
  Employé    = c("E1", "E2", "E3", "E4", "E5"),
  Ancienneté = c(2, 3, 5, 6, 8),
  Salaire    = c(2000, 2100, 3500, 4100, 10000)
)

# Étape 2 : Matrice des caractéristiques
X <- data[, c("Ancienneté", "Salaire")]

# Étape 3 : Clustering hiérarchique (Ward)
d <- dist(X)
hc <- hclust(d, method = "ward.D2")

# Étape 4 : Affichage du dendrogramme avec ylim plus grand
plot(hc, labels = data$Employé,
     main = "Dendrogramme - CAH (Ward)",
     xlab = "Employé", ylab = "Distance",
     ylim = c(0, 10000))  # Limite supérieure augmentée

# Hauteur de coupure
n_clusters <- 2
height_cut <- (sort(hc$height, decreasing = TRUE)[n_clusters - 1] +
               sort(hc$height, decreasing = TRUE)[n_clusters]) / 1.35
abline(h = height_cut, col = "red", lty = 2)

# Étape 5 : Clusters
clusters <- cutree(hc, k = n_clusters)
data$Cluster <- as.factor(clusters)

# Affichage final
print(data)
```


