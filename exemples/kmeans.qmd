---
title: "Algorithme des K-means"
author: "Olivier Malahel"
format: 
  html:
    code-fold: true
---

## Exemple 

Voici un exemple complet qui prend un jeu de données simulant les notes de 30 étudiants en 4 matières d’une filière ingénieur, applique le k-means pour segmenter les profils. 

### 1. Simuler un jeu de données : notes sur 4 matières pour 30 étudiants

```{R}
set.seed(123)
n_students <- 30
notes <- data.frame(
  Etudiant = paste0("E", sprintf("%02d", 1:n_students)),
  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),
  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),
  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.4), 1),
  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)
)
rownames(notes) <- notes$Etudiant
notes$Etudiant <- NULL
notes
```

### 2. Préparation : standardisation des notes

Ici, on centre et réduit chaque variable comme la dispersion des notes de chaque matière est différente.Ainsi, les notes des matières pourront être comparables


```{R}
notes_scaled <- scale(notes)
```

### 3. Choix du nombre de clusters k via la méthode du coude

```{R}

wss <- numeric(10)
for (k in 1:10) {
  km <- kmeans(notes_scaled, centers = k, nstart = 25)
  wss[k] <- km$tot.withinss
}
plot(1:10, wss, type = "b", pch = 19,
     xlab = "Nombre de clusters k",
     ylab = "Somme des carrés intra-clusters",
     main = "Méthode du coude pour choisir k")

```

On choisit ainsi k=3 donc 3 clusters. 

### 4. Exécution du k-means pour k = 3

```{R}
k <- 3
km_res <- kmeans(notes_scaled, centers = k, nstart = 25)
print(km_res)
```

### 5. Ajouter les labels de cluster au tableau original

```{R}
notes_cluster <- cbind(notes, Cluster = factor(km_res$cluster))
```

### 6. Visualisation 2D avec PCA + clusters
 
```{R}
suppressMessages(suppressWarnings(
  capture.output(install.packages("ggplot2"), file = "/dev/null")
))
library(ggplot2)

# Calcul des composantes principales
pca <- prcomp(notes_scaled)
scores <- as.data.frame(pca$x[, 1:2])
scores$Cluster <- factor(km_res$cluster)
scores$Étudiant <- rownames(scores)

ggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = Étudiant)) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5, size = 3) +
  labs(title = "Clustering k-means des profils étudiants",
       subtitle = "Projection sur les deux premières composantes principales",
       x = "PC1", y = "PC2") +
  theme_minimal()
```

```{R}
pca$rotation[, 1:2] 
```

Le premier axe factoriel oppose profils maths/info vs physique et le deuxième axe factoriel sépare les étudiants bons en chimie (en haut) des plus faibles en chimie (en bas)

### Conclusion 

Les clusters issus de k-means se retrouvent bien séparés :

- cluster (rouge) : Ces étudiants sont bons en physique. 

- cluster (vert) : Ces étudiants sont forts en Maths et informatique

- cluster (bleu) : Ces étudiants ont des performances en chimie. 
