[
  {
    "objectID": "presentation/definition.html",
    "href": "presentation/definition.html",
    "title": "DÃ©finition",
    "section": "",
    "text": "Classification Â« supervisÃ©e Â» signifie une classification guidÃ©e par des Ã©tiquettes ou â€œsuperviseurâ€ qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients dâ€™un magasin en â€œfidÃ¨leâ€ et â€œoccasionnelâ€.\nAinsi, une classification non supervisÃ©e est une classification non guidÃ©e et oÃ¹ lâ€™algorithme dÃ©couvre lui-mÃªme la structure des donnÃ©es et dÃ©termine lui-mÃªme le regroupement, sans Â« superviseur Â». Exemple : Ã©tablir un classification des clients (ou clustering) selon les montants dÃ©pensÃ©s dans un magasin sans prÃ©ciser Ã  lâ€™algorithme quels sont les clients fidÃ¨les ou non fidÃ¨les."
  },
  {
    "objectID": "presentation/definition.html#introduction",
    "href": "presentation/definition.html#introduction",
    "title": "DÃ©finition",
    "section": "",
    "text": "Classification Â« supervisÃ©e Â» signifie une classification guidÃ©e par des Ã©tiquettes ou â€œsuperviseurâ€ qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients dâ€™un magasin en â€œfidÃ¨leâ€ et â€œoccasionnelâ€.\nAinsi, une classification non supervisÃ©e est une classification non guidÃ©e et oÃ¹ lâ€™algorithme dÃ©couvre lui-mÃªme la structure des donnÃ©es et dÃ©termine lui-mÃªme le regroupement, sans Â« superviseur Â». Exemple : Ã©tablir un classification des clients (ou clustering) selon les montants dÃ©pensÃ©s dans un magasin sans prÃ©ciser Ã  lâ€™algorithme quels sont les clients fidÃ¨les ou non fidÃ¨les."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classification non supervisÃ©e",
    "section": "",
    "text": "External link to Quarto website\nR example\nPython example ğŸ"
  },
  {
    "objectID": "index.html#des-liens-intÃ©ressants",
    "href": "index.html#des-liens-intÃ©ressants",
    "title": "Classification non supervisÃ©e",
    "section": "",
    "text": "External link to Quarto website\nR example\nPython example ğŸ"
  },
  {
    "objectID": "modeles/mod_kmeans.html",
    "href": "modeles/mod_kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Lâ€™algorithme des K-means appelÃ©es aussi mÃ©thode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On considÃ¨re que les observations x= {ğ‘¥â‚, .., ğ‘¥â‚™} sont dÃ©crites par p variables (ğ‘¥â‚™ appartient Ã  â„áµ–) et que lâ€™espace est munie est de la mÃ©trique (gÃ©nÃ©ralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le mÃªme poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#introduction",
    "href": "modeles/mod_kmeans.html#introduction",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Lâ€™algorithme des K-means appelÃ©es aussi mÃ©thode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On considÃ¨re que les observations x= {ğ‘¥â‚, .., ğ‘¥â‚™} sont dÃ©crites par p variables (ğ‘¥â‚™ appartient Ã  â„áµ–) et que lâ€™espace est munie est de la mÃ©trique (gÃ©nÃ©ralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le mÃªme poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "href": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "title": "Algorithme des K-means",
    "section": "Algorithme de Lloyd (k-means)",
    "text": "Algorithme de Lloyd (k-means)\n\nAlgorithme des k-means\nLa mÃ©thode des centres mobiles a pour but de fournir une partition de x= {ğ‘¥â‚, .., ğ‘¥â‚™}.\n\nPrincipe : Pour lâ€™ensemble dâ€™observations ğ‘¥ = {ğ‘¥â‚, .., ğ‘¥â‚™}, on suppose quâ€™il existe\nğ¶á´· = {ğ¶â‚, .., ğ¶â‚–} tel que ğ‘¥ = â‹ƒâ‚–â‚Œâ‚á´· ğ¶â‚– est disjoint\n\n\nEtape 1. On commence par choisir K observations diffÃ©rentes : {Î¼â‚, .., Î¼â‚–}\nEtape 2. On rÃ©alise itÃ©rativement une succession ces actions:\n\nPour chaque observation x= {ğ‘¥â‚, .., ğ‘¥â‚™} on trouve son centre (barycentre) le plus proche pour crÃ©er ğ¶â‚– = {ensemble dâ€™observations les plus proches du centre Î¼â‚–}\nDans chaque nouvelle classe ğ¶â‚– on dÃ©finit le nouveau centre de classe Î¼â‚– comme Ã©tant le barycentre de ğ¶â‚–.\n\nEtape 3. on rÃ©itÃ¨re lâ€™Ã©tape 1.\n\n\n\nRemarques\n\nLâ€™algorithme sâ€™arrÃªte lorsque la partition des classes ne change plus (critÃ¨re de convergence)\nlâ€™algorithme ne sâ€™applique que sur des donnÃ©es quantitatives\nsi les barycentres sont donnÃ©s, on commence lâ€™algorithme avec ces barycentres."
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-1",
    "href": "modeles/mod_kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "Exemple 1",
    "text": "Exemple 1\nVoici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters) des donnÃ©es x={1, 2, 9, 12, 20} avec les barycentres Âµâ‚=1 et Î¼â‚‚=7\nItÃ©ration 1\nEtape 1 : Âµâ‚=1 et Î¼â‚‚=7 sont donnÃ©s par hypothÃ¨se\nEtape 2 : CommenÃ§ons par calculer la distance de chaque Ã©lÃ©ment de x par rapport Ã  Âµâ‚=1 et Î¼â‚‚=7\n\n# Fonction calculant les distances Ã  un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numÃ©rique\n  # mu : valeur numÃ©rique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu1 &lt;- 1\nmu2 &lt;- 7\ndistances1 &lt;- distance_au_centre(x, mu1)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu2)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu1\", \"_mu2\"))\nprint(df_merged)\n\n  points distance_mu1 distance_mu2\n1      1            0            6\n2      2            1            5\n3      9            8            2\n4     12           11            5\n5     20           19           13\n\n\nEtape 2 : les classes sont telles que la distance d(x,Âµâ‚) d(x,Î¼â‚‚) soit minimale. Donc les classes sont {1,2} et {9,12,20}.\nItÃ©ration 2  :\nEtape 1 : on dÃ©termine les centres de chaque classe\n\nmu3 &lt;- mean(c(1, 2))\nmu4 &lt;- mean(c(9, 12, 20))\nprint(mu3)\n\n[1] 1.5\n\nprint(mu4)\n\n[1] 13.66667\n\n\nEtape 2 : CommenÃ§ons par calculer la distance de chaque Ã©lÃ©ment de x par rapport Ã  Âµâ‚=1,5 et Î¼â‚‚=13,67\n\n# Fonction calculant les distances Ã  un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numÃ©rique\n  # mu : valeur numÃ©rique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu3 &lt;-1.5\nmu4 &lt;-13.67\n\ndistances1 &lt;- distance_au_centre(x, mu3)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu4)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu3\", \"_mu4\"))\nprint(df_merged)\n\n  points distance_mu3 distance_mu4\n1      1          0.5        12.67\n2      2          0.5        11.67\n3      9          7.5         4.67\n4     12         10.5         1.67\n5     20         18.5         6.33\n\n\nEtape 2 : les classes sont telles que la distance d(x,Âµ3) d(x,Î¼4) soit minimale. Le regroupement ne change pas de la 1Ã¨re Ã  la 2Ã¨me itÃ©ration (critÃ¨re de convergence) donc les classes sont {1,2} et {9,12,20}.\nSous R, la fonction kmeans permet directement de rÃ©aliser le regroupement selon lâ€™algorithme des k-means.\n\n# donnÃ©es\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des donnÃ©es par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs Ã©lÃ©ments\ncat(\"\\nÃ‰lÃ©ments par classe :\\n\")\n\n\nÃ‰lÃ©ments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nlâ€™inertie intra classe est Ã©gale Ã  :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-2",
    "href": "modeles/mod_kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters) des donnÃ©es x={1, 2, 9, 12, 20} avec les barycentres Âµ1 =1, Âµ2 =12 et Âµ3 =20\n\n# donnÃ©es\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des donnÃ©es par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs Ã©lÃ©ments\ncat(\"\\nÃ‰lÃ©ments par classe :\\n\")\n\n\nÃ‰lÃ©ments par classe :\n\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res2$centers)\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "modeles/mod_kmeans.html#inertie-intra-classe",
    "href": "modeles/mod_kmeans.html#inertie-intra-classe",
    "title": "Algorithme des K-means",
    "section": "Inertie intra-classe",
    "text": "Inertie intra-classe\n\nDÃ©finition\n\nLâ€™inertie intra-classe de x = {x1,..,xn} par rapport Ã  la partition \\[C_K = \\{\\,C_1,\\,C_2,\\,\\dots,\\,C_K\\} \\] et les centres des classes dans Âµ = {Âµ1,..,ÂµK} sâ€™Ã©crit: IW =\\(\\sum_{k=1}^K dÂ²(xi, Âµk)\\) Notons-la IW(C,Âµ) Ã  partir de maintenant.\n\nlâ€™inertie intra classe est Ã©gale Ã  :\n\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n[1] 5\n\n\n\n\nPropriÃ©tÃ©s\n\nLâ€™algorithme de Lloyd permet de diminuer lâ€™inertie intra-classe IW Ã  chaque itÃ©ration. Ainsi : \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)}) \\ge \\mathrm{IW}(C^{(m+1)}, \\mu^{(m)})\\)\nLa suite numÃ©rique \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)})\\) est stationnaire(i.e.,Ã  partir dâ€™un certain nombre dâ€™itÃ©rations,elle prend toujours la mÃªme valeur).\n\nVoici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters) des donnÃ©es x={1, 2, 9, 12, 20} avec les barycentres Âµ1 =1, Âµ2 =9, Âµ3 =12 et Âµ4 =20.\n\n# donnÃ©es\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des donnÃ©es par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs Ã©lÃ©ments\ncat(\"\\nÃ‰lÃ©ments par classe :\\n\")\n\n\nÃ‰lÃ©ments par classe :\n\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res3$centers)\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nlâ€™inertie intra classe est Ã©gale Ã  :\n\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n[1] 0.5"
  },
  {
    "objectID": "modeles/mod_kmeans.html#mÃ©thode-du-coude",
    "href": "modeles/mod_kmeans.html#mÃ©thode-du-coude",
    "title": "Algorithme des K-means",
    "section": "MÃ©thode du coude",
    "text": "MÃ©thode du coude\nLe critÃ¨re du coude est une mÃ©thode visuelle consistant Ã  tracer lâ€™inertie intra de k-means en fonction de k et Ã  choisir le nombre de clusters correspondant au point dâ€™inflexion de la courbe.\n\n# vecteur de valeurs de k Ã  tester\nks    &lt;- 2:4\n\n# prÃ©-allocation dâ€™un vecteur pour stocker lâ€™inertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# tracÃ© de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reliÃ©s par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"MÃ©thode du coude pour le choix de k\")\n\n\n\n\n\n\n\n\nConclusion : La mÃ©thode du coude privilÃ©gie donc 3 clusters qui permettent de capturer la plus grande partie de lâ€™inertie intra-classe. En effet, ajouter une 4Ã¨me classe, nâ€™apporte quâ€™un faible gain dâ€™inertie intra-classe."
  },
  {
    "objectID": "exemples/kmeans.html",
    "href": "exemples/kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters) des donnÃ©es x={1, 2, 9, 12, 20} avec les barycentres Âµâ‚=1 et Î¼â‚‚=7\n\n# donnÃ©es\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des donnÃ©es par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs Ã©lÃ©ments\ncat(\"\\nÃ‰lÃ©ments par classe :\\n\")\n\n\nÃ‰lÃ©ments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nlâ€™inertie intra classe est Ã©gale Ã  :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "exemples/kmeans.html#exemple-1",
    "href": "exemples/kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters) des donnÃ©es x={1, 2, 9, 12, 20} avec les barycentres Âµâ‚=1 et Î¼â‚‚=7\n\n# donnÃ©es\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des donnÃ©es par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs Ã©lÃ©ments\ncat(\"\\nÃ‰lÃ©ments par classe :\\n\")\n\n\nÃ‰lÃ©ments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nlâ€™inertie intra classe est Ã©gale Ã  :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "exemples/kmeans.html#exemple-2",
    "href": "exemples/kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters) des donnÃ©es x={1, 2, 9, 12, 20} avec les barycentres Âµ1 =1, Âµ2 =12 et Âµ3 =20\n\n# donnÃ©es\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des donnÃ©es par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs Ã©lÃ©ments\ncat(\"\\nÃ‰lÃ©ments par classe :\\n\")\n\n\nÃ‰lÃ©ments par classe :\n\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res2$centers)\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "exemples/kmeans.html#propriÃ©tÃ©s",
    "href": "exemples/kmeans.html#propriÃ©tÃ©s",
    "title": "Algorithme des K-means",
    "section": "PropriÃ©tÃ©s",
    "text": "PropriÃ©tÃ©s\n\nInertie intra-classe\n\nLâ€™inertie intra-classe de x = {x1,..,xn} par rapport Ã  la partition C ={C1,..,CK} et les centres des classes dans Âµ = {Âµ1,..,ÂµK} sâ€™Ã©crit: IW = d2(xi, Âµk) k=1xiâˆˆCk Notons-la IW(C,Âµ) Ã  partir de maintenant.\n\nNotons-la IW(C,Âµ) Ã  partir de maintenant.\nlâ€™inertie intra classe est Ã©gale Ã  :\n\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n[1] 5"
  },
  {
    "objectID": "exemples/kmeans.html#exemple-3",
    "href": "exemples/kmeans.html#exemple-3",
    "title": "Algorithme des K-means",
    "section": "Exemple 3",
    "text": "Exemple 3\nVoici un exemple de script R utilisant lâ€™algorithme k-means pour trouver les classes (les clusters) des donnÃ©es x={1, 2, 9, 12, 20} avec les barycentres Âµ1 =1, Âµ2 =9, Âµ3 =12 et Âµ4 =20.\n\n# donnÃ©es\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des donnÃ©es par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs Ã©lÃ©ments\ncat(\"\\nÃ‰lÃ©ments par classe :\\n\")\n\n\nÃ‰lÃ©ments par classe :\n\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res3$centers)\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nlâ€™inertie intra classe est Ã©gale Ã  :\n\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n[1] 0.5"
  },
  {
    "objectID": "exemples/kmeans.html#mÃ©thode-du-coude",
    "href": "exemples/kmeans.html#mÃ©thode-du-coude",
    "title": "Algorithme des K-means",
    "section": "MÃ©thode du coude",
    "text": "MÃ©thode du coude\nLa mÃ©thode du coude sert Ã  dÃ©terminer le nombre optimal de clusters Ã  retenir. PrÃ©cisÃ©ment, la mÃ©thode du â€œcoudeâ€, dÃ©termine le â€œkâ€ Ã  partir duquel lâ€™inertie cesse de beaucoup diminuer significativement quand on augmente k.\n\n# vecteur de valeurs de k Ã  tester\nks    &lt;- 2:4\n\n# prÃ©-allocation dâ€™un vecteur pour stocker lâ€™inertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# tracÃ© de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reliÃ©s par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"MÃ©thode du coude pour le choix de k\")"
  },
  {
    "objectID": "presentation/historique.html",
    "href": "presentation/historique.html",
    "title": "DÃ©finition",
    "section": "",
    "text": "Voici un bref historique des grandes Ã©tapes de la classification non supervisÃ©e :\n\nAnnÃ©es 1930 â€“ Naissance de la classification hiÃ©rarchique John Tukey et dâ€™autres statisticiens proposent des mÃ©thodes de regroupement basÃ©es sur la similaritÃ© (dendrogrammes) pour organiser les donnÃ©es sans labels.\n1965 â€“ Algorithme des nuÃ©es dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisÃ© par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s â€“ ModÃ¨les Ã  mÃ©langes et lâ€™algorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) gÃ©nÃ©ralisent les modÃ¨les de mÃ©lange gaussien, entraÃ®nÃ©s via lâ€™Expectationâ€“Maximization, pour estimer des sous-populations au sein dâ€™un jeu de donnÃ©es.\nAnnÃ©es 1980 â€“ Cartes auto-organisatrices (SOM) Teuvo Kohonen dÃ©veloppe les Self-Organizing Maps, rÃ©seaux de neurones non supervisÃ©s qui projettent des donnÃ©es de haute dimension en cartes topologiques 2D.\nAnnÃ©es 1990 â€“ Clustering spectral et plus de flexibilitÃ© Des mÃ©thodes comme le clustering spectral (utilisant les valeurs propres du graphe de similaritÃ©) permettent de dÃ©tecter des structures non sphÃ©riques. ParallÃ¨lement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnÃ©es 2000 â€“ Big Data et densitÃ© Lâ€™algorithme DBSCAN (1996) gagne en popularitÃ© ; en 2002, Ester et al.Â le prÃ©cisent pour dÃ©tecter les zones de forte densitÃ© dans de grands volumes de donnÃ©es.\nAnnÃ©es 2010 â€“ Deep clustering Lâ€™Ã©mergence du deep learning entraÃ®ne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanÃ©ment des reprÃ©sentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette Ã©volution illustre comment la classification non supervisÃ©e est passÃ©e de mÃ©thodes statistiques simples (hiÃ©rarchique, k-means) Ã  des approches sophistiquÃ©es mÃªlant reprÃ©sentations profondes et techniques de graphe."
  },
  {
    "objectID": "presentation/historique.html#dates-clÃ©s",
    "href": "presentation/historique.html#dates-clÃ©s",
    "title": "DÃ©finition",
    "section": "",
    "text": "Voici un bref historique des grandes Ã©tapes de la classification non supervisÃ©e :\n\nAnnÃ©es 1930 â€“ Naissance de la classification hiÃ©rarchique John Tukey et dâ€™autres statisticiens proposent des mÃ©thodes de regroupement basÃ©es sur la similaritÃ© (dendrogrammes) pour organiser les donnÃ©es sans labels.\n1965 â€“ Algorithme des nuÃ©es dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisÃ© par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s â€“ ModÃ¨les Ã  mÃ©langes et lâ€™algorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) gÃ©nÃ©ralisent les modÃ¨les de mÃ©lange gaussien, entraÃ®nÃ©s via lâ€™Expectationâ€“Maximization, pour estimer des sous-populations au sein dâ€™un jeu de donnÃ©es.\nAnnÃ©es 1980 â€“ Cartes auto-organisatrices (SOM) Teuvo Kohonen dÃ©veloppe les Self-Organizing Maps, rÃ©seaux de neurones non supervisÃ©s qui projettent des donnÃ©es de haute dimension en cartes topologiques 2D.\nAnnÃ©es 1990 â€“ Clustering spectral et plus de flexibilitÃ© Des mÃ©thodes comme le clustering spectral (utilisant les valeurs propres du graphe de similaritÃ©) permettent de dÃ©tecter des structures non sphÃ©riques. ParallÃ¨lement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnÃ©es 2000 â€“ Big Data et densitÃ© Lâ€™algorithme DBSCAN (1996) gagne en popularitÃ© ; en 2002, Ester et al.Â le prÃ©cisent pour dÃ©tecter les zones de forte densitÃ© dans de grands volumes de donnÃ©es.\nAnnÃ©es 2010 â€“ Deep clustering Lâ€™Ã©mergence du deep learning entraÃ®ne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanÃ©ment des reprÃ©sentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette Ã©volution illustre comment la classification non supervisÃ©e est passÃ©e de mÃ©thodes statistiques simples (hiÃ©rarchique, k-means) Ã  des approches sophistiquÃ©es mÃªlant reprÃ©sentations profondes et techniques de graphe."
  }
]