[
  {
    "objectID": "presentation/definition.html",
    "href": "presentation/definition.html",
    "title": "D√©finition",
    "section": "",
    "text": "Classification ¬´ supervis√©e ¬ª signifie une classification guid√©e par des √©tiquettes (ou ‚Äúsuperviseur‚Äù) qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d‚Äôun magasin en clients ‚Äúfid√®les‚Äù ou ‚Äúoccasionnels‚Äù.\nAinsi, une classification non supervis√©e est une classification non guid√©e et o√π l‚Äôalgorithme d√©couvre lui-m√™me la structure des donn√©es et d√©termine lui-m√™me le regroupement, sans utiliser de ‚Äúsuperviseur‚Äù. Exemple : √©tablir une classification des clients (ou clustering) selon les montants d√©pens√©s dans un magasin sans pr√©ciser √† l‚Äôalgorithme si les clients fid√®les ou occasionnels."
  },
  {
    "objectID": "presentation/definition.html#introduction",
    "href": "presentation/definition.html#introduction",
    "title": "D√©finition",
    "section": "",
    "text": "Classification ¬´ supervis√©e ¬ª signifie une classification guid√©e par des √©tiquettes (ou ‚Äúsuperviseur‚Äù) qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d‚Äôun magasin en clients ‚Äúfid√®les‚Äù ou ‚Äúoccasionnels‚Äù.\nAinsi, une classification non supervis√©e est une classification non guid√©e et o√π l‚Äôalgorithme d√©couvre lui-m√™me la structure des donn√©es et d√©termine lui-m√™me le regroupement, sans utiliser de ‚Äúsuperviseur‚Äù. Exemple : √©tablir une classification des clients (ou clustering) selon les montants d√©pens√©s dans un magasin sans pr√©ciser √† l‚Äôalgorithme si les clients fid√®les ou occasionnels."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classification non supervis√©e",
    "section": "",
    "text": "Clustering K-Means dans R\nClassification ascendante hi√©rarchique avec R"
  },
  {
    "objectID": "index.html#des-liens-int√©ressants",
    "href": "index.html#des-liens-int√©ressants",
    "title": "Classification non supervis√©e",
    "section": "",
    "text": "Clustering K-Means dans R\nClassification ascendante hi√©rarchique avec R"
  },
  {
    "objectID": "exemples/melange.html",
    "href": "exemples/melange.html",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "",
    "text": "Dans cet exemple, nous ajustons un mod√®le de m√©lange de lois normales √† des donn√©es simul√©es, en utilisant le package R mclust. Le but est d‚Äôidentifier automatiquement des groupes (ou classes) au sein des donn√©es."
  },
  {
    "objectID": "exemples/melange.html#introduction",
    "href": "exemples/melange.html#introduction",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "",
    "text": "Dans cet exemple, nous ajustons un mod√®le de m√©lange de lois normales √† des donn√©es simul√©es, en utilisant le package R mclust. Le but est d‚Äôidentifier automatiquement des groupes (ou classes) au sein des donn√©es."
  },
  {
    "objectID": "exemples/melange.html#pr√©ambule-simulation-des-donn√©es",
    "href": "exemples/melange.html#pr√©ambule-simulation-des-donn√©es",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "Pr√©ambule : Simulation des donn√©es",
    "text": "Pr√©ambule : Simulation des donn√©es\n\n# chargement des packages\ninstall.packages(\"mclust\") \n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nset.seed(123)\n\n# Deux populations gaussiennes\nx1 &lt;- rnorm(100, mean = 0, sd = 1)\nx2 &lt;- rnorm(100, mean = 5, sd = 1.5)\n\n# Donn√©es m√©lang√©es\nx &lt;- c(x1, x2)"
  },
  {
    "objectID": "exemples/melange.html#fonction-de-densit√©-du-m√©lange",
    "href": "exemples/melange.html#fonction-de-densit√©-du-m√©lange",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "1. Fonction de densit√© du m√©lange",
    "text": "1. Fonction de densit√© du m√©lange\n\n# Densit√© d‚Äôun m√©lange √† 2 composantes\ndensite_melange &lt;- function(x, pi, mu, sigma) {\n  pi[1] * dnorm(x, mean = mu[1], sd = sigma[1]) +\n  pi[2] * dnorm(x, mean = mu[2], sd = sigma[2])\n}\n\n# Exemple de trac√© avec pi[1]=0.5 et pi[2]=0.5\ncurve(densite_melange(x, pi = c(0.5, 0.5), mu = c(0, 5), sigma = c(1, 1.5)),\n      from = -5, to = 10, col = \"red\", lwd = 2, ylab = \"Densit√©\", main = \"Densit√© du m√©lange\")"
  },
  {
    "objectID": "exemples/melange.html#donn√©es-incompl√®tes-vs-compl√®tes",
    "href": "exemples/melange.html#donn√©es-incompl√®tes-vs-compl√®tes",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "2. Donn√©es incompl√®tes vs compl√®tes",
    "text": "2. Donn√©es incompl√®tes vs compl√®tes\n\n# Construction de Z complet (connu car simulation)\nz &lt;- c(rep(1, 100), rep(2, 100))\n\n# Log-vraisemblance compl√®te\nlog_vraisemblance_completee &lt;- function(x, z, pi, mu, sigma) {\n  sum(log(pi[z]) + dnorm(x, mean = mu[z], sd = sigma[z], log = TRUE))\n}\n\n# Exemple : vrais param√®tres\nlog_vraisemblance_completee(x, z, pi = c(0.5, 0.5), mu = c(0, 5), sigma = c(1, 1.5))\n\n[1] -451.4813"
  },
  {
    "objectID": "exemples/melange.html#estimation-par-em-via-mclust",
    "href": "exemples/melange.html#estimation-par-em-via-mclust",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "3. Estimation par EM via mclust",
    "text": "3. Estimation par EM via mclust\n\n# Estimation avec mclust\nres &lt;- Mclust(x, G = 2)  # G = nb de composantes\n\n# R√©sum√© des param√®tres\nsummary(res)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust V (univariate, unequal variance) model with 2 components: \n\n log-likelihood   n df      BIC       ICL\n      -441.9182 200  5 -910.328 -923.4324\n\nClustering table:\n  1   2 \n 96 104"
  },
  {
    "objectID": "exemples/melange.html#affichage-des-r√©sultats",
    "href": "exemples/melange.html#affichage-des-r√©sultats",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "4. Affichage des r√©sultats",
    "text": "4. Affichage des r√©sultats\n\nplot(res, what = \"density\", main = \"Estimation du mod√®le de m√©lange par EM\")"
  },
  {
    "objectID": "exemples/melange.html#affichage-des-param√®tres",
    "href": "exemples/melange.html#affichage-des-param√®tres",
    "title": "Mod√®le de M√©lange Gaussien",
    "section": "affichage des param√®tres",
    "text": "affichage des param√®tres\n\nprint(res$parameters)\n\n$pro\n[1] 0.4671275 0.5328725\n\n$mean\n           1            2 \n-0.005518509  4.629851983 \n\n$variance\n$variance$modelName\n[1] \"V\"\n\n$variance$d\n[1] 1\n\n$variance$G\n[1] 2\n\n$variance$sigmasq\n[1] 0.7145053 2.6426426\n\n$variance$scale\n[1] 0.7145053 2.6426426"
  },
  {
    "objectID": "exemples/kmeans_observable.html",
    "href": "exemples/kmeans_observable.html",
    "title": "K-means + ObservableJS",
    "section": "",
    "text": "install.packages(\"datasets\")\n\nInstalling package into '/usr/local/lib/R/site-library'\n(as 'lib' is unspecified)\n\n\nWarning: package 'datasets' is a base package, and should not be updated\n\nlibrary(datasets)\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nset.seed(123)\nkm &lt;- kmeans(iris[, 1:4], centers = 3)\niris$Cluster &lt;- as.character(km$cluster)\njsonlite::write_json(iris, \"iris_clusters.json\", pretty = TRUE, auto_unbox = TRUE)"
  },
  {
    "objectID": "exemples/kmeans_observable.html#k-means-sur-les-donn√©es-iris-r",
    "href": "exemples/kmeans_observable.html#k-means-sur-les-donn√©es-iris-r",
    "title": "K-means + ObservableJS",
    "section": "",
    "text": "install.packages(\"datasets\")\n\nInstalling package into '/usr/local/lib/R/site-library'\n(as 'lib' is unspecified)\n\n\nWarning: package 'datasets' is a base package, and should not be updated\n\nlibrary(datasets)\n\ndata(iris)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nset.seed(123)\nkm &lt;- kmeans(iris[, 1:4], centers = 3)\niris$Cluster &lt;- as.character(km$cluster)\njsonlite::write_json(iris, \"iris_clusters.json\", pretty = TRUE, auto_unbox = TRUE)"
  },
  {
    "objectID": "exemples/kmeans_observable.html#visualisation-interactive-avec-observable-js",
    "href": "exemples/kmeans_observable.html#visualisation-interactive-avec-observable-js",
    "title": "K-means + ObservableJS",
    "section": "üåê Visualisation interactive avec Observable JS",
    "text": "üåê Visualisation interactive avec Observable JS\n\nimport { FileAttachment } from \"@observablehq/stdlib\"\n\n// Force la d√©pendance sur le bloc R\n//await Promises.delay(4000);  \niris_data = FileAttachment(\"iris_clusters.json\").json()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marks: [\n    Plot.dot(iris_data, {\n      x: d =&gt; d[\"Sepal.Length\"],\n      y: d =&gt; d[\"Petal.Length\"],\n      fill: d =&gt; d.Cluster,\n      title: d =&gt; d.Species\n    })\n  ],\n  color: { legend: true },\n  x: { label: \"Longueur S√©pale\" },\n  y: { label: \"Longueur P√©tale\" },\n  width: 700,\n  height: 450\n})"
  },
  {
    "objectID": "modeles/mod_CAH.html",
    "href": "modeles/mod_CAH.html",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "",
    "text": "La classification hi√©rarchique ascendante est une m√©thode de partitionnement dont l‚Äôobjectif est de construire une hi√©rarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant it√©rativement les groupes deux √† deux."
  },
  {
    "objectID": "modeles/mod_CAH.html#introduction",
    "href": "modeles/mod_CAH.html#introduction",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "",
    "text": "La classification hi√©rarchique ascendante est une m√©thode de partitionnement dont l‚Äôobjectif est de construire une hi√©rarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant it√©rativement les groupes deux √† deux."
  },
  {
    "objectID": "modeles/mod_CAH.html#principe-de-la-m√©thode",
    "href": "modeles/mod_CAH.html#principe-de-la-m√©thode",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Principe de la m√©thode",
    "text": "Principe de la m√©thode\nSoit x = {x‚ÇÅ, ‚Ä¶, x‚Çô} l‚Äôensemble d‚Äôobservations √† classer :\n\n√âtape initiale\nLes n individus x constituent chacun une classe singleton.\nFusion des plus proches\nOn calcule la distance deux √† deux entre tous les individus, puis on r√©unit dans la m√™me classe les deux individus les plus proches.\nIt√©ration\nOn recalcule la distance entre la nouvelle classe form√©e et les n‚Äì2 individus restants.\nOn fusionne √† nouveau les deux √©l√©ments (classes ou individus) les plus proches.\nCe processus est it√©r√© jusqu‚Äô√† ce qu‚Äôil ne reste plus qu‚Äôune unique classe: x.\n\nRemarque :\nCrit√®res d‚Äôagr√©gation\nPour pouvoir regrouper des parties de ‚Ñ¶ :\n\nDissimilarit√© entre observations\nd(xi,xj) ‚àà R+, xi,xj ‚àà x,\nDissimilarit√© entre classes\nD(A,B) ‚àà R+, o√π A,B ‚àà P(x) avec A‚à©B =‚àÖ."
  },
  {
    "objectID": "modeles/mod_CAH.html#algorithme-de-cah",
    "href": "modeles/mod_CAH.html#algorithme-de-cah",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Algorithme de CAH",
    "text": "Algorithme de CAH\nData : x‚ÇÅ, ‚Ä¶, x‚Çô\nInitialisation : d√©finir la partition C‚ÅΩ‚Å∞‚Åæ constitu√©e des singletons\nfor m = 1, ‚Ä¶, K‚Äì1 do\n- Calculer les distances deux √† deux entre les classes de la partition C‚ÅΩ·µê‚Åª¬π‚Åæ √† l‚Äôaide de D\n- Former la partition C‚ÅΩ·µê‚Åæ en regroupant les deux classes les plus proches selon D\nend\nResult : Hi√©rarchie indic√©e\nL‚Äôalgorithme de la CAH fournit une hi√©rarchie indic√©e o√π D est l‚Äôindice si D est croissante."
  },
  {
    "objectID": "modeles/mod_CAH.html#distance-ou-dissimilarit√©-entre-les-classes",
    "href": "modeles/mod_CAH.html#distance-ou-dissimilarit√©-entre-les-classes",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Distance (ou dissimilarit√©) entre les classes",
    "text": "Distance (ou dissimilarit√©) entre les classes\nSoit A et B deux classes de ‚Ñ¶, on a les crit√®res d‚Äôagr√©gation (linkages) suivants :\n\nCrit√®re du saut minimum (single linkage)\n\\(D_m\\)(A,B) = min{d(x,y); x ‚àà A, y ‚àà B}.\nCrit√®re du saut maximum (complete linkage)\n\\(D_M\\)(A,B) = max{d(x,y); x ‚àà A, y ‚àà B}.\nCrit√®re de la distance moyenne (average linkage)\n\\(D_a(A,B) = \\frac{\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)}{n_A\\,n_B}\\)\no√π \\(n_A\\)=card(A) et \\(n_B\\)=card(B)\nCrit√®re de Ward (Ward linkage)\n\\(D_W(A, B) = \\frac{n_A\\,n_B}{n_A + n_B}\\,d^2(\\mu_A, \\mu_B)\\)\no√π \\(¬µ_A=\\frac{\\sum_{x‚ààA}x}{n_A}\\) et \\(¬µ_B=\\frac{\\sum_{y‚ààB}y}{n_B}\\)"
  },
  {
    "objectID": "modeles/mod_CAH.html#dendrogramme",
    "href": "modeles/mod_CAH.html#dendrogramme",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Dendrogramme",
    "text": "Dendrogramme\nUn dendrogramme est un type de diagramme arborescent utilis√© pour repr√©senter visuellement le r√©sultat d‚Äôune analyse de regroupement hi√©rarchique (clustering hi√©rarchique).\nLes branches repr√©sentent les fusions successives de groupes d‚Äôobjets selon leur similarit√© ou leur distance.\n\nR√®gle de la plus grande distance verticale\nLa r√®gle de la plus grande distance verticale (aussi appel√©e ‚Äúr√®gle de l‚Äô√©lagage maximal‚Äù) est une m√©thode visuelle simple pour d√©terminer le nombre optimal de clusters √† partir d‚Äôun dendrogramme.\nEtapes de la r√®gle :\n\nTracer le dendrogramme.\nRep√©rer les hauteurs des lignes verticales.\nIdentifier la plus grande distance verticale sans ligne horizontale qui la coupe : C‚Äôest-√†-dire la plus grande ‚Äúbarre verticale libre‚Äù.\nTracer une ligne horizontale qui coupe cette barre.\nLe nombre optimal de clusters est alors √©gal au nombre de branches coup√©es par cette ligne horizontale.\n\n\n\navantages, limites et sp√©cificit√© de l‚Äôalgorithme CAH\n\navantages : pas besoin de sp√©cifier K √† l‚Äôavance, visualisation hi√©rarchique.\nlimites: complexit√© quadratique, sensible √† l‚Äô√©chelle des variables.\nsp√©cificit√© : Structure arborescente\n\n\n\nApplication directe\n\n# √âtape 1 : Donn√©es originales\ndata &lt;- data.frame(\n  Employ√©    = c(\"E1\", \"E2\", \"E3\", \"E4\", \"E5\"),\n  Anciennet√© = c(2, 3, 5, 6, 8),\n  Salaire    = c(2000, 2100, 3500, 4100, 10000)\n)\n\n# √âtape 2 : Matrice des caract√©ristiques\nX &lt;- data[, c(\"Anciennet√©\", \"Salaire\")]\n\n# √âtape 3 : Clustering hi√©rarchique (Ward)\nd &lt;- dist(X)\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n# √âtape 4 : Affichage du dendrogramme avec ylim plus grand\nplot(hc, labels = data$Employ√©,\n     main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Employ√©\", ylab = \"Distance\",\n     ylim = c(0, 10000))  # Limite sup√©rieure augment√©e\n\n# Hauteur de coupure\nn_clusters &lt;- 2\nheight_cut &lt;- (sort(hc$height, decreasing = TRUE)[n_clusters - 1] +\n               sort(hc$height, decreasing = TRUE)[n_clusters]) / 1.35\nabline(h = height_cut, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n# √âtape 5 : Clusters\nclusters &lt;- cutree(hc, k = n_clusters)\ndata$Cluster &lt;- as.factor(clusters)\n\n# Affichage final\nprint(data)\n\n  Employ√© Anciennet√© Salaire Cluster\n1      E1          2    2000       1\n2      E2          3    2100       1\n3      E3          5    3500       1\n4      E4          6    4100       1\n5      E5          8   10000       2"
  },
  {
    "objectID": "modeles/mod_kmeans.html",
    "href": "modeles/mod_kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L‚Äôalgorithme des K-means appel√©es aussi m√©thode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On consid√®re que les observations x= {ùë•‚ÇÅ, .., ùë•‚Çô} sont d√©crites par p variables (ùë•‚Çô appartient √† ‚Ñù·µñ) et que l‚Äôespace est munie est de la m√©trique (g√©n√©ralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le m√™me poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#introduction",
    "href": "modeles/mod_kmeans.html#introduction",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L‚Äôalgorithme des K-means appel√©es aussi m√©thode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On consid√®re que les observations x= {ùë•‚ÇÅ, .., ùë•‚Çô} sont d√©crites par p variables (ùë•‚Çô appartient √† ‚Ñù·µñ) et que l‚Äôespace est munie est de la m√©trique (g√©n√©ralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le m√™me poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "href": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "title": "Algorithme des K-means",
    "section": "Algorithme de Lloyd (k-means)",
    "text": "Algorithme de Lloyd (k-means)\n\nAlgorithme des k-means\nLa m√©thode des centres mobiles a pour but de fournir une partition de x= {ùë•‚ÇÅ, .., ùë•‚Çô}.\n\nPrincipe : Pour l‚Äôensemble d‚Äôobservations ùë• = {ùë•‚ÇÅ, .., ùë•‚Çô}, on suppose qu‚Äôil existe\nùê∂·¥∑ = {ùê∂‚ÇÅ, .., ùê∂‚Çñ} tel que ùë• = ‚ãÉ‚Çñ‚Çå‚ÇÅ·¥∑ ùê∂‚Çñ est disjoint\n\n\nEtape 1. On commence par choisir K observations diff√©rentes : {Œº‚ÇÅ, .., Œº‚Çñ}\nEtape 2. On r√©alise it√©rativement une succession ces actions:\n\nPour chaque observation x= {ùë•‚ÇÅ, .., ùë•‚Çô} on trouve son centre (barycentre) le plus proche pour cr√©er ùê∂‚Çñ = {ensemble d‚Äôobservations les plus proches du centre Œº‚Çñ}\nDans chaque nouvelle classe ùê∂‚Çñ on d√©finit le nouveau centre de classe Œº‚Çñ comme √©tant le barycentre de ùê∂‚Çñ.\n\nEtape 3. on r√©it√®re l‚Äô√©tape 1.\n\n\n\nRemarques\n\nL‚Äôalgorithme s‚Äôarr√™te lorsque la partition des classes ne change plus (crit√®re de convergence)\nl‚Äôalgorithme ne s‚Äôapplique que sur des donn√©es quantitatives\nsi les barycentres sont donn√©s, on commence l‚Äôalgorithme avec ces barycentres.\n\n\n\navantage, limite et sp√©cificit√© de l‚Äôalgorithme\n\navantage : algorithme rapide\nlimites: besoin de sp√©cifier le nombre de classes K\nsp√©cificit√© : minimisation de l‚Äôinertie intra-classe"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-1",
    "href": "modeles/mod_kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "Exemple 1",
    "text": "Exemple 1\nVoici un exemple de script R utilisant l‚Äôalgorithme k-means pour trouver les classes (les clusters) des donn√©es x={1, 2, 9, 12, 20} avec les barycentres ¬µ‚ÇÅ=1 et Œº‚ÇÇ=7\nIt√©ration 1\nEtape 1 : ¬µ‚ÇÅ=1 et Œº‚ÇÇ=7 sont donn√©s par hypoth√®se\nEtape 2 : Commen√ßons par calculer la distance de chaque √©l√©ment de x par rapport √† ¬µ‚ÇÅ=1 et Œº‚ÇÇ=7\n\n# Fonction calculant les distances √† un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur num√©rique\n  # mu : valeur num√©rique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu1 &lt;- 1\nmu2 &lt;- 7\ndistances1 &lt;- distance_au_centre(x, mu1)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu2)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu1\", \"_mu2\"))\nprint(df_merged)\n\n  points distance_mu1 distance_mu2\n1      1            0            6\n2      2            1            5\n3      9            8            2\n4     12           11            5\n5     20           19           13\n\n\nEtape 2 : les classes sont telles que la distance d(x,¬µ‚ÇÅ) d(x,Œº‚ÇÇ) soit minimale. Donc les classes sont {1,2} et {9,12,20}.\nIt√©ration 2  :\nEtape 1 : on d√©termine les centres de chaque classe\n\nmu3 &lt;- mean(c(1, 2))\nmu4 &lt;- mean(c(9, 12, 20))\nprint(mu3)\n\n[1] 1.5\n\nprint(mu4)\n\n[1] 13.66667\n\n\nEtape 2 : Commen√ßons par calculer la distance de chaque √©l√©ment de x par rapport √† ¬µ‚ÇÅ=1,5 et Œº‚ÇÇ=13,67\n\n# Fonction calculant les distances √† un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur num√©rique\n  # mu : valeur num√©rique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu3 &lt;-1.5\nmu4 &lt;-13.67\n\ndistances1 &lt;- distance_au_centre(x, mu3)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu4)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu3\", \"_mu4\"))\nprint(df_merged)\n\n  points distance_mu3 distance_mu4\n1      1          0.5        12.67\n2      2          0.5        11.67\n3      9          7.5         4.67\n4     12         10.5         1.67\n5     20         18.5         6.33\n\n\nEtape 2 : les classes sont telles que la distance d(x,¬µ3) d(x,Œº4) soit minimale. Le regroupement ne change pas de la 1√®re √† la 2√®me it√©ration (crit√®re de convergence) donc les classes sont {1,2} et {9,12,20}.\nSous R, la fonction kmeans permet directement de r√©aliser le regroupement selon l‚Äôalgorithme des k-means.\n\n# donn√©es\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des donn√©es par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs √©l√©ments\ncat(\"\\n√âl√©ments par classe :\\n\")\n\n\n√âl√©ments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nl‚Äôinertie intra classe est √©gale √† :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-2",
    "href": "modeles/mod_kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant l‚Äôalgorithme k-means pour trouver les classes (les clusters) des donn√©es x={1, 2, 9, 12, 20} avec les barycentres ¬µ1 =1, ¬µ2 =12 et ¬µ3 =20\n\n# donn√©es\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des donn√©es par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs √©l√©ments\ncat(\"\\n√âl√©ments par classe :\\n\")\n\n\n√âl√©ments par classe :\n\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res2$centers)\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "modeles/mod_kmeans.html#inertie-intra-classe",
    "href": "modeles/mod_kmeans.html#inertie-intra-classe",
    "title": "Algorithme des K-means",
    "section": "Inertie intra-classe",
    "text": "Inertie intra-classe\n\nD√©finition\n\nL‚Äôinertie intra-classe de x = {x1,..,xn} par rapport √† la partition \\[C_K = \\{\\,C_1,\\,C_2,\\,\\dots,\\,C_K\\} \\] et les centres des classes dans ¬µ = {¬µ1,..,¬µK} s‚Äô√©crit: IW =\\(\\sum_{k=1}^K d¬≤(xi, ¬µk)\\) Notons-la IW(C,¬µ) √† partir de maintenant.\n\nl‚Äôinertie intra classe est √©gale √† :\n\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n[1] 5\n\n\n\n\nPropri√©t√©s\n\nL‚Äôalgorithme de Lloyd permet de diminuer l‚Äôinertie intra-classe IW √† chaque it√©ration. Ainsi : \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)}) \\ge \\mathrm{IW}(C^{(m+1)}, \\mu^{(m)})\\)\nLa suite num√©rique \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)})\\) est stationnaire(i.e.,√† partir d‚Äôun certain nombre d‚Äôit√©rations,elle prend toujours la m√™me valeur).\n\nVoici un exemple de script R utilisant l‚Äôalgorithme k-means pour trouver les classes (les clusters) des donn√©es x={1, 2, 9, 12, 20} avec les barycentres ¬µ1 =1, ¬µ2 =9, ¬µ3 =12 et ¬µ4 =20.\n\n# donn√©es\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des donn√©es par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs √©l√©ments\ncat(\"\\n√âl√©ments par classe :\\n\")\n\n\n√âl√©ments par classe :\n\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res3$centers)\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nl‚Äôinertie intra classe est √©gale √† :\n\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n[1] 0.5"
  },
  {
    "objectID": "modeles/mod_kmeans.html#m√©thode-du-coude",
    "href": "modeles/mod_kmeans.html#m√©thode-du-coude",
    "title": "Algorithme des K-means",
    "section": "M√©thode du coude",
    "text": "M√©thode du coude\nLe crit√®re du coude est une m√©thode visuelle consistant √† tracer l‚Äôinertie intra de k-means en fonction de k et √† choisir le nombre de clusters correspondant au point d‚Äôinflexion de la courbe.\n\n# vecteur de valeurs de k √† tester\nks    &lt;- 2:4\n\n# pr√©-allocation d‚Äôun vecteur pour stocker l‚Äôinertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# trac√© de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reli√©s par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"M√©thode du coude pour le choix de k\")\n\n\n\n\n\n\n\n\nConclusion : La m√©thode du coude privil√©gie donc 3 clusters qui permettent de capturer la plus grande partie de l‚Äôinertie intra-classe. En effet, ajouter une 4√®me classe, n‚Äôapporte qu‚Äôun faible gain d‚Äôinertie intra-classe."
  },
  {
    "objectID": "modeles/mod_mmelange.html",
    "href": "modeles/mod_mmelange.html",
    "title": "Loi de m√©lange",
    "section": "",
    "text": "La classification par mod√®les de m√©lange est une approche probabiliste. L‚Äôid√©e est de mod√©liser la distribution des donn√©es observ√©es par un m√©lange de distributions de probabilit√©. On d√©finit une classe comme l‚Äôensemble des observations issues de la m√™me composante du m√©lange."
  },
  {
    "objectID": "modeles/mod_mmelange.html#introduction",
    "href": "modeles/mod_mmelange.html#introduction",
    "title": "Loi de m√©lange",
    "section": "",
    "text": "La classification par mod√®les de m√©lange est une approche probabiliste. L‚Äôid√©e est de mod√©liser la distribution des donn√©es observ√©es par un m√©lange de distributions de probabilit√©. On d√©finit une classe comme l‚Äôensemble des observations issues de la m√™me composante du m√©lange."
  },
  {
    "objectID": "modeles/mod_mmelange.html#loi-de-m√©lange",
    "href": "modeles/mod_mmelange.html#loi-de-m√©lange",
    "title": "Loi de m√©lange",
    "section": "Loi de m√©lange",
    "text": "Loi de m√©lange\n\n\n\n\n\n\nD√©finition\n\n\n\nUne loi de m√©lange \\(p\\) √† \\(K\\) composantes et √† support sur un espace \\(\\mathcal{X}\\) est une combinaison lin√©aire de \\(K\\) lois de probabilit√© \\(p_1, \\ldots, p_K\\) sur \\(\\mathcal{X}\\).\n\n\nAinsi, il existe \\(K\\) coefficients \\(\\pi_1, \\ldots, \\pi_K\\), avec \\(\\pi_k &gt; 0\\) et \\(\\sum_{k=1}^K \\pi_k = 1\\), tels que, pour tout mesurable \\(A \\subset \\mathcal{X}\\), on a\n\\[\np(A ; \\theta) = \\sum_{k=1}^K \\pi_k p_k(A ; \\alpha_k), \\tag{1}\n\\]\no√π \\(\\theta = \\left\\{ \\{\\pi_k, \\alpha_k \\} : k = 1, \\ldots, K \\right\\}\\) groupe les param√®tres du mod√®le et \\(\\alpha_k\\) groupe les param√®tres de la composante \\(k\\).\nSi elle existe, on peut √©crire la densit√© d‚Äôune loi de m√©lange comme :\n\\[\nf(x ; \\theta) = \\sum_{k=1}^K \\pi_k f_k(x ; \\alpha_k), \\tag{2}\n\\]\npour tout \\(x \\in \\mathcal{X}\\), o√π \\(f_k\\) est la densit√© associ√©e √† \\(p_k\\)."
  },
  {
    "objectID": "modeles/mod_mmelange.html#mod√©lisation",
    "href": "modeles/mod_mmelange.html#mod√©lisation",
    "title": "Loi de m√©lange",
    "section": "Mod√©lisation",
    "text": "Mod√©lisation\nSoient \\(x_1, \\ldots, x_n\\) des observations correspondant √† \\(n\\) individus issus d‚Äôune population \\(\\Omega\\) compos√©e par \\(K\\) sous-populations disjointes :\n\\[\n\\Omega = \\{1, \\ldots, n\\} = C_1 \\cup \\ldots \\cup C_K \\quad \\leftarrow \\text{Vrai classement}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nOn mod√©lise les observations \\(x_1, \\ldots, x_n\\) comme des r√©alisations de \\(n\\) variables al√©atoires i.i.d. \\(\\; X_1, \\ldots, X_n\\).\n\n\nOn d√©finit \\(Z_i = (Z_{i1}, \\ldots, Z_{iK})\\) par :\n\\[\nZ_{ik} =\n\\begin{cases}\n1 & \\text{si } i \\in C_k \\\\\n0 & \\text{sinon}\n\\end{cases}\n\\quad \\text{avec} \\quad \\mathbb{P}(Z_{ik} = 1) = \\pi_k\n\\]\nprobabilit√© a priori d‚Äôappartenance √† \\(C_k\\).\nD‚Äôapr√®s la d√©finition de \\(Z_i\\) : \\(\\sum_{k=1}^K \\pi_k = 1\\), et puis, par construction,\n\\[\nZ_i \\sim \\mathcal{M}(1; \\pi_1, \\ldots, \\pi_K). \\quad \\text{(Loi multinomiale)}\n\\]\nPour tout \\(i \\in \\{1, \\ldots, n\\}\\), on mod√©lise par \\(p_k(\\cdot)\\) la loi de \\(X_i \\mid \\{Z_{ik} = 1\\}\\) et par \\(p(\\cdot)\\) la loi de \\(X_i\\), i.e.\n\\[\np_k(A) = \\mathbb{P}(X_i \\in A \\mid Z_{ik} = 1), \\\\\np(A) = \\mathbb{P}(X_i \\in A),\n\\]\npour tout \\(A\\) mesurable dans \\(\\mathcal{X}\\). En utilisant la formule des probabilit√©s totales, on peut √©crire :\n\\[\np(A) = \\mathbb{P}(X_i \\in A) = \\sum_{k=1}^K \\mathbb{P}(X_i \\in A \\mid Z_{ik} = 1)\\mathbb{P}(Z_{ik} = 1) = \\sum_{k=1}^K \\pi_k \\mathbb{P}(X_i \\in A \\mid Z_{ik} = 1).\n\\]\nPar cons√©quent, on a :\n\\[\np(A) = \\sum_{k=1}^K \\pi_k p_k(A), \\tag{3}\n\\]\npour tout mesurable \\(A \\subset \\mathcal{X}\\).\nFinalement, en d√©notant les \\(p_k(\\cdot)\\) et \\(p(\\cdot)\\) en fonction de leurs param√®tres, on peut r√©√©crire (3) comme :\n\\[\np(A, \\theta) = \\sum_{k=1}^K \\pi_k p_k(A ; \\alpha_k)\n\\quad \\text{o√π} \\quad \\theta = \\left\\{ \\{\\pi_k, \\alpha_k\\} : \\; k = 1, \\ldots, K \\right\\}, \\tag{4}\n\\]\nqui est une loi de m√©lange.\nEt la densit√© du m√©lange (fonction de masse) s‚Äô√©crit comme une somme pond√©r√©e de lois conditionnelles :\n\\[\nf(x_i; \\theta) = \\sum_{k=1}^{K} \\mathbb{P}(Z_i = k) \\cdot \\mathbb{P}(X_i = x_i \\mid Z_i = k) = \\sum_{k=1}^{K} \\pi_k f_k(x_i; \\alpha_k)\n\\]\no√π :\n\n\\(Z_i\\) est la variable latente (pas directement observable mais qui influence le mod√®le) indiquant la composante √† laquelle appartient l‚Äôobservation \\(x_i\\) ;\n\n\\(\\pi_k = \\mathbb{P}(Z_i = k)\\) est la probabilit√© a priori d‚Äôappartenir √† la classe \\(k\\) ;\n\n\\(f_k(x_i; \\alpha_k)\\) est la densit√© conditionnelle dans la classe \\(k\\) avec les param√®tres \\(\\alpha_k\\).\n\n\n\n\n\n\n\nNote\n\n\n\nAinsi les observations \\(x_1, \\ldots, x_n\\) sont mod√©lis√©es pour √™tre tir√©es d‚Äôune loi de m√©lange.\n\n\nEn r√©sum√©, ce mod√®le caract√©rise chaque sous-population \\(C_k\\) par :\n\n\\(\\pi_k\\), qui repr√©sente la proportion d‚Äôindividus appartenant √† la \\(k\\)-i√®me sous-population,\n\\(\\alpha_k\\), i.e.¬†les param√®tres de la distribution de \\(p_k\\).\n\n\nMod√®le de m√©lange le plus utilis√©\nLe mod√®le de m√©lange le plus utilis√© est le m√©lange gaussien :\nSi \\(\\mathcal{X} = \\mathbb{R}^d\\), le mod√®le le plus utilis√© est le m√©lange gaussien. Ainsi\n\\[\nf(x_i ; \\theta) = \\sum_{k=1}^K \\pi_k \\, \\phi_d(x_i ; \\mu_k, \\Sigma_k),\n\\]\no√π \\(\\theta\\) groupe tous les param√®tres, \\(\\pi_k\\) est la proportion de la classe \\(k\\),\n\\(\\mu_k \\in \\mathbb{R}^d\\) est le centre de la classe \\(k\\) et \\(\\Sigma_k\\) est sa matrice de covariance.\nPr√©cis√©ment :\n\n\nExemple: M√©lange de 3 gaussiennes univari√©es\nLa densit√© d‚Äôun m√©lange de 3 gaussiennes univari√©es (ne d√©pendant que de x) s‚Äô√©crit :\n\\[\nf(x_i; \\theta) = \\pi_1 f_1(x; \\alpha_1) + \\pi_2 f_2(x; \\alpha_2) + \\pi_3 f_3(x; \\alpha_3)\n\\]\no√π :\n\\[\nf_k(x; \\alpha_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp\\left( -\\frac{(x - \\mu_k)^2}{2\\sigma_k^2} \\right)\n\\]\net :\n\\[\n\\theta = \\left\\{ \\{\\pi_k, \\alpha_k\\} : \\alpha_k = (\\mu_k, \\sigma_k) \\text{ pour } k = 1, \\ldots, 3 \\right\\}\n\\]"
  },
  {
    "objectID": "modeles/mod_mmelange.html#vraisemblance",
    "href": "modeles/mod_mmelange.html#vraisemblance",
    "title": "Loi de m√©lange",
    "section": "Vraisemblance",
    "text": "Vraisemblance\nPour un mod√®le de m√©lange, on utilise une vraisemblance car en la maximisant, nous trouverons les valeurs des param√®tres qui rendent les donn√©es les plus probables (valeurs optimales)\n\nVraisemblance incompl√®te du mod√®le\nLa vraisemblance incompl√®te du mod√®le est :\n\\[\n\\mathcal{L}(\\theta; x) = \\prod_{i=1}^{n} f(x_i, \\theta) = \\prod_{i=1}^{n} \\sum_{k=1}^{K} \\pi_k f_k(x_i; \\alpha_k)\n\\]\net sa log-vraisemblance est :\n\\[\n\\ell(\\theta; x) = \\sum_{i=1}^{n} \\log \\left( \\sum_{k=1}^{K} \\pi_k f_k(x_i; \\alpha_k) \\right)\n\\]\n\n\nVraisemblance compl√®te du mod√®le\nLa vraisemblance compl√®te du mod√®le est :\n\\[\n\\mathcal{L}(\\theta; x, z) = \\prod_{i=1}^{n} \\prod_{k=1}^{K} \\left[ \\pi_k f_k(x_i; \\alpha_k) \\right]^{z_{ik}}\n\\]\no√π \\(z_{ik} = 1\\) si l‚Äôobservation \\(i\\) provient de la composante \\(k\\), \\(0\\) sinon.\nDonc la log-vraisemblance compl√©t√©e est :\n\\[\n\\ell(\\theta; x, z) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} z_{ik} \\left( \\log \\pi_k + \\log f_k(x_i; \\alpha_k) \\right)\n\\]"
  },
  {
    "objectID": "modeles/mod_mmelange.html#lalgorithme-em-expectation-maximisation",
    "href": "modeles/mod_mmelange.html#lalgorithme-em-expectation-maximisation",
    "title": "Loi de m√©lange",
    "section": "L‚Äôalgorithme EM (Expectation Maximisation)",
    "text": "L‚Äôalgorithme EM (Expectation Maximisation)\nL‚Äôalgorithme EM est une m√©thode d‚Äôoptimisation utilis√©e pour estimer les param√®tres d‚Äôun mod√®le statistique lorsque les donn√©es sont incompl√®tes ou comportent des variables latentes.\n\nAlgorithme EM : d√©finition\nL‚Äôalgorithme EM est initialis√© au param√®tre \\(\\theta^{[0]}\\).\nIl it√®re entre les √©tapes E et M d√©finies √† l‚Äôit√©ration \\(r\\) par :\n‚Äî √âtape E : calcul des probabilit√©s a posteriori \\(t_{ik}\\) sachant \\(\\theta^{[r-1]}\\)\n\\[\nt_{ik}(\\theta^{[r-1]}) = \\frac{\\pi_k^{[r-1]} f_k(x_i; \\alpha_k^{[r-1]})}{\\sum_{l=1}^{K} \\pi_l^{[r-1]} f_l(x_i; \\alpha_l^{[r-1]})}\n\\]\n‚Äî √âtape M : maximisation de l‚Äôesp√©rance de la log-vraisemblance compl√©t√©e\n\\[\n\\theta^{[r]} = \\arg\\max_{\\alpha_k, \\pi_k} \\sum_{i=1}^{n} \\sum_{k=1}^{K} t_{ik}(\\theta^{[r-1]}) \\log \\left( \\pi_k f_k(x_i; \\alpha_k) \\right)\n\\]\nOn s‚Äôarr√™te lorsque les valeurs des param√®tres ou de la quantit√© √† maximiser entre deux it√©rations successives ne varient ¬´ presque ¬ª plus.\nL‚Äôalgorithme EM permet de calculer les valeurs du param√®tres qui maximise la vraisemblance √† chaque it√©ration."
  },
  {
    "objectID": "modeles/mod_mmelange.html#application-distribution-de-m√©lange-de-poisson",
    "href": "modeles/mod_mmelange.html#application-distribution-de-m√©lange-de-poisson",
    "title": "Loi de m√©lange",
    "section": "Application : distribution de m√©lange de Poisson",
    "text": "Application : distribution de m√©lange de Poisson\n\nFonction de masse d‚Äôune observation xi\nLa densit√© (fonction de masse) du m√©lange s‚Äô√©crit :\n\\[\nf(x_i ; \\theta) = \\sum_{k=1}^{K} \\pi_k \\cdot \\frac{e^{-\\lambda_k} \\lambda_k^{x_i}}{x_i!}\n\\]\navec \\(\\sum_k\\) \\(\\pi_k = 1\\) , \\(\\pi_k &gt;0\\) et \\(\\lambda_k &gt; 0\\).\n\n\nfonctions de log-vraisemblance et log-vraisemblance compl√©t√©e du mod√®le\n\nVraisemblance :\n\n\\[\n\\mathcal{L}(\\theta ; x) = \\prod_{i=1}^{n} \\left( \\sum_{k=1}^{K} \\pi_k \\cdot \\frac{e^{-\\lambda_k} \\lambda_k^{x_i}}{x_i!} \\right)\n\\]\n\nLog-vraisemblance :\n\n\\[\n\\ell(\\theta ; x) = \\sum_{i=1}^{n} \\log \\left( \\sum_{k=1}^{K} \\pi_k \\cdot \\frac{e^{-\\lambda_k} \\lambda_k^{x_i}}{x_i!} \\right)\n\\]\n\nLog-vraisemblance compl√©t√©e :\n\n\\[\n\\ell(\\theta ; x, z) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} z_{ik} \\log(\\pi_k f_k(x_i ; \\lambda_k))\n= \\sum_{i=1}^{n} \\sum_{k=1}^{K} z_{ik} \\left( \\log \\pi_k - \\lambda_k + x_i \\log \\lambda_k - \\log(x_i!) \\right)\n\\]\nLe terme \\(\\log(x_i!)\\), √©tant constant par rapport √† \\(\\theta\\), peut √™tre ignor√© lors de l‚Äôoptimisation.\n\n\nAlgorithme EM\n\n√âtape E : calcul des probabilit√©s a posteriori \\(t_{ik}\\) sachant \\(\\theta^{[r-1]}\\) :\n\n\\[\nt_{ik}(\\theta^{[r-1]}) =\n\\frac{\n  \\pi_k^{[r-1]} e^{-\\lambda_k^{[r-1]}} \\left( \\lambda_k^{[r-1]} \\right)^{x_i}\n}{\n  \\sum_{h=1}^{K} \\pi_h^{[r-1]} e^{-\\lambda_h^{[r-1]}} \\left( \\lambda_h^{[r-1]} \\right)^{x_i}\n}\n\\]\n\nInitialisation : \\(\\theta^{[0]}\\) = \\(\\left\\{ \\pi_k^{[0]}, \\lambda_k^{[0]} \\right\\}\\)\n√âtape M : Maximiser √† chaque √©tape l‚Äôesp√©rance conditionnelle de la log-vraisemblance compl√©t√©e :\n\n\\[\nQ(\\theta ; \\theta^{[r-1]}) =\n\\sum_{i=1}^{n} \\sum_{k=1}^{K}\nt_{ik}(\\theta^{[r-1]}) \\left( \\log \\pi_k - \\lambda_k + x_i \\log \\lambda_k \\right)\n\\]\n\\[\n\\theta^{[r]} = \\arg \\max_{\\lambda_k, \\pi_k}\n\\sum_{i=1}^{n} \\sum_{k=1}^{K}\nt_{ik}(\\theta^{[r-1]}) \\left( \\log \\pi_k - \\lambda_k + x_i \\log \\lambda_k \\right)\n\\]\n\n\nValeurs des param√®tres qui maximisent la vraisemblance √† chaque it√©ration\n\n\n1. Mise √† jour des param√®tres \\(\\lambda_k^{[r]}\\)\nOn commence par maximiser par rapport √† \\(\\lambda_k\\), car cette optimisation ne d√©pend pas des contraintes sur les \\(\\pi_k\\), et les termes sont s√©parables pour chaque composante ( k ).\nOn veut maximiser pour chaque k :\n\\[\nL_k(\\lambda_k) = \\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) (-\\lambda_k + x_i \\log \\lambda_k)\n\\]\nOn d√©rive cette fonction :\n\\[\n\\frac{\\partial L_k}{\\partial \\lambda_k}\n= \\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) \\left( -1 + \\frac{x_i}{\\lambda_k} \\right)\n= -\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) + \\frac{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) x_i}{\\lambda_k}\n\\]\nOn annule la d√©riv√©e :\n\\[\n-\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) + \\frac{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) x_i}{\\lambda_k} = 0\n\\quad \\Rightarrow \\quad\n\\lambda_k^{[r]} = \\frac{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) x_i}{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]})}\n\\]\n\n\n2. Mise √† jour des poids \\(\\pi_k^{[r]}\\)\nOn cherche √† maximiser sur \\((\\pi_1, \\pi_2, \\ldots, \\pi_K, \\phi)\\) la fonction :\n\\[\n\\sum_{k=1}^{K} \\left( \\sum_{i=1}^{n} t_{ik}(\\lambda_k^{[r-1]}) \\right) \\log \\pi_k\n\\quad \\text{ sous la contrainte } \\quad \\sum_{k=1}^{K} \\pi_k = 1\n\\]\nOn introduit un multiplicateur de Lagrange \\(\\phi\\) :\n\\[\n\\mathcal{L}(\\pi, \\phi) =\n\\sum_{k=1}^{K} \\left( \\sum_{i} t_{ik}(\\lambda_k^{[r-1]}) \\right) \\log \\pi_k\n- \\phi \\left( \\sum_{k=1}^{K} \\pi_k - 1 \\right)\n\\]\nOn d√©rive la fonction de Lagrange :\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi_k}\n= \\frac{\\sum_i t_{ik}(\\theta^{[r-1]})}{\\pi_k} - \\phi = 0\n\\quad \\Rightarrow \\quad\n\\pi_k = \\frac{\\sum_i t_{ik}(\\theta^{[r-1]})}{\\phi}\n\\]\nOr, sous la contrainte \\(\\sum_{k=1}^{K} \\pi_k = 1\\) :\n\\[\n\\phi = \\sum_{i=1}^{n} \\sum_{k=1}^{K} t_{ik}(\\theta^{[r-1]}) = n\n\\quad \\Rightarrow \\quad\n\\boxed{\n\\pi_k^{[r]} = \\frac{1}{n} \\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]})\n}\n\\]"
  },
  {
    "objectID": "exemples/kmeans.html",
    "href": "exemples/kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de donn√©es simulant les notes de 30 √©tudiants en 4 mati√®res d‚Äôune fili√®re ing√©nieur, applique le k-means pour segmenter les profils.\n\n\n\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.5), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\n\n\n\n\nIci, on centre et r√©duit chaque variable comme la dispersion des notes de chaque mati√®re est diff√©rente.Ainsi, les notes des mati√®res pourront √™tre comparables\n\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carr√©s intra-clusters\",\n     main = \"M√©thode du coude pour choisir k\")\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\nset.seed(42)\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\nK-means clustering with 3 clusters of sizes 9, 12, 9\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.8619349  0.7183503   -0.4564989 -0.6801298\n2     0.8123169 -0.4677940    0.6541109 -0.3870757\n3    -0.2211544 -0.0946250   -0.4156489  1.1962307\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  3   3   2   1   3   2   3   3   2   2   2   3   2   1   1   2   2   1   2   3 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  1   3   1   1   3   1   2   2   1   2 \n\nWithin cluster sum of squares by cluster:\n[1] 20.63932 29.27492 16.28552\n (between_SS / total_SS =  42.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$√âtudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = √âtudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils √©tudiants\",\n       subtitle = \"Projection sur les deux premi√®res composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\npca$rotation[, 1:2] \n\n                      PC1         PC2\nMathematiques  0.50381044 -0.34672112\nPhysique      -0.60778171 -0.20884255\nInformatique   0.61054070 -0.01669262\nChimie        -0.06337573 -0.91427053\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxi√®me axe factoriel s√©pare les √©tudiants bons en chimie (en haut) des plus faibles en chimie (en bas)"
  },
  {
    "objectID": "exemples/kmeans.html#exemple",
    "href": "exemples/kmeans.html#exemple",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de donn√©es simulant les notes de 30 √©tudiants en 4 mati√®res d‚Äôune fili√®re ing√©nieur, applique le k-means pour segmenter les profils.\n\n\n\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.5), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\n\n\n\n\nIci, on centre et r√©duit chaque variable comme la dispersion des notes de chaque mati√®re est diff√©rente.Ainsi, les notes des mati√®res pourront √™tre comparables\n\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carr√©s intra-clusters\",\n     main = \"M√©thode du coude pour choisir k\")\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\nset.seed(42)\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\nK-means clustering with 3 clusters of sizes 9, 12, 9\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.8619349  0.7183503   -0.4564989 -0.6801298\n2     0.8123169 -0.4677940    0.6541109 -0.3870757\n3    -0.2211544 -0.0946250   -0.4156489  1.1962307\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  3   3   2   1   3   2   3   3   2   2   2   3   2   1   1   2   2   1   2   3 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  1   3   1   1   3   1   2   2   1   2 \n\nWithin cluster sum of squares by cluster:\n[1] 20.63932 29.27492 16.28552\n (between_SS / total_SS =  42.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$√âtudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = √âtudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils √©tudiants\",\n       subtitle = \"Projection sur les deux premi√®res composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\npca$rotation[, 1:2] \n\n                      PC1         PC2\nMathematiques  0.50381044 -0.34672112\nPhysique      -0.60778171 -0.20884255\nInformatique   0.61054070 -0.01669262\nChimie        -0.06337573 -0.91427053\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxi√®me axe factoriel s√©pare les √©tudiants bons en chimie (en haut) des plus faibles en chimie (en bas)"
  },
  {
    "objectID": "exemples/kmeans.html#conclusion",
    "href": "exemples/kmeans.html#conclusion",
    "title": "Algorithme des K-means",
    "section": "Conclusion",
    "text": "Conclusion\nLes clusters issus de k-means se retrouvent bien s√©par√©s :\n\ncluster (rouge) : Ces √©tudiants sont bons en physique.\ncluster (vert) : Ces √©tudiants sont forts en Maths et informatique\ncluster (bleu) : Ces √©tudiants ont des performances en chimie."
  },
  {
    "objectID": "exemples/CAH.html",
    "href": "exemples/CAH.html",
    "title": "Algorithme CAH",
    "section": "",
    "text": "Les donn√©es repr√©sentent les d√©penses de clients dans un magasin selon 3 cat√©gories: alimentaire, habillement et √©lectronique.\nEnjeu : On souhaite identifier des typologies de clients pr√©sentant des comportements similaires pour adapter l‚Äôoffre √† chaque poste de d√©penses identifi√©.\nProbl√©matique : Comment regrouper de mani√®re homog√®ne une client√®le selon ses habitudes de consommation dans trois domaines : l‚Äôalimentaire, l‚Äôhabillement, et l‚Äô√©lectronique ?\n\n\n\ndata &lt;- read.delim2(\"clients_depenses.csv\", sep = \"\\t\", stringsAsFactors = FALSE)\ndata\n\n      Client Alimentaire Habillement √âlectronique\n1   Client_1      539.73      373.28      1110.77\n2   Client_2      488.93      288.71      1025.71\n3   Client_3      551.81      303.37       982.65\n4   Client_4      621.84      228.76       954.83\n5   Client_5      481.26      272.78       778.22\n6   Client_6      481.26      305.54       892.02\n7   Client_7      626.33      242.45       930.90\n8   Client_8      561.39      318.78      1158.57\n9   Client_9      462.44      269.96      1051.54\n10 Client_10      543.40      285.41       735.54\n11 Client_11      462.92      269.91      1048.61\n12 Client_12      462.74      392.61       942.24\n13 Client_13      519.35      299.32       898.46\n14 Client_14      346.93      247.11      1091.75\n15 Client_15      362.14      341.12      1154.65\n16 Client_16      455.01      238.95      1139.69\n17 Client_17      418.97      310.44       874.12\n18 Client_18      525.13      202.01       953.62\n19 Client_19      427.35      233.59      1049.69\n20 Client_20      387.01      309.84      1146.33\n\n\n\n\n\n\n# Extraction des donn√©es num√©riques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la m√©thode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n# D√©coupage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de d√©part\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n# √âtape 1 : R√©aliser l'ACP sur les colonnes num√©riques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# √âtape 2 : Extraire les coordonn√©es sur les deux premi√®res composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n                    PC1        PC2\nAlimentaire  -0.6615989  0.1989097\nHabillement   0.3955003  0.9117238\n√âlectronique  0.6370765 -0.3594365"
  },
  {
    "objectID": "exemples/CAH.html#exemple",
    "href": "exemples/CAH.html#exemple",
    "title": "Algorithme CAH",
    "section": "",
    "text": "Les donn√©es repr√©sentent les d√©penses de clients dans un magasin selon 3 cat√©gories: alimentaire, habillement et √©lectronique.\nEnjeu : On souhaite identifier des typologies de clients pr√©sentant des comportements similaires pour adapter l‚Äôoffre √† chaque poste de d√©penses identifi√©.\nProbl√©matique : Comment regrouper de mani√®re homog√®ne une client√®le selon ses habitudes de consommation dans trois domaines : l‚Äôalimentaire, l‚Äôhabillement, et l‚Äô√©lectronique ?\n\n\n\ndata &lt;- read.delim2(\"clients_depenses.csv\", sep = \"\\t\", stringsAsFactors = FALSE)\ndata\n\n      Client Alimentaire Habillement √âlectronique\n1   Client_1      539.73      373.28      1110.77\n2   Client_2      488.93      288.71      1025.71\n3   Client_3      551.81      303.37       982.65\n4   Client_4      621.84      228.76       954.83\n5   Client_5      481.26      272.78       778.22\n6   Client_6      481.26      305.54       892.02\n7   Client_7      626.33      242.45       930.90\n8   Client_8      561.39      318.78      1158.57\n9   Client_9      462.44      269.96      1051.54\n10 Client_10      543.40      285.41       735.54\n11 Client_11      462.92      269.91      1048.61\n12 Client_12      462.74      392.61       942.24\n13 Client_13      519.35      299.32       898.46\n14 Client_14      346.93      247.11      1091.75\n15 Client_15      362.14      341.12      1154.65\n16 Client_16      455.01      238.95      1139.69\n17 Client_17      418.97      310.44       874.12\n18 Client_18      525.13      202.01       953.62\n19 Client_19      427.35      233.59      1049.69\n20 Client_20      387.01      309.84      1146.33\n\n\n\n\n\n\n# Extraction des donn√©es num√©riques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la m√©thode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n# D√©coupage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de d√©part\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n# √âtape 1 : R√©aliser l'ACP sur les colonnes num√©riques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# √âtape 2 : Extraire les coordonn√©es sur les deux premi√®res composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n                    PC1        PC2\nAlimentaire  -0.6615989  0.1989097\nHabillement   0.3955003  0.9117238\n√âlectronique  0.6370765 -0.3594365"
  },
  {
    "objectID": "exemples/CAH.html#conclusion",
    "href": "exemples/CAH.html#conclusion",
    "title": "Algorithme CAH",
    "section": "Conclusion",
    "text": "Conclusion\nLa CAH permet de regrouper les 20 clients en 3 segments de consommation distincts. Ces clusters peuvent correspondre √† :\n\nCluster 1 (rouge) : Ces clients correspondent √† des gros consommateurs en √©lectronique. Pour une entreprise, il est int√©ressant de cibler de type de client pour leur proposer des offres int√©ressantes ou des nouveaut√©s.\nCluster 2 (vert) : Ce sont des clients orient√©s alimentaire et l‚Äôhabillement, peu consommateurs d‚Äô√©lectronique. Pour une entreprise, il est int√©ressant de cibler de type de client pour leur proposer par exemple des cartes de fid√©lit√©.\nCluster 3 (bleu) : Ce sont des clients √† fort int√©r√™t pour l‚Äôhabillement, avec des comportements mixtes sur les autres postes. Pour une entreprise, il est int√©ressant de cibler de type de client pour leur proposer des promotions ou des nouvelles gammes."
  },
  {
    "objectID": "presentation/historique.html",
    "href": "presentation/historique.html",
    "title": "D√©finition",
    "section": "",
    "text": "Voici un bref historique des grandes √©tapes de la classification non supervis√©e :\n\nAnn√©es 1930 ‚Äì Naissance de la classification hi√©rarchique John Tukey et d‚Äôautres statisticiens proposent des m√©thodes de regroupement bas√©es sur la similarit√© (dendrogrammes) pour organiser les donn√©es sans labels.\nApparue de la classification ascendante hi√©rarchique dans les ann√©es 1950-60 (notamment grace √† Sokal & Michener en 1958).\n1965 ‚Äì Algorithme des nu√©es dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularis√© par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s ‚Äì Mod√®les √† m√©langes et l‚Äôalgorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) g√©n√©ralisent les mod√®les de m√©lange gaussien, entra√Æn√©s via l‚ÄôExpectation‚ÄìMaximization, pour estimer des sous-populations au sein d‚Äôun jeu de donn√©es.\nAnn√©es 1980 ‚Äì Cartes auto-organisatrices (SOM) Teuvo Kohonen d√©veloppe les Self-Organizing Maps, r√©seaux de neurones non supervis√©s qui projettent des donn√©es de haute dimension en cartes topologiques 2D.\nAnn√©es 1990 ‚Äì Clustering spectral et plus de flexibilit√© Des m√©thodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarit√©) permettent de d√©tecter des structures non sph√©riques. Parall√®lement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnn√©es 2000 ‚Äì Big Data et densit√© L‚Äôalgorithme DBSCAN (1996) gagne en popularit√© ; en 2002, Ester et al.¬†le pr√©cisent pour d√©tecter les zones de forte densit√© dans de grands volumes de donn√©es.\nAnn√©es 2010 ‚Äì Deep clustering L‚Äô√©mergence du deep learning entra√Æne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultan√©ment des repr√©sentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette √©volution illustre comment la classification non supervis√©e est pass√©e de m√©thodes statistiques simples (hi√©rarchique, k-means) √† des approches sophistiqu√©es m√™lant repr√©sentations profondes et techniques de graphe."
  },
  {
    "objectID": "presentation/historique.html#dates-cl√©s",
    "href": "presentation/historique.html#dates-cl√©s",
    "title": "D√©finition",
    "section": "",
    "text": "Voici un bref historique des grandes √©tapes de la classification non supervis√©e :\n\nAnn√©es 1930 ‚Äì Naissance de la classification hi√©rarchique John Tukey et d‚Äôautres statisticiens proposent des m√©thodes de regroupement bas√©es sur la similarit√© (dendrogrammes) pour organiser les donn√©es sans labels.\nApparue de la classification ascendante hi√©rarchique dans les ann√©es 1950-60 (notamment grace √† Sokal & Michener en 1958).\n1965 ‚Äì Algorithme des nu√©es dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularis√© par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s ‚Äì Mod√®les √† m√©langes et l‚Äôalgorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) g√©n√©ralisent les mod√®les de m√©lange gaussien, entra√Æn√©s via l‚ÄôExpectation‚ÄìMaximization, pour estimer des sous-populations au sein d‚Äôun jeu de donn√©es.\nAnn√©es 1980 ‚Äì Cartes auto-organisatrices (SOM) Teuvo Kohonen d√©veloppe les Self-Organizing Maps, r√©seaux de neurones non supervis√©s qui projettent des donn√©es de haute dimension en cartes topologiques 2D.\nAnn√©es 1990 ‚Äì Clustering spectral et plus de flexibilit√© Des m√©thodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarit√©) permettent de d√©tecter des structures non sph√©riques. Parall√®lement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnn√©es 2000 ‚Äì Big Data et densit√© L‚Äôalgorithme DBSCAN (1996) gagne en popularit√© ; en 2002, Ester et al.¬†le pr√©cisent pour d√©tecter les zones de forte densit√© dans de grands volumes de donn√©es.\nAnn√©es 2010 ‚Äì Deep clustering L‚Äô√©mergence du deep learning entra√Æne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultan√©ment des repr√©sentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette √©volution illustre comment la classification non supervis√©e est pass√©e de m√©thodes statistiques simples (hi√©rarchique, k-means) √† des approches sophistiqu√©es m√™lant repr√©sentations profondes et techniques de graphe."
  }
]