[
  {
    "objectID": "presentation/historique.html",
    "href": "presentation/historique.html",
    "title": "Définition",
    "section": "",
    "text": "Voici un bref historique des grandes étapes de la classification non supervisée :\n\nAnnées 1930 – Naissance de la classification hiérarchique John Tukey et d’autres statisticiens proposent des méthodes de regroupement basées sur la similarité (dendrogrammes) pour organiser les données sans labels.\nApparue de la classification ascendante hiérarchique dans les années 1950-60 (notamment grace à Sokal & Michener en 1958).\n1965 – Algorithme des nuées dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisé par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s – Modèles à mélanges et l’algorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) généralisent les modèles de mélange gaussien, entraînés via l’Expectation–Maximization, pour estimer des sous-populations au sein d’un jeu de données.\nAnnées 1980 – Cartes auto-organisatrices (SOM) Teuvo Kohonen développe les Self-Organizing Maps, réseaux de neurones non supervisés qui projettent des données de haute dimension en cartes topologiques 2D.\nAnnées 1990 – Clustering spectral et plus de flexibilité Des méthodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarité) permettent de détecter des structures non sphériques. Parallèlement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnées 2000 – Big Data et densité L’algorithme DBSCAN (1996) gagne en popularité ; en 2002, Ester et al. le précisent pour détecter les zones de forte densité dans de grands volumes de données.\nAnnées 2010 – Deep clustering L’émergence du deep learning entraîne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanément des représentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette évolution illustre comment la classification non supervisée est passée de méthodes statistiques simples (hiérarchique, k-means) à des approches sophistiquées mêlant représentations profondes et techniques de graphe."
  },
  {
    "objectID": "presentation/historique.html#dates-clés",
    "href": "presentation/historique.html#dates-clés",
    "title": "Définition",
    "section": "",
    "text": "Voici un bref historique des grandes étapes de la classification non supervisée :\n\nAnnées 1930 – Naissance de la classification hiérarchique John Tukey et d’autres statisticiens proposent des méthodes de regroupement basées sur la similarité (dendrogrammes) pour organiser les données sans labels.\nApparue de la classification ascendante hiérarchique dans les années 1950-60 (notamment grace à Sokal & Michener en 1958).\n1965 – Algorithme des nuées dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisé par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s – Modèles à mélanges et l’algorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) généralisent les modèles de mélange gaussien, entraînés via l’Expectation–Maximization, pour estimer des sous-populations au sein d’un jeu de données.\nAnnées 1980 – Cartes auto-organisatrices (SOM) Teuvo Kohonen développe les Self-Organizing Maps, réseaux de neurones non supervisés qui projettent des données de haute dimension en cartes topologiques 2D.\nAnnées 1990 – Clustering spectral et plus de flexibilité Des méthodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarité) permettent de détecter des structures non sphériques. Parallèlement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnées 2000 – Big Data et densité L’algorithme DBSCAN (1996) gagne en popularité ; en 2002, Ester et al. le précisent pour détecter les zones de forte densité dans de grands volumes de données.\nAnnées 2010 – Deep clustering L’émergence du deep learning entraîne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanément des représentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette évolution illustre comment la classification non supervisée est passée de méthodes statistiques simples (hiérarchique, k-means) à des approches sophistiquées mêlant représentations profondes et techniques de graphe."
  },
  {
    "objectID": "exemples/kmeans.html",
    "href": "exemples/kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de données simulant les notes de 30 étudiants en 4 matières d’une filière ingénieur, applique le k-means pour segmenter les profils.\n\n\n\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.5), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\n\n\n\n\nIci, on centre et réduit chaque variable comme la dispersion des notes de chaque matière est différente.Ainsi, les notes des matières pourront être comparables\n\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carrés intra-clusters\",\n     main = \"Méthode du coude pour choisir k\")\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\nset.seed(42)\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\nK-means clustering with 3 clusters of sizes 9, 12, 9\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.8619349  0.7183503   -0.4564989 -0.6801298\n2     0.8123169 -0.4677940    0.6541109 -0.3870757\n3    -0.2211544 -0.0946250   -0.4156489  1.1962307\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  3   3   2   1   3   2   3   3   2   2   2   3   2   1   1   2   2   1   2   3 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  1   3   1   1   3   1   2   2   1   2 \n\nWithin cluster sum of squares by cluster:\n[1] 20.63932 29.27492 16.28552\n (between_SS / total_SS =  42.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$Étudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = Étudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils étudiants\",\n       subtitle = \"Projection sur les deux premières composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\npca$rotation[, 1:2] \n\n                      PC1         PC2\nMathematiques  0.50381044 -0.34672112\nPhysique      -0.60778171 -0.20884255\nInformatique   0.61054070 -0.01669262\nChimie        -0.06337573 -0.91427053\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxième axe factoriel sépare les étudiants bons en chimie (en haut) des plus faibles en chimie (en bas)"
  },
  {
    "objectID": "exemples/kmeans.html#exemple",
    "href": "exemples/kmeans.html#exemple",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de données simulant les notes de 30 étudiants en 4 matières d’une filière ingénieur, applique le k-means pour segmenter les profils.\n\n\n\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.5), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\n\n\n\n\nIci, on centre et réduit chaque variable comme la dispersion des notes de chaque matière est différente.Ainsi, les notes des matières pourront être comparables\n\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carrés intra-clusters\",\n     main = \"Méthode du coude pour choisir k\")\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\nset.seed(42)\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\nK-means clustering with 3 clusters of sizes 9, 12, 9\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.8619349  0.7183503   -0.4564989 -0.6801298\n2     0.8123169 -0.4677940    0.6541109 -0.3870757\n3    -0.2211544 -0.0946250   -0.4156489  1.1962307\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  3   3   2   1   3   2   3   3   2   2   2   3   2   1   1   2   2   1   2   3 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  1   3   1   1   3   1   2   2   1   2 \n\nWithin cluster sum of squares by cluster:\n[1] 20.63932 29.27492 16.28552\n (between_SS / total_SS =  42.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$Étudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = Étudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils étudiants\",\n       subtitle = \"Projection sur les deux premières composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\npca$rotation[, 1:2] \n\n                      PC1         PC2\nMathematiques  0.50381044 -0.34672112\nPhysique      -0.60778171 -0.20884255\nInformatique   0.61054070 -0.01669262\nChimie        -0.06337573 -0.91427053\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxième axe factoriel sépare les étudiants bons en chimie (en haut) des plus faibles en chimie (en bas)"
  },
  {
    "objectID": "exemples/kmeans.html#conclusion",
    "href": "exemples/kmeans.html#conclusion",
    "title": "Algorithme des K-means",
    "section": "Conclusion",
    "text": "Conclusion\nLes clusters issus de k-means se retrouvent bien séparés :\n\ncluster (rouge) : Ces étudiants sont bons en physique.\ncluster (vert) : Ces étudiants sont forts en Maths et informatique\ncluster (bleu) : Ces étudiants ont des performances en chimie."
  },
  {
    "objectID": "exemples/CAH.html",
    "href": "exemples/CAH.html",
    "title": "Algorithme CAH",
    "section": "",
    "text": "Les données représentent les dépenses de clients dans un magasin selon 3 catégories: alimentaire, habillement et électronique.\nEnjeu : On souhaite identifier des typologies de clients présentant des comportements similaires pour adapter l’offre à chaque poste de dépenses identifié.\nProblématique : Comment regrouper de manière homogène une clientèle selon ses habitudes de consommation dans trois domaines : l’alimentaire, l’habillement, et l’électronique ?\n\n\n\ndata &lt;- read.delim2(\"clients_depenses.csv\", sep = \"\\t\", stringsAsFactors = FALSE)\ndata\n\n      Client Alimentaire Habillement Électronique\n1   Client_1      539.73      373.28      1110.77\n2   Client_2      488.93      288.71      1025.71\n3   Client_3      551.81      303.37       982.65\n4   Client_4      621.84      228.76       954.83\n5   Client_5      481.26      272.78       778.22\n6   Client_6      481.26      305.54       892.02\n7   Client_7      626.33      242.45       930.90\n8   Client_8      561.39      318.78      1158.57\n9   Client_9      462.44      269.96      1051.54\n10 Client_10      543.40      285.41       735.54\n11 Client_11      462.92      269.91      1048.61\n12 Client_12      462.74      392.61       942.24\n13 Client_13      519.35      299.32       898.46\n14 Client_14      346.93      247.11      1091.75\n15 Client_15      362.14      341.12      1154.65\n16 Client_16      455.01      238.95      1139.69\n17 Client_17      418.97      310.44       874.12\n18 Client_18      525.13      202.01       953.62\n19 Client_19      427.35      233.59      1049.69\n20 Client_20      387.01      309.84      1146.33\n\n\n\n\n\n\n# Extraction des données numériques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la méthode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n# Découpage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de départ\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n# Étape 1 : Réaliser l'ACP sur les colonnes numériques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# Étape 2 : Extraire les coordonnées sur les deux premières composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n                    PC1        PC2\nAlimentaire  -0.6615989  0.1989097\nHabillement   0.3955003  0.9117238\nÉlectronique  0.6370765 -0.3594365"
  },
  {
    "objectID": "exemples/CAH.html#exemple",
    "href": "exemples/CAH.html#exemple",
    "title": "Algorithme CAH",
    "section": "",
    "text": "Les données représentent les dépenses de clients dans un magasin selon 3 catégories: alimentaire, habillement et électronique.\nEnjeu : On souhaite identifier des typologies de clients présentant des comportements similaires pour adapter l’offre à chaque poste de dépenses identifié.\nProblématique : Comment regrouper de manière homogène une clientèle selon ses habitudes de consommation dans trois domaines : l’alimentaire, l’habillement, et l’électronique ?\n\n\n\ndata &lt;- read.delim2(\"clients_depenses.csv\", sep = \"\\t\", stringsAsFactors = FALSE)\ndata\n\n      Client Alimentaire Habillement Électronique\n1   Client_1      539.73      373.28      1110.77\n2   Client_2      488.93      288.71      1025.71\n3   Client_3      551.81      303.37       982.65\n4   Client_4      621.84      228.76       954.83\n5   Client_5      481.26      272.78       778.22\n6   Client_6      481.26      305.54       892.02\n7   Client_7      626.33      242.45       930.90\n8   Client_8      561.39      318.78      1158.57\n9   Client_9      462.44      269.96      1051.54\n10 Client_10      543.40      285.41       735.54\n11 Client_11      462.92      269.91      1048.61\n12 Client_12      462.74      392.61       942.24\n13 Client_13      519.35      299.32       898.46\n14 Client_14      346.93      247.11      1091.75\n15 Client_15      362.14      341.12      1154.65\n16 Client_16      455.01      238.95      1139.69\n17 Client_17      418.97      310.44       874.12\n18 Client_18      525.13      202.01       953.62\n19 Client_19      427.35      233.59      1049.69\n20 Client_20      387.01      309.84      1146.33\n\n\n\n\n\n\n# Extraction des données numériques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la méthode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n# Découpage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de départ\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n# Étape 1 : Réaliser l'ACP sur les colonnes numériques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# Étape 2 : Extraire les coordonnées sur les deux premières composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n                    PC1        PC2\nAlimentaire  -0.6615989  0.1989097\nHabillement   0.3955003  0.9117238\nÉlectronique  0.6370765 -0.3594365"
  },
  {
    "objectID": "exemples/CAH.html#conclusion",
    "href": "exemples/CAH.html#conclusion",
    "title": "Algorithme CAH",
    "section": "Conclusion",
    "text": "Conclusion\nLa CAH permet de regrouper les 20 clients en 3 segments de consommation distincts. Ces clusters peuvent correspondre à :\n\nCluster 1 (rouge) : Ces clients correspondent à des gros consommateurs en électronique. Pour une entreprise, il est intéressant de cibler de type de client pour leur proposer des offres intéressantes ou des nouveautés.\nCluster 2 (vert) : Ce sont des clients orientés alimentaire et l’habillement, peu consommateurs d’électronique. Pour une entreprise, il est intéressant de cibler de type de client pour leur proposer par exemple des cartes de fidélité.\nCluster 3 (bleu) : Ce sont des clients à fort intérêt pour l’habillement, avec des comportements mixtes sur les autres postes. Pour une entreprise, il est intéressant de cibler de type de client pour leur proposer des promotions ou des nouvelles gammes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classification non supervisée",
    "section": "",
    "text": "Clustering K-Means dans R\nClassification ascendante hiérarchique avec R"
  },
  {
    "objectID": "index.html#des-liens-intéressants",
    "href": "index.html#des-liens-intéressants",
    "title": "Classification non supervisée",
    "section": "",
    "text": "Clustering K-Means dans R\nClassification ascendante hiérarchique avec R"
  },
  {
    "objectID": "modeles/mod_CAH.html",
    "href": "modeles/mod_CAH.html",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "",
    "text": "La classification hiérarchique ascendante est une méthode de partitionnement dont l’objectif est de construire une hiérarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant itérativement les groupes deux à deux."
  },
  {
    "objectID": "modeles/mod_CAH.html#introduction",
    "href": "modeles/mod_CAH.html#introduction",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "",
    "text": "La classification hiérarchique ascendante est une méthode de partitionnement dont l’objectif est de construire une hiérarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant itérativement les groupes deux à deux."
  },
  {
    "objectID": "modeles/mod_CAH.html#principe-de-la-méthode",
    "href": "modeles/mod_CAH.html#principe-de-la-méthode",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Principe de la méthode",
    "text": "Principe de la méthode\nSoit x = {x₁, …, xₙ} l’ensemble d’observations à classer :\n\nÉtape initiale\nLes n individus x constituent chacun une classe singleton.\nFusion des plus proches\nOn calcule la distance deux à deux entre tous les individus, puis on réunit dans la même classe les deux individus les plus proches.\nItération\nOn recalcule la distance entre la nouvelle classe formée et les n–2 individus restants.\nOn fusionne à nouveau les deux éléments (classes ou individus) les plus proches.\nCe processus est itéré jusqu’à ce qu’il ne reste plus qu’une unique classe: x.\n\nRemarque :\nCritères d’agrégation\nPour pouvoir regrouper des parties de Ω :\n\nDissimilarité entre observations\nd(xi,xj) ∈ R+, xi,xj ∈ x,\nDissimilarité entre classes\nD(A,B) ∈ R+, où A,B ∈ P(x) avec A∩B =∅."
  },
  {
    "objectID": "modeles/mod_CAH.html#algorithme-de-cah",
    "href": "modeles/mod_CAH.html#algorithme-de-cah",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Algorithme de CAH",
    "text": "Algorithme de CAH\nData : x₁, …, xₙ\nInitialisation : définir la partition C⁽⁰⁾ constituée des singletons\nfor m = 1, …, K–1 do\n- Calculer les distances deux à deux entre les classes de la partition C⁽ᵐ⁻¹⁾ à l’aide de D\n- Former la partition C⁽ᵐ⁾ en regroupant les deux classes les plus proches selon D\nend\nResult : Hiérarchie indicée\nL’algorithme de la CAH fournit une hiérarchie indicée où D est l’indice si D est croissante."
  },
  {
    "objectID": "modeles/mod_CAH.html#distance-ou-dissimilarité-entre-les-classes",
    "href": "modeles/mod_CAH.html#distance-ou-dissimilarité-entre-les-classes",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Distance (ou dissimilarité) entre les classes",
    "text": "Distance (ou dissimilarité) entre les classes\nSoit A et B deux classes de Ω, on a les critères d’agrégation (linkages) suivants :\n\nCritère du saut minimum (single linkage)\n\\(D_m\\)(A,B) = min{d(x,y); x ∈ A, y ∈ B}.\nCritère du saut maximum (complete linkage)\n\\(D_M\\)(A,B) = max{d(x,y); x ∈ A, y ∈ B}.\nCritère de la distance moyenne (average linkage)\n\\(D_a(A,B) = \\frac{\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)}{n_A\\,n_B}\\)\noù \\(n_A\\)=card(A) et \\(n_B\\)=card(B)\nCritère de Ward (Ward linkage)\n\\(D_W(A, B) = \\frac{n_A\\,n_B}{n_A + n_B}\\,d^2(\\mu_A, \\mu_B)\\)\noù \\(µ_A=\\frac{\\sum_{x∈A}x}{n_A}\\) et \\(µ_B=\\frac{\\sum_{y∈B}y}{n_B}\\)"
  },
  {
    "objectID": "modeles/mod_CAH.html#dendrogramme",
    "href": "modeles/mod_CAH.html#dendrogramme",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Dendrogramme",
    "text": "Dendrogramme\nUn dendrogramme est un type de diagramme arborescent utilisé pour représenter visuellement le résultat d’une analyse de regroupement hiérarchique (clustering hiérarchique).\nLes branches représentent les fusions successives de groupes d’objets selon leur similarité ou leur distance.\n\nRègle de la plus grande distance verticale\nLa règle de la plus grande distance verticale (aussi appelée “règle de l’élagage maximal”) est une méthode visuelle simple pour déterminer le nombre optimal de clusters à partir d’un dendrogramme.\nEtapes de la règle :\n\nTracer le dendrogramme.\nRepérer les hauteurs des lignes verticales.\nIdentifier la plus grande distance verticale sans ligne horizontale qui la coupe : C’est-à-dire la plus grande “barre verticale libre”.\nTracer une ligne horizontale qui coupe cette barre.\nLe nombre optimal de clusters est alors égal au nombre de branches coupées par cette ligne horizontale.\n\n\n\navantages, limites et spécificité de l’algorithme CAH\n\navantages : pas besoin de spécifier K à l’avance, visualisation hiérarchique.\nlimites: complexité quadratique, sensible à l’échelle des variables.\nspécificité : Structure arborescente\n\n\n\nApplication directe\n\n# Étape 1 : Données originales\ndata &lt;- data.frame(\n  Employé    = c(\"E1\", \"E2\", \"E3\", \"E4\", \"E5\"),\n  Ancienneté = c(2, 3, 5, 6, 8),\n  Salaire    = c(2000, 2100, 3500, 4100, 10000)\n)\n\n# Étape 2 : Matrice des caractéristiques\nX &lt;- data[, c(\"Ancienneté\", \"Salaire\")]\n\n# Étape 3 : Clustering hiérarchique (Ward)\nd &lt;- dist(X)\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n# Étape 4 : Affichage du dendrogramme avec ylim plus grand\nplot(hc, labels = data$Employé,\n     main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Employé\", ylab = \"Distance\",\n     ylim = c(0, 10000))  # Limite supérieure augmentée\n\n# Hauteur de coupure\nn_clusters &lt;- 2\nheight_cut &lt;- (sort(hc$height, decreasing = TRUE)[n_clusters - 1] +\n               sort(hc$height, decreasing = TRUE)[n_clusters]) / 1.35\nabline(h = height_cut, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n# Étape 5 : Clusters\nclusters &lt;- cutree(hc, k = n_clusters)\ndata$Cluster &lt;- as.factor(clusters)\n\n# Affichage final\nprint(data)\n\n  Employé Ancienneté Salaire Cluster\n1      E1          2    2000       1\n2      E2          3    2100       1\n3      E3          5    3500       1\n4      E4          6    4100       1\n5      E5          8   10000       2"
  },
  {
    "objectID": "modeles/mod_kmeans.html",
    "href": "modeles/mod_kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L’algorithme des K-means appelées aussi méthode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On considère que les observations x= {𝑥₁, .., 𝑥ₙ} sont décrites par p variables (𝑥ₙ appartient à ℝᵖ) et que l’espace est munie est de la métrique (généralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le même poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#introduction",
    "href": "modeles/mod_kmeans.html#introduction",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L’algorithme des K-means appelées aussi méthode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On considère que les observations x= {𝑥₁, .., 𝑥ₙ} sont décrites par p variables (𝑥ₙ appartient à ℝᵖ) et que l’espace est munie est de la métrique (généralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le même poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "href": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "title": "Algorithme des K-means",
    "section": "Algorithme de Lloyd (k-means)",
    "text": "Algorithme de Lloyd (k-means)\n\nAlgorithme des k-means\nLa méthode des centres mobiles a pour but de fournir une partition de x= {𝑥₁, .., 𝑥ₙ}.\n\nPrincipe : Pour l’ensemble d’observations 𝑥 = {𝑥₁, .., 𝑥ₙ}, on suppose qu’il existe\n𝐶ᴷ = {𝐶₁, .., 𝐶ₖ} tel que 𝑥 = ⋃ₖ₌₁ᴷ 𝐶ₖ est disjoint\n\n\nEtape 1. On commence par choisir K observations différentes : {μ₁, .., μₖ}\nEtape 2. On réalise itérativement une succession ces actions:\n\nPour chaque observation x= {𝑥₁, .., 𝑥ₙ} on trouve son centre (barycentre) le plus proche pour créer 𝐶ₖ = {ensemble d’observations les plus proches du centre μₖ}\nDans chaque nouvelle classe 𝐶ₖ on définit le nouveau centre de classe μₖ comme étant le barycentre de 𝐶ₖ.\n\nEtape 3. on réitère l’étape 1.\n\n\n\nRemarques\n\nL’algorithme s’arrête lorsque la partition des classes ne change plus (critère de convergence)\nl’algorithme ne s’applique que sur des données quantitatives\nsi les barycentres sont donnés, on commence l’algorithme avec ces barycentres.\n\n\n\navantage, limite et spécificité de l’algorithme\n\navantage : algorithme rapide\nlimites: besoin de spécifier le nombre de classes K\nspécificité : minimisation de l’inertie intra-classe"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-1",
    "href": "modeles/mod_kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "Exemple 1",
    "text": "Exemple 1\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ₁=1 et μ₂=7\nItération 1\nEtape 1 : µ₁=1 et μ₂=7 sont donnés par hypothèse\nEtape 2 : Commençons par calculer la distance de chaque élément de x par rapport à µ₁=1 et μ₂=7\n\n# Fonction calculant les distances à un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numérique\n  # mu : valeur numérique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu1 &lt;- 1\nmu2 &lt;- 7\ndistances1 &lt;- distance_au_centre(x, mu1)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu2)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu1\", \"_mu2\"))\nprint(df_merged)\n\n  points distance_mu1 distance_mu2\n1      1            0            6\n2      2            1            5\n3      9            8            2\n4     12           11            5\n5     20           19           13\n\n\nEtape 2 : les classes sont telles que la distance d(x,µ₁) d(x,μ₂) soit minimale. Donc les classes sont {1,2} et {9,12,20}.\nItération 2  :\nEtape 1 : on détermine les centres de chaque classe\n\nmu3 &lt;- mean(c(1, 2))\nmu4 &lt;- mean(c(9, 12, 20))\nprint(mu3)\n\n[1] 1.5\n\nprint(mu4)\n\n[1] 13.66667\n\n\nEtape 2 : Commençons par calculer la distance de chaque élément de x par rapport à µ₁=1,5 et μ₂=13,67\n\n# Fonction calculant les distances à un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numérique\n  # mu : valeur numérique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu3 &lt;-1.5\nmu4 &lt;-13.67\n\ndistances1 &lt;- distance_au_centre(x, mu3)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu4)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu3\", \"_mu4\"))\nprint(df_merged)\n\n  points distance_mu3 distance_mu4\n1      1          0.5        12.67\n2      2          0.5        11.67\n3      9          7.5         4.67\n4     12         10.5         1.67\n5     20         18.5         6.33\n\n\nEtape 2 : les classes sont telles que la distance d(x,µ3) d(x,μ4) soit minimale. Le regroupement ne change pas de la 1ère à la 2ème itération (critère de convergence) donc les classes sont {1,2} et {9,12,20}.\nSous R, la fonction kmeans permet directement de réaliser le regroupement selon l’algorithme des k-means.\n\n# données\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des données par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-2",
    "href": "modeles/mod_kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =12 et µ3 =20\n\n# données\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des données par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res2$centers)\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "modeles/mod_kmeans.html#inertie-intra-classe",
    "href": "modeles/mod_kmeans.html#inertie-intra-classe",
    "title": "Algorithme des K-means",
    "section": "Inertie intra-classe",
    "text": "Inertie intra-classe\n\nDéfinition\n\nL’inertie intra-classe de x = {x1,..,xn} par rapport à la partition \\[C_K = \\{\\,C_1,\\,C_2,\\,\\dots,\\,C_K\\} \\] et les centres des classes dans µ = {µ1,..,µK} s’écrit: IW =\\(\\sum_{k=1}^K d²(xi, µk)\\) Notons-la IW(C,µ) à partir de maintenant.\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n[1] 5\n\n\n\n\nPropriétés\n\nL’algorithme de Lloyd permet de diminuer l’inertie intra-classe IW à chaque itération. Ainsi : \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)}) \\ge \\mathrm{IW}(C^{(m+1)}, \\mu^{(m)})\\)\nLa suite numérique \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)})\\) est stationnaire(i.e.,à partir d’un certain nombre d’itérations,elle prend toujours la même valeur).\n\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =9, µ3 =12 et µ4 =20.\n\n# données\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des données par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res3$centers)\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n[1] 0.5"
  },
  {
    "objectID": "modeles/mod_kmeans.html#méthode-du-coude",
    "href": "modeles/mod_kmeans.html#méthode-du-coude",
    "title": "Algorithme des K-means",
    "section": "Méthode du coude",
    "text": "Méthode du coude\nLe critère du coude est une méthode visuelle consistant à tracer l’inertie intra de k-means en fonction de k et à choisir le nombre de clusters correspondant au point d’inflexion de la courbe.\n\n# vecteur de valeurs de k à tester\nks    &lt;- 2:4\n\n# pré-allocation d’un vecteur pour stocker l’inertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# tracé de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reliés par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"Méthode du coude pour le choix de k\")\n\n\n\n\n\n\n\n\nConclusion : La méthode du coude privilégie donc 3 clusters qui permettent de capturer la plus grande partie de l’inertie intra-classe. En effet, ajouter une 4ème classe, n’apporte qu’un faible gain d’inertie intra-classe."
  },
  {
    "objectID": "presentation/definition.html",
    "href": "presentation/definition.html",
    "title": "Définition",
    "section": "",
    "text": "Classification « supervisée » signifie une classification guidée par des étiquettes (ou “superviseur”) qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d’un magasin en clients “fidèles” ou “occasionnels”.\nAinsi, une classification non supervisée est une classification non guidée et où l’algorithme découvre lui-même la structure des données et détermine lui-même le regroupement, sans utiliser de “superviseur”. Exemple : établir une classification des clients (ou clustering) selon les montants dépensés dans un magasin sans préciser à l’algorithme si les clients fidèles ou occasionnels."
  },
  {
    "objectID": "presentation/definition.html#introduction",
    "href": "presentation/definition.html#introduction",
    "title": "Définition",
    "section": "",
    "text": "Classification « supervisée » signifie une classification guidée par des étiquettes (ou “superviseur”) qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d’un magasin en clients “fidèles” ou “occasionnels”.\nAinsi, une classification non supervisée est une classification non guidée et où l’algorithme découvre lui-même la structure des données et détermine lui-même le regroupement, sans utiliser de “superviseur”. Exemple : établir une classification des clients (ou clustering) selon les montants dépensés dans un magasin sans préciser à l’algorithme si les clients fidèles ou occasionnels."
  }
]