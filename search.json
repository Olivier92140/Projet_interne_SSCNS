[
  {
    "objectID": "historique.html",
    "href": "historique.html",
    "title": "Définition",
    "section": "",
    "text": "Voici un bref historique des grandes étapes de la classification non supervisée :\n\nAnnées 1930 – Naissance de la classification hiérarchique John Tukey et d’autres statisticiens proposent des méthodes de regroupement basées sur la similarité (dendrogrammes) pour organiser les données sans labels.\n\n\n\n\n\n\n\nNote\n\n\n\n\nen 1958: Apparition de la classification ascendante hiérarchique dans les années 1950-60 (notamment grâce à Sokal & Michener).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nen 1965 – Algorithme des nuées dynamiques  Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisé par James MacQueen en 1967 : partitionnement d’un jeu de points en k groupes en minimisant la variance intra-groupe.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nen 1970 – Modèles à mélanges et l’algorithme EM  Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) généralisent les modèles de mélange gaussien, entraînés via l’Expectation–Maximization, pour estimer des sous-populations au sein d’un jeu de données.\n\n\n\n\nAnnées 1980 – Cartes auto-organisatrices  Teuvo Kohonen développe les Self-Organizing Maps, réseaux de neurones non supervisés qui projettent des données de haute dimension en cartes topologiques 2D. L’objectif est de projeter des données d’un espace de très haute dimension en 2D tout en conservant, autant que possible, la structure topologique des données.\nAnnées 1990 – Clustering spectral et plus de flexibilité  Des méthodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarité) permettent de détecter des structures non sphériques. Parallèlement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnées 2000 – Big Data et densité  en 1996, l’algorithme DBSCAN (Density-Based Spatial Clustering of Applications with Noise) gagne en popularité. en 2002, Ester et al. précisent l’algorithme DBSCAN pour détecter les zones de forte densité dans de grands volumes de données. L’algorithme DBSCAN permet detecter les formes non convexes (chaînes, anneaux, …) et d’identifier automatiquement le bruit.\nAnnées 2010 – Deep clustering  L’émergence du deep learning (sous-famille du machine learning) entraîne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanément des représentations (via auto-encodeurs) et des partitions.En effet, par rapport les méthodes de clustering (k-means, DBSCAN, spectral…) qui opèrent sur des représentations fixes des données, le deep clustering simplifie la structure des clusters et améliore le partitionnement. Le deep clustering peut être utiliser par exemple dans le domaine de l’industrie pour assurer la maintenance prédictive pour repérer les dysfonctionnements anormaux avant panne.\n\nConclusion :\nCette évolution illustre comment la classification non supervisée est passée de méthodes statistiques simples (hiérarchique, k-means) à des approches sophistiquées mêlant représentations profondes et techniques de graphe."
  },
  {
    "objectID": "historique.html#historique",
    "href": "historique.html#historique",
    "title": "Définition",
    "section": "",
    "text": "Voici un bref historique des grandes étapes de la classification non supervisée :\n\nAnnées 1930 – Naissance de la classification hiérarchique John Tukey et d’autres statisticiens proposent des méthodes de regroupement basées sur la similarité (dendrogrammes) pour organiser les données sans labels.\n\n\n\n\n\n\n\nNote\n\n\n\n\nen 1958: Apparition de la classification ascendante hiérarchique dans les années 1950-60 (notamment grâce à Sokal & Michener).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nen 1965 – Algorithme des nuées dynamiques  Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisé par James MacQueen en 1967 : partitionnement d’un jeu de points en k groupes en minimisant la variance intra-groupe.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nen 1970 – Modèles à mélanges et l’algorithme EM  Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) généralisent les modèles de mélange gaussien, entraînés via l’Expectation–Maximization, pour estimer des sous-populations au sein d’un jeu de données.\n\n\n\n\nAnnées 1980 – Cartes auto-organisatrices  Teuvo Kohonen développe les Self-Organizing Maps, réseaux de neurones non supervisés qui projettent des données de haute dimension en cartes topologiques 2D. L’objectif est de projeter des données d’un espace de très haute dimension en 2D tout en conservant, autant que possible, la structure topologique des données.\nAnnées 1990 – Clustering spectral et plus de flexibilité  Des méthodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarité) permettent de détecter des structures non sphériques. Parallèlement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnées 2000 – Big Data et densité  en 1996, l’algorithme DBSCAN (Density-Based Spatial Clustering of Applications with Noise) gagne en popularité. en 2002, Ester et al. précisent l’algorithme DBSCAN pour détecter les zones de forte densité dans de grands volumes de données. L’algorithme DBSCAN permet detecter les formes non convexes (chaînes, anneaux, …) et d’identifier automatiquement le bruit.\nAnnées 2010 – Deep clustering  L’émergence du deep learning (sous-famille du machine learning) entraîne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanément des représentations (via auto-encodeurs) et des partitions.En effet, par rapport les méthodes de clustering (k-means, DBSCAN, spectral…) qui opèrent sur des représentations fixes des données, le deep clustering simplifie la structure des clusters et améliore le partitionnement. Le deep clustering peut être utiliser par exemple dans le domaine de l’industrie pour assurer la maintenance prédictive pour repérer les dysfonctionnements anormaux avant panne.\n\nConclusion :\nCette évolution illustre comment la classification non supervisée est passée de méthodes statistiques simples (hiérarchique, k-means) à des approches sophistiquées mêlant représentations profondes et techniques de graphe."
  },
  {
    "objectID": "exemples/CAH.html",
    "href": "exemples/CAH.html",
    "title": "Algorithme CAH utilisant R",
    "section": "",
    "text": "Les données représentent les dépenses de clients dans un magasin selon 3 catégories: alimentaire, habillement et électronique.\nEnjeu : On souhaite identifier des typologies de clients présentant des comportements similaires pour adapter l’offre à chaque poste de dépenses identifié.\nProblématique : Comment regrouper de manière homogène une clientèle selon ses habitudes de consommation dans trois domaines : l’alimentaire, l’habillement, et l’électronique ?\n\n\n\n\nCode\ndata &lt;- read.csv2(\"clients_depenses.csv\")\nhead(data)\n\n\n    Client Alimentaire Habillement Électronique\n1 Client_1      539.73      373.28      1110.77\n2 Client_2      488.93      288.71      1025.71\n3 Client_3      551.81      303.37       982.65\n4 Client_4      621.84      228.76       954.83\n5 Client_5      481.26      272.78       778.22\n6 Client_6      491.26      305.54       892.02\n\n\n\n\n\n\n\nCode\n# Extraction des données numériques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n\n\nCode\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la méthode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n\n\nCode\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n\nCode\n# Découpage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de départ\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\n\n\nCode\ninstall.packages(\"ggplot2\")\n\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\n\nCode\nlibrary(ggplot2)\n# Étape 1 : Réaliser l'ACP sur les colonnes numériques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# Étape 2 : Extraire les coordonnées sur les deux premières composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n\n                    PC1        PC2\nAlimentaire  -0.6625029  0.2006243\nHabillement   0.3893867  0.9152965\nÉlectronique  0.6398968 -0.3492596"
  },
  {
    "objectID": "exemples/CAH.html#exemple",
    "href": "exemples/CAH.html#exemple",
    "title": "Algorithme CAH utilisant R",
    "section": "",
    "text": "Les données représentent les dépenses de clients dans un magasin selon 3 catégories: alimentaire, habillement et électronique.\nEnjeu : On souhaite identifier des typologies de clients présentant des comportements similaires pour adapter l’offre à chaque poste de dépenses identifié.\nProblématique : Comment regrouper de manière homogène une clientèle selon ses habitudes de consommation dans trois domaines : l’alimentaire, l’habillement, et l’électronique ?\n\n\n\n\nCode\ndata &lt;- read.csv2(\"clients_depenses.csv\")\nhead(data)\n\n\n    Client Alimentaire Habillement Électronique\n1 Client_1      539.73      373.28      1110.77\n2 Client_2      488.93      288.71      1025.71\n3 Client_3      551.81      303.37       982.65\n4 Client_4      621.84      228.76       954.83\n5 Client_5      481.26      272.78       778.22\n6 Client_6      491.26      305.54       892.02\n\n\n\n\n\n\n\nCode\n# Extraction des données numériques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n\n\nCode\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la méthode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n\n\nCode\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n\nCode\n# Découpage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de départ\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\n\n\nCode\ninstall.packages(\"ggplot2\")\n\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\n\nCode\nlibrary(ggplot2)\n# Étape 1 : Réaliser l'ACP sur les colonnes numériques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# Étape 2 : Extraire les coordonnées sur les deux premières composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n\n                    PC1        PC2\nAlimentaire  -0.6625029  0.2006243\nHabillement   0.3893867  0.9152965\nÉlectronique  0.6398968 -0.3492596"
  },
  {
    "objectID": "exemples/CAH.html#conclusion",
    "href": "exemples/CAH.html#conclusion",
    "title": "Algorithme CAH utilisant R",
    "section": "Conclusion",
    "text": "Conclusion\nLa CAH permet de regrouper les 20 clients en 3 segments de consommation distincts. Ces clusters peuvent correspondre à :\n\nCluster 1 (rouge) : Ces clients correspondent à des gros consommateurs en électronique. Pour une entreprise, il est intéressant de cibler de type de client pour leur proposer des offres intéressantes ou des nouveautés.\nCluster 2 (vert) : Ce sont des clients orientés alimentaire et l’habillement, peu consommateurs d’électronique. Pour une entreprise, il est intéressant de cibler de type de client pour leur proposer par exemple des cartes de fidélité.\nCluster 3 (bleu) : Ce sont des clients à fort intérêt pour l’habillement, avec des comportements mixtes sur les autres postes. Pour une entreprise, il est intéressant de cibler de type de client pour leur proposer des promotions ou des nouvelles gammes."
  },
  {
    "objectID": "exemples/kmeans_observable.html",
    "href": "exemples/kmeans_observable.html",
    "title": "K-means avec Observable",
    "section": "",
    "text": "On commence par importer la bibliothèque observablehq\n\n\nCode\nimport { Inputs } from \"@observablehq/stdlib\"\nimport { Plot } from \"@observablehq/plot\""
  },
  {
    "objectID": "exemples/kmeans_observable.html#import-des-bibliothèques-observable",
    "href": "exemples/kmeans_observable.html#import-des-bibliothèques-observable",
    "title": "K-means avec Observable",
    "section": "",
    "text": "On commence par importer la bibliothèque observablehq\n\n\nCode\nimport { Inputs } from \"@observablehq/stdlib\"\nimport { Plot } from \"@observablehq/plot\""
  },
  {
    "objectID": "exemples/kmeans_observable.html#génération-des-notes-dans-quatre-matières",
    "href": "exemples/kmeans_observable.html#génération-des-notes-dans-quatre-matières",
    "title": "K-means avec Observable",
    "section": "Génération des notes dans quatre matières",
    "text": "Génération des notes dans quatre matières\nDes notes de quatre matières sont générés selon une loi uniforme pour 30 élèves: par exemple les notes de math sont approximativement autour de 14.\n\n\nCode\n// 1. Données initiales : notes sur 4 matières\ninitial = Array.from({length:30}, (_, i) =&gt; ({\n  Etudiant: `E${String(i+1).padStart(2,\"0\")}`,\n  Maths:   +(14   + 2   * (Math.random() - 0.5)).toFixed(1),\n  Physique:+(12   + 3   * (Math.random() - 0.5)).toFixed(1),\n  Info:    +(15   + 2.5 * (Math.random() - 0.5)).toFixed(1),\n  Chimie:  +(11   + 3   * (Math.random() - 0.5)).toFixed(1)\n}))"
  },
  {
    "objectID": "exemples/kmeans_observable.html#affichage-du-tableau-des-notes",
    "href": "exemples/kmeans_observable.html#affichage-du-tableau-des-notes",
    "title": "K-means avec Observable",
    "section": "Affichage du tableau des notes",
    "text": "Affichage du tableau des notes\n\n\nCode\nviewof data = Inputs.table(initial, {\n  columns: [\"Etudiant\", \"Maths\", \"Physique\", \"Info\", \"Chimie\"],\n  label: \"Éditez les notes des étudiants\"\n})"
  },
  {
    "objectID": "exemples/kmeans_observable.html#curseur-pour-faire-varier-le-nombre-de-clusters",
    "href": "exemples/kmeans_observable.html#curseur-pour-faire-varier-le-nombre-de-clusters",
    "title": "K-means avec Observable",
    "section": "Curseur pour faire varier le nombre de clusters",
    "text": "Curseur pour faire varier le nombre de clusters\n\n\nCode\nviewof k = Inputs.range([1, 6], {\n  value: 3,\n  step: 1,\n  label: \"Nombre de clusters (k)\"\n})\n\n// 4. Calcul du k-means\npoints = data.map(d =&gt; ({\n  x: +d.Maths,      // abscisse = Mathématiques\n  y: +d.Chimie,     // ordonnée = Chimie\n  name: d.Etudiant\n}))\n\nfunction kmeans(points, k, maxIter = 50) {\n  let centroids = points\n    .slice()\n    .sort(() =&gt; Math.random() - 0.5)  // le k-mean choisit son centre au début de manière aléatoire\n    .slice(0, k)\n    .map(p =&gt; ({x: p.x, y: p.y}))\n  let assignments = new Array(points.length)\n\n  for (let iter = 0; iter &lt; maxIter; iter++) {\n    // Affectation\n    for (let i = 0; i &lt; points.length; i++) {\n      let best = 0, bestDist = Infinity\n      for (let j = 0; j &lt; k; j++) {\n        const dx = points[i].x - centroids[j].x\n        const dy = points[i].y - centroids[j].y\n        const d2 = dx*dx + dy*dy\n        if (d2 &lt; bestDist) { bestDist = d2; best = j }\n      }\n      assignments[i] = best\n    }\n    // Recalcul des centroïdes\n    const sums = Array.from({length: k}, () =&gt; ({x:0,y:0,count:0}))\n    for (let i = 0; i &lt; points.length; i++) {\n      const c = assignments[i]\n      sums[c].x += points[i].x\n      sums[c].y += points[i].y\n      sums[c].count++\n    }\n    let moved = false\n    for (let j = 0; j &lt; k; j++) {\n      if (!sums[j].count) continue\n      const nx = sums[j].x / sums[j].count\n      const ny = sums[j].y / sums[j].count\n      if (nx !== centroids[j].x || ny !== centroids[j].y) moved = true\n      centroids[j].x = nx; centroids[j].y = ny\n    }\n    if (!moved) break\n  }\n\n  return {centroids, assignments}\n}\n\nresult = kmeans(points, k)\ncentroids = result.centroids\nassignments = result.assignments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataWithCluster = data.map((d,i) =&gt; ({\n  ...d,\n  Cluster: `Cluster ${assignments[i]+1}`\n}))\n\n\n// 6. Tableau des centroïdes\ncentroidsWithLabel = centroids.map((c,i) =&gt; ({\n  Cluster: `Cluster ${i+1}`,\n  Centre_Maths: c.x,\n  Centre_Chimie: c.y\n}))"
  },
  {
    "objectID": "exemples/kmeans_observable.html#représentation-graphique",
    "href": "exemples/kmeans_observable.html#représentation-graphique",
    "title": "K-means avec Observable",
    "section": "Représentation graphique",
    "text": "Représentation graphique\nIci, les notes des élèves en mathématiques et en chimie, sont représentées dans le plan\n\n\nCode\n// 7. Affichage du scatter : Maths vs Chimie\nPlot.plot({\n  width: 600,\n  height: 400,\n  x: {label: \"Mathématiques\"},\n  y: {label: \"Chimie\"},\n  marks: [\n    // points\n    Plot.dot(\n      points.map((p,i) =&gt; ({...p, cluster: String(assignments[i])})),\n      {x:\"x\", y:\"y\", fill:\"cluster\", title:\"name\"}\n    ),\n    // croix = centroïdes\n    Plot.dot(\n      centroids.map((c,i) =&gt; ({x:c.x, y:c.y, cluster:String(i)})),\n      {x:\"x\", y:\"y\", stroke:\"black\", symbol:\"cross\", strokeWidth:2}\n    )\n  ]\n})\n\n\n\n\n\n\n\n\nInterprétation graphique\nPlus les points sont vers la droite, plus les élèves sont bons en maths.\nPlus les points sont vers la haut, plus les élèves sont bons en chimie."
  },
  {
    "objectID": "exemples/kmeans_observable.html#tableau-des-notes-avec-les-clusters",
    "href": "exemples/kmeans_observable.html#tableau-des-notes-avec-les-clusters",
    "title": "K-means avec Observable",
    "section": "Tableau des notes avec les clusters",
    "text": "Tableau des notes avec les clusters\n\n\nCode\nviewof clusterTable = Inputs.table(dataWithCluster, {\n  columns: [\"Etudiant\",\"Maths\",\"Physique\",\"Info\",\"Chimie\",\"Cluster\"],\n  label: \"Profils et clusters\"\n})\n\nviewof centroidTable = Inputs.table(centroidsWithLabel, {\n  columns: [\"Cluster\",\"Centre_Maths\",\"Centre_Chimie\"],\n  label: \"Centres des clusters\"\n})"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classification non supervisée",
    "section": "",
    "text": "Une classification non supervisée est une classification non guidée, c’est-à-dire qui ne précise pas selon quel groupe la classification doit se faire. L’algorithme découvre donc lui-même la structure des données et détermine lui-même le regroupement.\nExemple : établir une classification des clients (ou clustering) selon les montants dépensés dans un magasin sans préciser à l’algorithme si les clients fidèles ou occasionnels."
  },
  {
    "objectID": "index.html#définition",
    "href": "index.html#définition",
    "title": "Classification non supervisée",
    "section": "",
    "text": "Une classification non supervisée est une classification non guidée, c’est-à-dire qui ne précise pas selon quel groupe la classification doit se faire. L’algorithme découvre donc lui-même la structure des données et détermine lui-même le regroupement.\nExemple : établir une classification des clients (ou clustering) selon les montants dépensés dans un magasin sans préciser à l’algorithme si les clients fidèles ou occasionnels."
  },
  {
    "objectID": "modeles/comparaison.html",
    "href": "modeles/comparaison.html",
    "title": "Comparaison des modèles de classification non supervisée",
    "section": "",
    "text": "Ce tableau compare les techniques de classification non supervisée en donnant leurs avantages et leurs inconvénients.\n\n\n\n\n\n\n\n\nMéthode\nAvantages\nInconvénients\n\n\n\n\nK-means\n- Simple et rapide à implémenter- Efficace pour de grands volumes de données- Interprétation intuitive des centres de clusters\n- Nécessite de fixer le nombre de clusters à l’avance- Sensible aux valeurs initiales et aux outliers- Ne capture pas les formes non sphériques\n\n\nCAH\n- Ne nécessite pas de fixer un nombre de clusters à l’avance- Représentation visuelle via le dendrogramme- Fonctionne bien sur petits jeux de données\n- Complexité élevée pour grands jeux de données- Une fusion ne peut pas être annulée- Sensible aux choix de distance et de méthode d’agrégation\n\n\nModèle de mélange\n- Probabiliste : attribue des probabilités d’appartenance- Peut capturer des formes complexes (non sphériques)- Estimation des paramètres via l’algorithme EM\n- Plus complexe à mettre en œuvre- Peut converger vers des optima locaux- Choix du nombre de classes critique (critères BIC/AIC nécessaires)"
  },
  {
    "objectID": "modeles/mod_CAH.html",
    "href": "modeles/mod_CAH.html",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "",
    "text": "Définition\n\n\n\nLa classification hiérarchique ascendante est une méthode de partitionnement dont l’objectif est de construire une hiérarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant itérativement les groupes deux à deux.\n\n\nDans la pratique, la CAH, c’est comme si tu voulais ranger des objets (observations) qui se ressemblent dans des boîtes (clusters ou groupes ou classes)\nAu début, tu mets chaque objet dans sa propre boîte. Ensuite, tu regardes quelles boîtes contiennent les objets les plus proches, les plus similaires, et tu les rassembles à l’aide d’une distance qu’on appelle distance entre observations (généralement la distance euclidienne). Puis tu continues : à chaque étape, tu regroupes deux boîtes qui se ressemblent le plus, à l’aide de la distance entre groupes.\nÀ la fin, tu obtiens une grande pyramide de regroupements, un peu comme un arbre qu’on appelle un dendrogramme. Et tu peux choisir de couper l’arbre à un certain niveau (grâce à la plus longue distance verticale) pour former des groupes."
  },
  {
    "objectID": "modeles/mod_CAH.html#introduction",
    "href": "modeles/mod_CAH.html#introduction",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "",
    "text": "Définition\n\n\n\nLa classification hiérarchique ascendante est une méthode de partitionnement dont l’objectif est de construire une hiérarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant itérativement les groupes deux à deux.\n\n\nDans la pratique, la CAH, c’est comme si tu voulais ranger des objets (observations) qui se ressemblent dans des boîtes (clusters ou groupes ou classes)\nAu début, tu mets chaque objet dans sa propre boîte. Ensuite, tu regardes quelles boîtes contiennent les objets les plus proches, les plus similaires, et tu les rassembles à l’aide d’une distance qu’on appelle distance entre observations (généralement la distance euclidienne). Puis tu continues : à chaque étape, tu regroupes deux boîtes qui se ressemblent le plus, à l’aide de la distance entre groupes.\nÀ la fin, tu obtiens une grande pyramide de regroupements, un peu comme un arbre qu’on appelle un dendrogramme. Et tu peux choisir de couper l’arbre à un certain niveau (grâce à la plus longue distance verticale) pour former des groupes."
  },
  {
    "objectID": "modeles/mod_CAH.html#principe-de-la-méthode",
    "href": "modeles/mod_CAH.html#principe-de-la-méthode",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Principe de la méthode",
    "text": "Principe de la méthode\nSoit x = {x₁, …, xₙ} l’ensemble d’observations à classer :\n\nÉtape initiale\nLes n individus x constituent chacun une classe singleton.\nFusion des plus proches\nOn calcule la distance deux à deux entre tous les individus, puis on réunit dans la même classe les deux individus les plus proches.\nItération\nOn recalcule la distance entre la nouvelle classe formée et les n–2 individus restants.\nOn fusionne à nouveau les deux éléments (classes ou individus) les plus proches.\nCe processus est itéré jusqu’à ce qu’il ne reste plus qu’une unique classe: x.\n\nRemarque :\nCritères d’agrégation\nPour pouvoir regrouper des parties de Ω :\n\nDissimilarité entre observations\nd(xi,xj) ∈ R+, xi,xj ∈ x,\n\n\n\n\n\n\n\nNote\n\n\n\nLa distance entre observations le plus utilisée est la distance euclidienne.\n\n\n\nDissimilarité entre classes\nD(A,B) ∈ R+, où A,B ∈ P(x) avec A∩B =∅."
  },
  {
    "objectID": "modeles/mod_CAH.html#algorithme-de-cah",
    "href": "modeles/mod_CAH.html#algorithme-de-cah",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Algorithme de CAH",
    "text": "Algorithme de CAH\nData : x₁, …, xₙ\nInitialisation : définir la partition C⁽⁰⁾ constituée des singletons\nfor m = 1, …, K–1 do\n- Calculer les distances deux à deux entre les classes de la partition C⁽ᵐ⁻¹⁾ à l’aide de D\n- Former la partition C⁽ᵐ⁾ en regroupant les deux classes les plus proches selon D\nend\nResult : Hiérarchie indicée\nL’algorithme de la CAH fournit une hiérarchie indicée où D est l’indice si D est croissante."
  },
  {
    "objectID": "modeles/mod_CAH.html#distance-ou-dissimilarité-entre-les-classes",
    "href": "modeles/mod_CAH.html#distance-ou-dissimilarité-entre-les-classes",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Distance (ou dissimilarité) entre les classes",
    "text": "Distance (ou dissimilarité) entre les classes\nSoit A et B deux classes de Ω, on a les critères d’agrégation (linkages) suivants :\n\nCritère du saut minimum (single linkage)\n\\(D_m\\)(A,B) = min{d(x,y); x ∈ A, y ∈ B}.\nCritère du saut maximum (complete linkage)\n\\(D_M\\)(A,B) = max{d(x,y); x ∈ A, y ∈ B}.\nCritère de la distance moyenne (average linkage)\n\\(D_a(A,B) = \\frac{\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)}{n_A\\,n_B}\\)\noù \\(n_A\\)=card(A) et \\(n_B\\)=card(B)\nCritère de Ward (Ward linkage)\n\\(D_W(A, B) = \\frac{n_A\\,n_B}{n_A + n_B}\\,d^2(\\mu_A, \\mu_B)\\)\noù \\(µ_A=\\frac{\\sum_{x∈A}x}{n_A}\\) et \\(µ_B=\\frac{\\sum_{y∈B}y}{n_B}\\)"
  },
  {
    "objectID": "modeles/mod_CAH.html#dendrogramme",
    "href": "modeles/mod_CAH.html#dendrogramme",
    "title": "Classification Ascendante Hiérarchique (CAH)",
    "section": "Dendrogramme",
    "text": "Dendrogramme\nUn dendrogramme est un type de diagramme arborescent utilisé pour représenter visuellement le résultat d’une analyse de regroupement hiérarchique (clustering hiérarchique).\nLes branches représentent les fusions successives de groupes d’objets selon leur similarité ou leur distance.\n\nRègle de la plus grande distance verticale\nLa règle de la plus grande distance verticale (aussi appelée “règle de l’élagage maximal”) est une méthode visuelle simple pour déterminer le nombre optimal de clusters à partir d’un dendrogramme.\nEtapes de la règle :\n\nTracer le dendrogramme.\nRepérer les hauteurs des lignes verticales.\nIdentifier la plus grande distance verticale sans ligne horizontale qui la coupe : C’est-à-dire la plus grande “barre verticale libre”.\nTracer une ligne horizontale qui coupe cette barre.\nLe nombre optimal de clusters est alors égal au nombre de branches coupées par cette ligne horizontale.\n\n\n\navantages, limites et spécificité de l’algorithme CAH\n\navantages : pas besoin de spécifier K à l’avance, visualisation hiérarchique.\nlimites: complexité quadratique, sensible à l’échelle des variables.\nspécificité : Structure arborescente\n\n\n\nApplication directe\n\n\nCode\n# Étape 1 : Données originales\ndata &lt;- data.frame(\n  Employé    = c(\"E1\", \"E2\", \"E3\", \"E4\", \"E5\"),\n  Ancienneté = c(2, 3, 5, 6, 8),\n  Salaire    = c(2000, 2100, 3500, 4100, 10000)\n)\n\n# Étape 2 : Matrice des caractéristiques\nX &lt;- data[, c(\"Ancienneté\", \"Salaire\")]\n\n# Étape 3 : Clustering hiérarchique (Ward)\nd &lt;- dist(X)\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n# Étape 4 : Affichage du dendrogramme avec ylim plus grand\nplot(hc, labels = data$Employé,\n     main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Employé\", ylab = \"Distance\",\n     ylim = c(0, 10000))  # Limite supérieure augmentée\n\n# Hauteur de coupure\nn_clusters &lt;- 2\nheight_cut &lt;- (sort(hc$height, decreasing = TRUE)[n_clusters - 1] +\n               sort(hc$height, decreasing = TRUE)[n_clusters]) / 1.35\nabline(h = height_cut, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nCode\n# Étape 5 : Clusters\nclusters &lt;- cutree(hc, k = n_clusters)\ndata$Cluster &lt;- as.factor(clusters)\n\n# Affichage final\nprint(data)\n\n\n  Employé Ancienneté Salaire Cluster\n1      E1          2    2000       1\n2      E2          3    2100       1\n3      E3          5    3500       1\n4      E4          6    4100       1\n5      E5          8   10000       2"
  },
  {
    "objectID": "quizz.html",
    "href": "quizz.html",
    "title": "Quiz : Classification non supervisée",
    "section": "",
    "text": "Le k-means est une méthode :\nchoix:\n  - supervisée de classification\n  - non supervisée qui minimise la variance intra-classe\n  - basée sur des arbres de décision\n  - de régression linéaire\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 2\n\n\n\n\n\n\nDans la classification hiérarchique ascendante (CAH), on commence par :\nchoix:\n  - Regrouper les observations les plus éloignées\n  - Créer des groupes arbitraires\n  - Considérer chaque observation comme un cluster individuel\n  - Tirer un échantillon aléatoire\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 3\n\n\n\n\n\n\nLe critère de Ward utilisé en CAH vise à :\nchoix:\n  - Maximiser la distance entre les clusters\n  - Minimiser l’augmentation de l’inertie intra-classe\n  - Maximiser la probabilité conditionnelle\n  - Réduire la dimension des données\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 2\n\n\n\n\n\n\nLes modèles de mélange gaussiens reposent sur :\nchoix:\n  - Une combinaison pondérée de lois normales\n  - La classification supervisée\n  - L’existence d’une variable latente indiquant l’appartenance au groupe\n  - L’algorithme de classification ascendante\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponses 1 et 3\n\n\n\n\n\n\nDans un modèle de mélange gaussien, que représente la probabilité \\(\\pi_k\\) ?\nchoix:\n  - La variance de la k-ième composante\n  - La probabilité a priori d’appartenir à la composante k\n  - La distance entre deux centres\n  - La probabilité conditionnelle de l'observation donnée la composante $k$\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 2"
  },
  {
    "objectID": "quizz.html#quiz-sur-la-classification-non-supervisée",
    "href": "quizz.html#quiz-sur-la-classification-non-supervisée",
    "title": "Quiz : Classification non supervisée",
    "section": "",
    "text": "Le k-means est une méthode :\nchoix:\n  - supervisée de classification\n  - non supervisée qui minimise la variance intra-classe\n  - basée sur des arbres de décision\n  - de régression linéaire\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 2\n\n\n\n\n\n\nDans la classification hiérarchique ascendante (CAH), on commence par :\nchoix:\n  - Regrouper les observations les plus éloignées\n  - Créer des groupes arbitraires\n  - Considérer chaque observation comme un cluster individuel\n  - Tirer un échantillon aléatoire\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 3\n\n\n\n\n\n\nLe critère de Ward utilisé en CAH vise à :\nchoix:\n  - Maximiser la distance entre les clusters\n  - Minimiser l’augmentation de l’inertie intra-classe\n  - Maximiser la probabilité conditionnelle\n  - Réduire la dimension des données\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 2\n\n\n\n\n\n\nLes modèles de mélange gaussiens reposent sur :\nchoix:\n  - Une combinaison pondérée de lois normales\n  - La classification supervisée\n  - L’existence d’une variable latente indiquant l’appartenance au groupe\n  - L’algorithme de classification ascendante\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponses 1 et 3\n\n\n\n\n\n\nDans un modèle de mélange gaussien, que représente la probabilité \\(\\pi_k\\) ?\nchoix:\n  - La variance de la k-ième composante\n  - La probabilité a priori d’appartenir à la composante k\n  - La distance entre deux centres\n  - La probabilité conditionnelle de l'observation donnée la composante $k$\n\n\n\n\n\n\nRéponse\n\n\n\n\n\nRéponse 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Classification non supervisée",
    "section": "",
    "text": "MacQueen, J. (1967). Some Methods for Classification and Analysis of Multivariate Observations.\n➤ Article qui introduit l’algorithme des k-means.\nDempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society: Series B, 39(1), 1–38.\n➤ Fondement de l’algorithme EM utilisé pour estimer les paramètres dans les modèles de mélange."
  },
  {
    "objectID": "about.html#articles-fondateurs",
    "href": "about.html#articles-fondateurs",
    "title": "Classification non supervisée",
    "section": "",
    "text": "MacQueen, J. (1967). Some Methods for Classification and Analysis of Multivariate Observations.\n➤ Article qui introduit l’algorithme des k-means.\nDempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society: Series B, 39(1), 1–38.\n➤ Fondement de l’algorithme EM utilisé pour estimer les paramètres dans les modèles de mélange."
  },
  {
    "objectID": "about.html#vidéos",
    "href": "about.html#vidéos",
    "title": "Classification non supervisée",
    "section": "Vidéos",
    "text": "Vidéos\n\nClassification ascendante hiérarchique sur exemple simple\nClassification ascendante hiérarchique et application en python"
  },
  {
    "objectID": "about.html#exemples-de-code-r-ou-python",
    "href": "about.html#exemples-de-code-r-ou-python",
    "title": "Classification non supervisée",
    "section": "Exemples de code R ou Python",
    "text": "Exemples de code R ou Python\n\nClustering K-Means dans R\nModèle de mélange gaussien\n\n\nRessources en ligne\n\nDocumentation officielle scikit-learn – Clustering\n➤ Tutoriels Python : k-means, DBSCAN, GMM, etc.\nR-bloggers – Articles sur le clustering\n➤ Tutoriels en R sur les méthodes de classification non supervisée.\nCours ENSAI / ENSAE – Classification non supervisée\n➤ Supports de cours en statistique pour étudiants et chercheurs."
  },
  {
    "objectID": "modeles/mod_kmeans.html",
    "href": "modeles/mod_kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Définition\n\n\n\nL’algorithme des K-means, également appelé méthode des centres mobiles ou algorithme de Lloyd, est une méthode de classification non supervisée visant à regrouper un ensemble d’observations en K groupes homogènes, appelés clusters. Le but est de déterminer une partition des données telle que les observations appartenant à un même groupe soient similaires entre elles, et différentes des observations des autres groupes. On considère que les observations \\(x= {x_₁, .., x_ₙ}\\) sont décrites par p variables (\\(x_n\\) appartient à ℝᵖ) et que l’espace est munie est de la métrique (généralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le même poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#introduction",
    "href": "modeles/mod_kmeans.html#introduction",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Définition\n\n\n\nL’algorithme des K-means, également appelé méthode des centres mobiles ou algorithme de Lloyd, est une méthode de classification non supervisée visant à regrouper un ensemble d’observations en K groupes homogènes, appelés clusters. Le but est de déterminer une partition des données telle que les observations appartenant à un même groupe soient similaires entre elles, et différentes des observations des autres groupes. On considère que les observations \\(x= {x_₁, .., x_ₙ}\\) sont décrites par p variables (\\(x_n\\) appartient à ℝᵖ) et que l’espace est munie est de la métrique (généralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le même poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "href": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "title": "Algorithme des K-means",
    "section": "Algorithme de Lloyd (k-means)",
    "text": "Algorithme de Lloyd (k-means)\n\nAlgorithme des k-means\nLa méthode des centres mobiles a pour but de fournir une partition de \\(x= {x_1, .., x_n}\\).\n\nPrincipe : Pour l’ensemble d’observations \\(x= {x_1, .., x_1}\\), on suppose qu’il existe\n\n\n\\(C^k = {C_n, .., C_k}\\) tel que \\(x = ⋃ₖ₌₁ᴷ C_k\\) est disjoint\n\n\nEtape 1. On commence par choisir K observations différentes : \\({μ_1, .., μ_n}\\)\nEtape 2. On réalise itérativement une succession ces actions:\n\nPour chaque observation \\(x= {x_1, .., x_n}\\) on trouve son centre (barycentre) le plus proche pour créer \\(C_k\\) = {ensemble d’observations les plus proches du centre \\(μ_k\\)}\nDans chaque nouvelle classe Cₖ on définit le nouveau centre de classe \\(μ_k\\) comme étant le barycentre de \\(C_k\\).\n\nEtape 3. on réitère l’étape 1.\n\n\n\nRemarques\n\nL’algorithme s’arrête lorsque la partition des classes ne change plus (critère de convergence)\nl’algorithme ne s’applique que sur des données quantitatives\nsi les barycentres sont donnés, on commence l’algorithme avec ces barycentres.\n\n\n\navantage, limite et spécificité de l’algorithme\n\navantage : algorithme rapide\nlimites: besoin de spécifier le nombre de classes K\nspécificité : minimisation de l’inertie intra-classe"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-1",
    "href": "modeles/mod_kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "Exemple 1",
    "text": "Exemple 1\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ₁=1 et μ₂=7\nItération 1\nEtape 1 : µ₁=1 et μ₂=7 sont donnés par hypothèse\nEtape 2 : Commençons par calculer la distance de chaque élément de x par rapport à µ₁=1 et μ₂=7\n\n\nCode\n# Fonction calculant les distances à un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numérique\n  # mu : valeur numérique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu1 &lt;- 1\nmu2 &lt;- 7\ndistances1 &lt;- distance_au_centre(x, mu1)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu2)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu1\", \"_mu2\"))\nprint(df_merged)\n\n\n  points distance_mu1 distance_mu2\n1      1            0            6\n2      2            1            5\n3      9            8            2\n4     12           11            5\n5     20           19           13\n\n\nEtape 2 : les classes sont telles que la distance d(x,µ₁) d(x,μ₂) soit minimale. Donc les classes sont {1,2} et {9,12,20}.\nItération 2  :\nEtape 1 : on détermine les centres de chaque classe\n\n\nCode\nmu3 &lt;- mean(c(1, 2))\nmu4 &lt;- mean(c(9, 12, 20))\nprint(mu3)\n\n\n[1] 1.5\n\n\nCode\nprint(mu4)\n\n\n[1] 13.66667\n\n\nEtape 2 : Commençons par calculer la distance de chaque élément de x par rapport à µ₁=1,5 et μ₂=13,67\n\n\nCode\n# Fonction calculant les distances à un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numérique\n  # mu : valeur numérique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu3 &lt;-1.5\nmu4 &lt;-13.67\n\ndistances1 &lt;- distance_au_centre(x, mu3)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu4)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu3\", \"_mu4\"))\nprint(df_merged)\n\n\n  points distance_mu3 distance_mu4\n1      1          0.5        12.67\n2      2          0.5        11.67\n3      9          7.5         4.67\n4     12         10.5         1.67\n5     20         18.5         6.33\n\n\nEtape 2 : les classes sont telles que la distance d(x,µ3) d(x,μ4) soit minimale. Le regroupement ne change pas de la 1ère à la 2ème itération (critère de convergence) donc les classes sont {1,2} et {9,12,20}.\nSous R, la fonction kmeans permet directement de réaliser le regroupement selon l’algorithme des k-means.\n\n\nCode\n# données\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des données par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\n\nÉléments par classe :\n\n\nCode\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n\nCode\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\n\nCentres finaux :\n\n\nCode\nprint(res1$centers)\n\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nl’inertie intra classe est égale à :\n\n\nCode\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n\n[1] 65.16667"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-2",
    "href": "modeles/mod_kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =12 et µ3 =20\n\n\nCode\n# données\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des données par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\n\nÉléments par classe :\n\n\nCode\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n\nCode\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\n\nCentres finaux :\n\n\nCode\nprint(res2$centers)\n\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "modeles/mod_kmeans.html#inertie-intra-classe",
    "href": "modeles/mod_kmeans.html#inertie-intra-classe",
    "title": "Algorithme des K-means",
    "section": "Inertie intra-classe",
    "text": "Inertie intra-classe\n\nDéfinition\n\nL’inertie intra-classe de x = {x1,..,xn} par rapport à la partition \\[C_K = \\{\\,C_1,\\,C_2,\\,\\dots,\\,C_K\\} \\] et les centres des classes dans \\(µ = {µ_1,..,µ_K}\\) s’écrit: IW =\\(\\sum_{k=1}^K d²(xi, µk)\\) Notons-la IW(C,µ) à partir de maintenant.\n\nl’inertie intra classe est égale à :\n\n\nCode\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n\n[1] 5\n\n\n\n\nPropriétés\n\nL’algorithme de Lloyd permet de diminuer l’inertie intra-classe IW à chaque itération. Ainsi : \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)}) \\ge \\mathrm{IW}(C^{(m+1)}, \\mu^{(m)})\\)\nLa suite numérique \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)})\\) est stationnaire(i.e.,à partir d’un certain nombre d’itérations,elle prend toujours la même valeur).\n\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =9, µ3 =12 et µ4 =20.\n\n\nCode\n# données\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des données par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\n\nÉléments par classe :\n\n\nCode\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n\nCode\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\n\nCentres finaux :\n\n\nCode\nprint(res3$centers)\n\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nl’inertie intra classe est égale à :\n\n\nCode\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n\n[1] 0.5"
  },
  {
    "objectID": "modeles/mod_kmeans.html#méthode-du-coude",
    "href": "modeles/mod_kmeans.html#méthode-du-coude",
    "title": "Algorithme des K-means",
    "section": "Méthode du coude",
    "text": "Méthode du coude\nLe critère du coude est une méthode visuelle consistant à tracer l’inertie intra de k-means en fonction de k et à choisir le nombre de clusters correspondant au point d’inflexion de la courbe.\n\n\nCode\n# vecteur de valeurs de k à tester\nks    &lt;- 2:4\n\n# pré-allocation d’un vecteur pour stocker l’inertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# tracé de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reliés par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"Méthode du coude pour le choix de k\")\n\n\n\n\n\n\n\n\n\nConclusion : La méthode du coude privilégie donc 3 clusters qui permettent de capturer la plus grande partie de l’inertie intra-classe. En effet, ajouter une 4ème classe, n’apporte qu’un faible gain d’inertie intra-classe."
  },
  {
    "objectID": "modeles/mod_mmelange.html",
    "href": "modeles/mod_mmelange.html",
    "title": "Loi de mélange",
    "section": "",
    "text": "La classification par modèles de mélange est une approche probabiliste. L’idée est de modéliser la distribution des données observées par un mélange de distributions de probabilité. On définit une classe comme l’ensemble des observations issues de la même composante du mélange. Ainsi, chaque loi élémentaire génère une partie des points."
  },
  {
    "objectID": "modeles/mod_mmelange.html#introduction",
    "href": "modeles/mod_mmelange.html#introduction",
    "title": "Loi de mélange",
    "section": "",
    "text": "La classification par modèles de mélange est une approche probabiliste. L’idée est de modéliser la distribution des données observées par un mélange de distributions de probabilité. On définit une classe comme l’ensemble des observations issues de la même composante du mélange. Ainsi, chaque loi élémentaire génère une partie des points."
  },
  {
    "objectID": "modeles/mod_mmelange.html#loi-de-mélange",
    "href": "modeles/mod_mmelange.html#loi-de-mélange",
    "title": "Loi de mélange",
    "section": "Loi de mélange",
    "text": "Loi de mélange\n\n\n\n\n\n\nDéfinition\n\n\n\nUne loi de mélange \\(p\\) à \\(K\\) composantes et à support sur un espace \\(\\mathcal{X}\\) est une combinaison linéaire de \\(K\\) lois de probabilité \\(p_1, \\ldots, p_K\\) sur \\(\\mathcal{X}\\).\n\n\nAinsi, il existe \\(K\\) coefficients \\(\\pi_1, \\ldots, \\pi_K\\), avec \\(\\pi_k &gt; 0\\) et \\(\\sum_{k=1}^K \\pi_k = 1\\), tels que, pour tout mesurable \\(A \\subset \\mathcal{X}\\), on a\n\\[\np(A ; \\theta) = \\sum_{k=1}^K \\pi_k p_k(A ; \\alpha_k), \\tag{1}\n\\]\noù \\(\\theta = \\left\\{ \\{\\pi_k, \\alpha_k \\} : k = 1, \\ldots, K \\right\\}\\) groupe les paramètres du modèle et \\(\\alpha_k\\) groupe les paramètres de la composante \\(k\\).\nSi elle existe, on peut écrire la densité d’une loi de mélange comme :\n\\[\nf(x ; \\theta) = \\sum_{k=1}^K \\pi_k f_k(x ; \\alpha_k), \\tag{2}\n\\]\npour tout \\(x \\in \\mathcal{X}\\), où \\(f_k\\) est la densité associée à \\(p_k\\)."
  },
  {
    "objectID": "modeles/mod_mmelange.html#modélisation",
    "href": "modeles/mod_mmelange.html#modélisation",
    "title": "Loi de mélange",
    "section": "Modélisation",
    "text": "Modélisation\nSoient \\(x_1, \\ldots, x_n\\) des observations correspondant à \\(n\\) individus issus d’une population \\(\\Omega\\) composée par \\(K\\) sous-populations disjointes :\n\\[\n\\Omega = \\{1, \\ldots, n\\} = C_1 \\cup \\ldots \\cup C_K \\quad \\leftarrow \\text{Vrai classement}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nOn modélise les observations \\(x_1, \\ldots, x_n\\) comme des réalisations de \\(n\\) variables aléatoires i.i.d. \\(\\; X_1, \\ldots, X_n\\).\n\n\nOn définit \\(Z_i = (Z_{i1}, \\ldots, Z_{iK})\\) par :\n\\[\nZ_{ik} =\n\\begin{cases}\n1 & \\text{si } i \\in C_k \\\\\n0 & \\text{sinon}\n\\end{cases}\n\\quad \\text{avec} \\quad \\mathbb{P}(Z_{ik} = 1) = \\pi_k\n\\]\nprobabilité a priori d’appartenance à \\(C_k\\).\nD’après la définition de \\(Z_i\\) : \\(\\sum_{k=1}^K \\pi_k = 1\\), et puis, par construction,\n\\[\nZ_i \\sim \\mathcal{M}(1; \\pi_1, \\ldots, \\pi_K). \\quad \\text{(Loi multinomiale)}\n\\]\nPour tout \\(i \\in \\{1, \\ldots, n\\}\\), on modélise par \\(p_k(\\cdot)\\) la loi de \\(X_i \\mid \\{Z_{ik} = 1\\}\\) et par \\(p(\\cdot)\\) la loi de \\(X_i\\), i.e.\n\\[\np_k(A) = \\mathbb{P}(X_i \\in A \\mid Z_{ik} = 1), \\\\\np(A) = \\mathbb{P}(X_i \\in A),\n\\]\npour tout \\(A\\) mesurable dans \\(\\mathcal{X}\\). En utilisant la formule des probabilités totales, on peut écrire :\n\\[\np(A) = \\mathbb{P}(X_i \\in A) = \\sum_{k=1}^K \\mathbb{P}(X_i \\in A \\mid Z_{ik} = 1)\\mathbb{P}(Z_{ik} = 1) = \\sum_{k=1}^K \\pi_k \\mathbb{P}(X_i \\in A \\mid Z_{ik} = 1).\n\\]\nPar conséquent, on a :\n\\[\np(A) = \\sum_{k=1}^K \\pi_k p_k(A), \\tag{3}\n\\]\npour tout mesurable \\(A \\subset \\mathcal{X}\\).\nFinalement, en dénotant les \\(p_k(\\cdot)\\) et \\(p(\\cdot)\\) en fonction de leurs paramètres, on peut réécrire (3) comme :\n\\[\np(A, \\theta) = \\sum_{k=1}^K \\pi_k p_k(A ; \\alpha_k)\n\\quad \\text{où} \\quad \\theta = \\left\\{ \\{\\pi_k, \\alpha_k\\} : \\; k = 1, \\ldots, K \\right\\}, \\tag{4}\n\\]\nqui est une loi de mélange.\nEt la densité du mélange (fonction de masse) s’écrit comme une somme pondérée de lois conditionnelles :\n\\[\nf(x_i; \\theta) = \\sum_{k=1}^{K} \\mathbb{P}(Z_i = k) \\cdot \\mathbb{P}(X_i = x_i \\mid Z_i = k) = \\sum_{k=1}^{K} \\pi_k f_k(x_i; \\alpha_k)\n\\]\noù :\n\n\\(Z_i\\) est la variable latente (pas directement observable mais qui influence le modèle) indiquant la composante à laquelle appartient l’observation \\(x_i\\) ;\n\n\\(\\pi_k = \\mathbb{P}(Z_i = k)\\) est la probabilité a priori d’appartenir à la classe \\(k\\) ;\n\n\\(f_k(x_i; \\alpha_k)\\) est la densité conditionnelle dans la classe \\(k\\) avec les paramètres \\(\\alpha_k\\).\n\n\n\n\n\n\n\nNote\n\n\n\nAinsi les observations \\(x_1, \\ldots, x_n\\) sont modélisées pour être tirées d’une loi de mélange.\n\n\nEn résumé, ce modèle caractérise chaque sous-population \\(C_k\\) par :\n\n\\(\\pi_k\\), qui représente la proportion d’individus appartenant à la \\(k\\)-ième sous-population,\n\\(\\alpha_k\\), i.e. les paramètres de la distribution de \\(p_k\\).\n\n\nModèle de mélange le plus utilisé\nLe modèle de mélange le plus utilisé est le mélange gaussien :\nSi \\(\\mathcal{X} = \\mathbb{R}^d\\), le modèle le plus utilisé est le mélange gaussien. Ainsi\n\\[\nf(x_i ; \\theta) = \\sum_{k=1}^K \\pi_k \\, \\phi_d(x_i ; \\mu_k, \\Sigma_k),\n\\]\noù \\(\\theta\\) groupe tous les paramètres, \\(\\pi_k\\) est la proportion de la classe \\(k\\),\n\\(\\mu_k \\in \\mathbb{R}^d\\) est le centre de la classe \\(k\\) et \\(\\Sigma_k\\) est sa matrice de covariance.\nPrécisément :\n\n\nExemple: Mélange de 3 gaussiennes univariées\nLa densité d’un mélange de 3 gaussiennes univariées (ne dépendant que de x) s’écrit :\n\\[\nf(x_i; \\theta) = \\pi_1 f_1(x; \\alpha_1) + \\pi_2 f_2(x; \\alpha_2) + \\pi_3 f_3(x; \\alpha_3)\n\\]\noù :\n\\[\nf_k(x; \\alpha_k) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp\\left( -\\frac{(x - \\mu_k)^2}{2\\sigma_k^2} \\right)\n\\]\net :\n\\[\n\\theta = \\left\\{ \\{\\pi_k, \\alpha_k\\} : \\alpha_k = (\\mu_k, \\sigma_k) \\text{ pour } k = 1, \\ldots, 3 \\right\\}\n\\]\n\n\nA quoi sert les modèles de mélange?\n\n\n\n\n\n\nNote\n\n\n\nLes modèles de mélange permettent de reconstruire les données manquantes en estimant les valeurs manquantes (a posteriori)"
  },
  {
    "objectID": "modeles/mod_mmelange.html#vraisemblance",
    "href": "modeles/mod_mmelange.html#vraisemblance",
    "title": "Loi de mélange",
    "section": "Vraisemblance",
    "text": "Vraisemblance\nPour un modèle de mélange, on utilise une vraisemblance car en la maximisant, nous trouverons les valeurs des paramètres qui rendent les données les plus probables (valeurs optimales)\n\nVraisemblance incomplète du modèle\n\n\n\n\n\n\nDéfinition\n\n\n\nLa vraisemblance incomplète du modèle est :\n\\[\n\\mathcal{L}(\\theta; x) = \\prod_{i=1}^{n} f(x_i, \\theta) = \\prod_{i=1}^{n} \\sum_{k=1}^{K} \\pi_k f_k(x_i; \\alpha_k)\n\\]\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\net la log-vraisemblance incomplète est :\n\\[\n\\ell(\\theta; x) = \\sum_{i=1}^{n} \\log \\left( \\sum_{k=1}^{K} \\pi_k f_k(x_i; \\alpha_k) \\right)\n\\]\n\n\n\n\nVraisemblance complète du modèle\n\n\n\n\n\n\nDéfinition\n\n\n\nLa vraisemblance complète du modèle est :\n\\[\n\\mathcal{L}(\\theta; x, z) = \\prod_{i=1}^{n} \\prod_{k=1}^{K} \\left[ \\pi_k f_k(x_i; \\alpha_k) \\right]^{z_{ik}}\n\\]\noù \\(z_{ik} = 1\\) si l’observation \\(i\\) provient de la composante \\(k\\), \\(0\\) sinon.\n\n\n\n\n\n\n\n\nDéfinition\n\n\n\nDonc la log-vraisemblance complétée est :\n\\[\n\\ell(\\theta; x, z) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} z_{ik} \\left( \\log \\pi_k + \\log f_k(x_i; \\alpha_k) \\right)\n\\]"
  },
  {
    "objectID": "modeles/mod_mmelange.html#lalgorithme-em-expectation-maximisation",
    "href": "modeles/mod_mmelange.html#lalgorithme-em-expectation-maximisation",
    "title": "Loi de mélange",
    "section": "L’algorithme EM (Expectation Maximisation)",
    "text": "L’algorithme EM (Expectation Maximisation)\nL’algorithme EM est une méthode d’optimisation utilisée pour estimer les paramètres d’un modèle statistique lorsque les données sont incomplètes ou comportent des variables latentes.\n\nAlgorithme EM : définition\nL’algorithme EM est initialisé au paramètre \\(\\theta^{[0]}\\).\nIl itère entre les étapes E et M définies à l’itération \\(r\\) par :\n— Étape E : calcul des probabilités a posteriori \\(t_{ik}\\) sachant \\(\\theta^{[r-1]}\\)\n\\[\nt_{ik}(\\theta^{[r-1]}) = \\frac{\\pi_k^{[r-1]} f_k(x_i; \\alpha_k^{[r-1]})}{\\sum_{l=1}^{K} \\pi_l^{[r-1]} f_l(x_i; \\alpha_l^{[r-1]})}\n\\]\n— Étape M : maximisation de l’espérance de la log-vraisemblance complétée\n\\[\n\\theta^{[r]} = \\arg\\max_{\\alpha_k, \\pi_k} \\sum_{i=1}^{n} \\sum_{k=1}^{K} t_{ik}(\\theta^{[r-1]}) \\log \\left( \\pi_k f_k(x_i; \\alpha_k) \\right)\n\\]\nOn s’arrête lorsque les valeurs des paramètres ou de la quantité à maximiser entre deux itérations successives ne varient « presque » plus.\nL’algorithme EM permet de calculer les valeurs du paramètres qui maximise la vraisemblance à chaque itération.\n\n\nInterprétation\n\nAvant calcul des observations \\(x_n\\) :\nOn ne sait pas de quelle composante vient \\(x_n\\), on a seulement les probabilités a priori \\(\\pi_k\\)\nAprès calcul des observations \\(x_n\\) :\nOn ajuste nos croyances grâce à la vraisemblance \\(P(x_n \\mid \\theta_k)\\) et on obtient les probabilités a posteriori \\(t_{nk}\\)\nCes probabilités a posteriori sont essentielles pour « répartir » chaque donnée entre les composantes lors de l’estimation EM"
  },
  {
    "objectID": "modeles/mod_mmelange.html#application-distribution-de-mélange-de-poisson",
    "href": "modeles/mod_mmelange.html#application-distribution-de-mélange-de-poisson",
    "title": "Loi de mélange",
    "section": "Application : distribution de mélange de Poisson",
    "text": "Application : distribution de mélange de Poisson\n\nFonction de masse d’une observation \\(x_i\\)\nLa densité (fonction de masse) du mélange s’écrit :\n\\[\nf(x_i ; \\theta) = \\sum_{k=1}^{K} \\pi_k \\cdot \\frac{e^{-\\lambda_k} \\lambda_k^{x_i}}{x_i!}\n\\]\navec \\(\\sum_k\\) \\(\\pi_k = 1\\) , \\(\\pi_k &gt;0\\) et \\(\\lambda_k &gt; 0\\).\n\n\nfonctions de log-vraisemblance et log-vraisemblance complétée du modèle\n\nVraisemblance :\n\n\\[\n\\mathcal{L}(\\theta ; x) = \\prod_{i=1}^{n} \\left( \\sum_{k=1}^{K} \\pi_k \\cdot \\frac{e^{-\\lambda_k} \\lambda_k^{x_i}}{x_i!} \\right)\n\\]\n\nLog-vraisemblance :\n\n\\[\n\\ell(\\theta ; x) = \\sum_{i=1}^{n} \\log \\left( \\sum_{k=1}^{K} \\pi_k \\cdot \\frac{e^{-\\lambda_k} \\lambda_k^{x_i}}{x_i!} \\right)\n\\]\n\nLog-vraisemblance complétée :\n\n\\[\n\\ell(\\theta ; x, z) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} z_{ik} \\log(\\pi_k f_k(x_i ; \\lambda_k))\n= \\sum_{i=1}^{n} \\sum_{k=1}^{K} z_{ik} \\left( \\log \\pi_k - \\lambda_k + x_i \\log \\lambda_k - \\log(x_i!) \\right)\n\\]\nLe terme \\(\\log(x_i!)\\), étant constant par rapport à \\(\\theta\\), peut être ignoré lors de l’optimisation.\n\n\nAlgorithme EM\n\nÉtape E : calcul des probabilités a posteriori \\(t_{ik}\\) sachant \\(\\theta^{[r-1]}\\) :\n\n\\[\nt_{ik}(\\theta^{[r-1]}) =\n\\frac{\n  \\pi_k^{[r-1]} e^{-\\lambda_k^{[r-1]}} \\left( \\lambda_k^{[r-1]} \\right)^{x_i}\n}{\n  \\sum_{h=1}^{K} \\pi_h^{[r-1]} e^{-\\lambda_h^{[r-1]}} \\left( \\lambda_h^{[r-1]} \\right)^{x_i}\n}\n\\]\n\nInitialisation : \\(\\theta^{[0]}\\) = \\(\\left\\{ \\pi_k^{[0]}, \\lambda_k^{[0]} \\right\\}\\)\nÉtape M : Maximiser à chaque étape l’espérance conditionnelle de la log-vraisemblance complétée :\n\n\\[\nQ(\\theta ; \\theta^{[r-1]}) =\n\\sum_{i=1}^{n} \\sum_{k=1}^{K}\nt_{ik}(\\theta^{[r-1]}) \\left( \\log \\pi_k - \\lambda_k + x_i \\log \\lambda_k \\right)\n\\]\n\\[\n\\theta^{[r]} = \\arg \\max_{\\lambda_k, \\pi_k}\n\\sum_{i=1}^{n} \\sum_{k=1}^{K}\nt_{ik}(\\theta^{[r-1]}) \\left( \\log \\pi_k - \\lambda_k + x_i \\log \\lambda_k \\right)\n\\]\n\n\nValeurs des paramètres qui maximisent la vraisemblance à chaque itération\n\n\n1. Mise à jour des paramètres \\(\\lambda_k^{[r]}\\)\nOn commence par maximiser par rapport à \\(\\lambda_k\\), car cette optimisation ne dépend pas des contraintes sur les \\(\\pi_k\\), et les termes sont séparables pour chaque composante ( k ).\nOn veut maximiser pour chaque k :\n\\[\nL_k(\\lambda_k) = \\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) (-\\lambda_k + x_i \\log \\lambda_k)\n\\]\nOn dérive cette fonction :\n\\[\n\\frac{\\partial L_k}{\\partial \\lambda_k}\n= \\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) \\left( -1 + \\frac{x_i}{\\lambda_k} \\right)\n= -\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) + \\frac{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) x_i}{\\lambda_k}\n\\]\nOn annule la dérivée :\n\\[\n-\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) + \\frac{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) x_i}{\\lambda_k} = 0\n\\quad \\Rightarrow \\quad\n\\boxed{\n\\lambda_k^{[r]} = \\frac{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]}) x_i}{\\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]})}\n}\n\\]\n\n\n2. Mise à jour des poids \\(\\pi_k^{[r]}\\)\nOn cherche à maximiser sur \\((\\pi_1, \\pi_2, \\ldots, \\pi_K, \\phi)\\) la fonction :\n\\[\n\\sum_{k=1}^{K} \\left( \\sum_{i=1}^{n} t_{ik}(\\lambda_k^{[r-1]}) \\right) \\log \\pi_k\n\\quad \\text{ sous la contrainte } \\quad \\sum_{k=1}^{K} \\pi_k = 1\n\\]\nOn introduit un multiplicateur de Lagrange \\(\\phi\\) :\n\\[\n\\mathcal{L}(\\pi, \\phi) =\n\\sum_{k=1}^{K} \\left( \\sum_{i} t_{ik}(\\lambda_k^{[r-1]}) \\right) \\log \\pi_k\n- \\phi \\left( \\sum_{k=1}^{K} \\pi_k - 1 \\right)\n\\]\nOn dérive la fonction de Lagrange :\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi_k}\n= \\frac{\\sum_i t_{ik}(\\theta^{[r-1]})}{\\pi_k} - \\phi = 0\n\\quad \\Rightarrow \\quad\n\\pi_k = \\frac{\\sum_i t_{ik}(\\theta^{[r-1]})}{\\phi}\n\\]\nOr, sous la contrainte \\(\\sum_{k=1}^{K} \\pi_k = 1\\) :\n\\[\n\\phi = \\sum_{i=1}^{n} \\sum_{k=1}^{K} t_{ik}(\\theta^{[r-1]}) = n\n\\quad \\Rightarrow \\quad\n\\boxed{\n\\pi_k^{[r]} = \\frac{1}{n} \\sum_{i=1}^{n} t_{ik}(\\theta^{[r-1]})\n}\n\\]"
  },
  {
    "objectID": "modeles/mod_mmelange.html#critère-bic",
    "href": "modeles/mod_mmelange.html#critère-bic",
    "title": "Loi de mélange",
    "section": "Critère BIC",
    "text": "Critère BIC\nDu point de vue bayésien, un bon modèle est celui qui maximise la vraisemblance.\nDans ce cas, on utilise le critère BIC(Bayesian Information Criterion) :\n\\[\n\\mathrm{BIC}(m) \\;=\\; \\ell(\\hat\\theta_m; \\mathbf{x}) \\;-\\; \\frac{\\nu_m}{2}\\,\\ln n\n\\]\noù \\(\\nu_m\\) est le nombre de paramètres du modèle m et \\(\\hat\\theta_m\\) est l’estimateur du maximum de vraisemblance pour les paramètres du modèle m avec n est la taille de l’échantillon.\nPour comparer plusieurs modèles, on choisit celui qui maximise le BIC."
  },
  {
    "objectID": "modeles/mod_mmelange.html#critère-icl",
    "href": "modeles/mod_mmelange.html#critère-icl",
    "title": "Loi de mélange",
    "section": "Critère ICL",
    "text": "Critère ICL\nPour tenir compte de l’objectif de classification lors du choix de modèle, on peut utiliser le critère ICL(Integrated Completed Likelihood) :\n\\[\n\\mathrm{ICL}(m)\n\\;=\\;\n\\ell(\\hat\\theta_m; \\mathbf{x}, \\hat{\\mathbf{z}})\n\\;-\\;\n\\frac{\\nu_m}{2}\\,\\ln n\n\\;=\\;\n\\mathrm{BIC}(m)\n\\;+\\;\n\\sum_{i=1}^n \\sum_{k=1}^K \\hat z_{ik}\\,\\ln t_{ik}(\\hat\\theta_m)\n\\]\noù \\(\\hat{\\mathbf{z}}\\) est la partition donnée par la règle du MAP pour le modèle m et \\(t_{ik}(\\hat\\theta_m) = P(Z_i = k \\mid X_i = x_i; \\hat\\theta_m)\\) est la probabilité a posteriori."
  },
  {
    "objectID": "exemples/mmelange.html",
    "href": "exemples/mmelange.html",
    "title": "Modèles de mélange de gaussiennes",
    "section": "",
    "text": "Dans cet exemple, nous ajustons un modèle de mélange de lois normales à des données simulées, en utilisant le package R mclust. Le but est d’identifier automatiquement des groupes (ou classes) au sein des données."
  },
  {
    "objectID": "exemples/mmelange.html#introduction",
    "href": "exemples/mmelange.html#introduction",
    "title": "Modèles de mélange de gaussiennes",
    "section": "",
    "text": "Dans cet exemple, nous ajustons un modèle de mélange de lois normales à des données simulées, en utilisant le package R mclust. Le but est d’identifier automatiquement des groupes (ou classes) au sein des données."
  },
  {
    "objectID": "exemples/mmelange.html#préambule-simulation-des-données",
    "href": "exemples/mmelange.html#préambule-simulation-des-données",
    "title": "Modèles de mélange de gaussiennes",
    "section": "Préambule : Simulation des données",
    "text": "Préambule : Simulation des données\nOn simule 200 points selon des 2 lois normales\n\n\nCode\n# chargement des packages\nif (!requireNamespace(\"mclust\", quietly = TRUE)) {\n  install.packages(\"mclust\")\n}\nlibrary(mclust)\nset.seed(123)\n\n# Deux populations gaussiennes\nx1 &lt;- rnorm(100, mean = 0, sd = 1)\nx2 &lt;- rnorm(100, mean = 5, sd = 1.5)\n\n# Données mélangées\nx &lt;- c(x1, x2)"
  },
  {
    "objectID": "exemples/mmelange.html#densité-du-mélange",
    "href": "exemples/mmelange.html#densité-du-mélange",
    "title": "Modèles de mélange de gaussiennes",
    "section": "1. Densité du mélange",
    "text": "1. Densité du mélange\n\n\nCode\n# Densité d’un mélange à 2 composantes\ndensite_melange &lt;- function(x, pi, mu, sigma) {\n  pi[1] * dnorm(x, mean = mu[1], sd = sigma[1]) +\n  pi[2] * dnorm(x, mean = mu[2], sd = sigma[2])\n}"
  },
  {
    "objectID": "exemples/mmelange.html#données-complètes",
    "href": "exemples/mmelange.html#données-complètes",
    "title": "Modèles de mélange de gaussiennes",
    "section": "2. Données complètes",
    "text": "2. Données complètes\n\n\nCode\n# Construction de Z complet (connu car simulation)\nz &lt;- c(rep(1, 100), rep(2, 100))\n\n# Log-vraisemblance complète\nlog_vraisemblance_completee &lt;- function(x, z, pi, mu, sigma) {\n  sum(log(pi[z]) + dnorm(x, mean = mu[z], sd = sigma[z], log = TRUE))\n}"
  },
  {
    "objectID": "exemples/mmelange.html#estimation-par-em-via-mclust",
    "href": "exemples/mmelange.html#estimation-par-em-via-mclust",
    "title": "Modèles de mélange de gaussiennes",
    "section": "3. Estimation par EM via mclust",
    "text": "3. Estimation par EM via mclust\n\n\nCode\n# Estimation avec mclust\nres &lt;- Mclust(x, G = 2)  # G = nombre de composantes\n\n# Résumé des paramètres\nsummary(res)\n\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust V (univariate, unequal variance) model with 2 components: \n\n log-likelihood   n df      BIC       ICL\n      -441.9182 200  5 -910.328 -923.4324\n\nClustering table:\n  1   2 \n 96 104 \n\n\nConclusion : Sur les 200 points simulés, 96 ont été assignés au cluster 1, et 104 au cluster 2, selon la règle du maximum a posteriori \\(t_{ik} = P(Z_n = k \\mid X_n = x_n)\\). 96 points sont tels que la probabilité a posteriori \\(t_{i1} &gt; t_{i2}\\) et 104 points sont tels que \\(t_{i1} &lt; t_{i2}\\) avec \\(t_{i1} + t_{i2} = 1\\)"
  },
  {
    "objectID": "exemples/mmelange.html#affichage-de-la-densité-estimée",
    "href": "exemples/mmelange.html#affichage-de-la-densité-estimée",
    "title": "Modèles de mélange de gaussiennes",
    "section": "4. Affichage de la densité estimée",
    "text": "4. Affichage de la densité estimée\n\n\nCode\nplot(res, what = \"density\", main = \"Estimation du modèle de mélange par EM\")"
  },
  {
    "objectID": "exemples/CAH2.html",
    "href": "exemples/CAH2.html",
    "title": "Algorithme CAH utilisant Python",
    "section": "",
    "text": "Enjeu : On souhaite regrouper les pays selon leur PIB, leur espérance de vie et leur taux d’alphabétisation.\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\ndf = pd.read_csv(\"donnees_pays.csv\")\n\n\n\n\n\n\n\nCode\ndf = pd.read_csv(\"donnees_pays.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nPays\nPIB_par_hab\nEspérance_de_vie\nTaux_alphabétisation\n\n\n\n\n0\nFrance\n41463\n82.5\n99.0\n\n\n1\nAllemagne\n48649\n81.2\n99.0\n\n\n2\nBrésil\n9602\n75.0\n93.2\n\n\n3\nNigeria\n2229\n61.0\n62.0\n\n\n4\nChine\n12720\n77.3\n96.8\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nX = df[[\"PIB_par_hab\", \"Espérance_de_vie\", \"Taux_alphabétisation\"]]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\n\n\n\nCode\nZ = linkage(X_scaled, method=\"ward\")\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\ndendrogram(Z, labels=df[\"Pays\"].values, leaf_rotation=90)\nplt.axhline(\n    y=Z[-3, 2] + 0.5,\n    c='red',\n    linestyle='--',\n    label=\"Coupure à 3 clusters\"\n)\nplt.title(\"Dendrogramme - Classification Hiérarchique\")\nplt.ylabel(\"Distance\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf[\"Cluster\"] = fcluster(Z, t=3, criterion=\"maxclust\")\ndf[[\"Pays\", \"Cluster\"]]\n\n\n\n\n\n\n\n\n\nPays\nCluster\n\n\n\n\n0\nFrance\n3\n\n\n1\nAllemagne\n3\n\n\n2\nBrésil\n2\n\n\n3\nNigeria\n1\n\n\n4\nChine\n2\n\n\n5\nInde\n1\n\n\n6\nÉtats-Unis\n3\n\n\n7\nJapon\n3\n\n\n8\nAfrique du Sud\n1\n\n\n9\nMexique\n2\n\n\n10\nCanada\n3\n\n\n11\nItalie\n3\n\n\n12\nEspagne\n3\n\n\n13\nArgentine\n2\n\n\n14\nÉgypte\n1\n\n\n15\nRussie\n2\n\n\n16\nTurquie\n2\n\n\n17\nIndonésie\n2\n\n\n18\nCorée du Sud\n3\n\n\n19\nAustralie\n3\n\n\n\n\n\n\n\n\n\n\nLa CAH permet d’identifier 3 groupes de Pays:\n\nCluster 1 : Nigeria, Afrique du Sud, Inde, Égypte.\n\nCe sont les pays avec un développement faible (PIB par habitant plus faible, espérance de vie plus courte, taux d’alphabétisation relativement bas)\n\nCluster 2: Russie, Indonésie, Brésil, Argentine, Turquie, Chine, Mexique\n\nCe sont des pays émergents avec un niveau de développement économique moyen, une amélioration progressive de l’espérance de vie et de l’éducation,\net un fort potentiel économique.\n\nCluster 3 : États-Unis, Canada, Australie, Japon, Italie, Espagne, Corée du Sud, France, Allemagne\n\nCe sont les pays développés ayant un PIB par habitant élevé, une espérance de vie longue, et un taux d’alphabétisation proche de 100%."
  },
  {
    "objectID": "exemples/CAH2.html#exemple",
    "href": "exemples/CAH2.html#exemple",
    "title": "Algorithme CAH utilisant Python",
    "section": "",
    "text": "Enjeu : On souhaite regrouper les pays selon leur PIB, leur espérance de vie et leur taux d’alphabétisation.\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\ndf = pd.read_csv(\"donnees_pays.csv\")\n\n\n\n\n\n\n\nCode\ndf = pd.read_csv(\"donnees_pays.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nPays\nPIB_par_hab\nEspérance_de_vie\nTaux_alphabétisation\n\n\n\n\n0\nFrance\n41463\n82.5\n99.0\n\n\n1\nAllemagne\n48649\n81.2\n99.0\n\n\n2\nBrésil\n9602\n75.0\n93.2\n\n\n3\nNigeria\n2229\n61.0\n62.0\n\n\n4\nChine\n12720\n77.3\n96.8\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nX = df[[\"PIB_par_hab\", \"Espérance_de_vie\", \"Taux_alphabétisation\"]]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\n\n\n\nCode\nZ = linkage(X_scaled, method=\"ward\")\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\ndendrogram(Z, labels=df[\"Pays\"].values, leaf_rotation=90)\nplt.axhline(\n    y=Z[-3, 2] + 0.5,\n    c='red',\n    linestyle='--',\n    label=\"Coupure à 3 clusters\"\n)\nplt.title(\"Dendrogramme - Classification Hiérarchique\")\nplt.ylabel(\"Distance\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf[\"Cluster\"] = fcluster(Z, t=3, criterion=\"maxclust\")\ndf[[\"Pays\", \"Cluster\"]]\n\n\n\n\n\n\n\n\n\nPays\nCluster\n\n\n\n\n0\nFrance\n3\n\n\n1\nAllemagne\n3\n\n\n2\nBrésil\n2\n\n\n3\nNigeria\n1\n\n\n4\nChine\n2\n\n\n5\nInde\n1\n\n\n6\nÉtats-Unis\n3\n\n\n7\nJapon\n3\n\n\n8\nAfrique du Sud\n1\n\n\n9\nMexique\n2\n\n\n10\nCanada\n3\n\n\n11\nItalie\n3\n\n\n12\nEspagne\n3\n\n\n13\nArgentine\n2\n\n\n14\nÉgypte\n1\n\n\n15\nRussie\n2\n\n\n16\nTurquie\n2\n\n\n17\nIndonésie\n2\n\n\n18\nCorée du Sud\n3\n\n\n19\nAustralie\n3\n\n\n\n\n\n\n\n\n\n\nLa CAH permet d’identifier 3 groupes de Pays:\n\nCluster 1 : Nigeria, Afrique du Sud, Inde, Égypte.\n\nCe sont les pays avec un développement faible (PIB par habitant plus faible, espérance de vie plus courte, taux d’alphabétisation relativement bas)\n\nCluster 2: Russie, Indonésie, Brésil, Argentine, Turquie, Chine, Mexique\n\nCe sont des pays émergents avec un niveau de développement économique moyen, une amélioration progressive de l’espérance de vie et de l’éducation,\net un fort potentiel économique.\n\nCluster 3 : États-Unis, Canada, Australie, Japon, Italie, Espagne, Corée du Sud, France, Allemagne\n\nCe sont les pays développés ayant un PIB par habitant élevé, une espérance de vie longue, et un taux d’alphabétisation proche de 100%."
  },
  {
    "objectID": "exemples/kmeans.html",
    "href": "exemples/kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de données simulant les notes de 30 étudiants en 4 matières d’une filière ingénieur, applique le k-means pour segmenter les profils.\n\n\n\n\nCode\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.4), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\nnotes\n\n\n    Mathematiques Physique Informatique Chimie\nE01          12.9     13.3         15.9   14.0\nE02          13.5     11.1         13.8   12.6\nE03          17.1     14.7         14.2   11.7\nE04          14.1     14.6         12.6    9.1\nE05          14.3     14.5         12.4   15.1\nE06          17.4     14.1         15.7    9.2\nE07          14.9     13.7         16.1   17.6\nE08          11.5     11.8         15.1   15.6\nE09          12.6     11.1         17.2   10.3\nE10          13.1     10.9         19.9    7.9\nE11          16.4      9.9         13.8    8.9\nE12          14.7     11.4          9.5   11.8\nE13          14.8      8.2         17.4   10.3\nE14          14.2     18.5         13.3   10.0\nE15          12.9     15.6         13.3    8.1\nE16          17.6      8.6         17.5   10.9\nE17          15.0     10.8         14.3    8.6\nE18          10.1     10.6         12.1    6.0\nE19          15.4     14.3         15.4    9.9\nE20          13.1     11.7         14.7   13.8\nE21          11.9     12.8         15.0    9.3\nE22          13.6     11.9         15.9   12.8\nE23          11.9     11.9         14.1    6.1\nE24          12.5     16.1         16.5   10.8\nE25          12.7     11.3         14.5   12.6\nE26          10.6     16.5         15.8   11.9\nE27          15.7      7.4         17.6   11.3\nE28          14.3     13.8         16.0    9.1\nE29          11.7     12.4         14.2    8.5\nE30          16.5     12.6         17.8    7.9\n\n\n\n\n\nIci, on centre et réduit chaque variable comme la dispersion des notes de chaque matière est différente.Ainsi, les notes des matières pourront être comparables\n\n\nCode\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\n\n\nCode\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carrés intra-clusters\",\n     main = \"Méthode du coude pour choisir k\")\n\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\n\nCode\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\n\nK-means clustering with 3 clusters of sizes 9, 9, 12\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.2211544 -0.0946250   -0.4048354  1.1962307\n2    -0.8619349  0.7183503   -0.4582437 -0.6801298\n3     0.8123169 -0.4677940    0.6473093 -0.3870757\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  1   1   3   2   1   3   1   1   3   3   3   1   3   2   2   3   3   2   3   1 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  2   1   2   2   1   2   3   3   2   3 \n\nWithin cluster sum of squares by cluster:\n[1] 16.22915 20.59663 29.54569\n (between_SS / total_SS =  42.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\n\nCode\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\n\n\nCode\nsuppressMessages(suppressWarnings(\n  capture.output(install.packages(\"ggplot2\"), file = \"/dev/null\")\n))\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$Étudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = Étudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils étudiants\",\n       subtitle = \"Projection sur les deux premières composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\npca$rotation[, 1:2] \n\n\n                      PC1         PC2\nMathematiques  0.49868636 -0.34021342\nPhysique      -0.61219046 -0.21342747\nInformatique   0.61106222 -0.02010495\nChimie        -0.05601533 -0.91558688\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxième axe factoriel sépare les étudiants bons en chimie (en haut) des plus faibles en chimie (en bas)\n\n\n\nLes clusters issus de k-means se retrouvent bien séparés :\n\ncluster (rouge) : Ces étudiants sont bons en physique.\ncluster (vert) : Ces étudiants sont forts en Maths et informatique\ncluster (bleu) : Ces étudiants ont des performances en chimie."
  },
  {
    "objectID": "exemples/kmeans.html#exemple",
    "href": "exemples/kmeans.html#exemple",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de données simulant les notes de 30 étudiants en 4 matières d’une filière ingénieur, applique le k-means pour segmenter les profils.\n\n\n\n\nCode\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.4), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\nnotes\n\n\n    Mathematiques Physique Informatique Chimie\nE01          12.9     13.3         15.9   14.0\nE02          13.5     11.1         13.8   12.6\nE03          17.1     14.7         14.2   11.7\nE04          14.1     14.6         12.6    9.1\nE05          14.3     14.5         12.4   15.1\nE06          17.4     14.1         15.7    9.2\nE07          14.9     13.7         16.1   17.6\nE08          11.5     11.8         15.1   15.6\nE09          12.6     11.1         17.2   10.3\nE10          13.1     10.9         19.9    7.9\nE11          16.4      9.9         13.8    8.9\nE12          14.7     11.4          9.5   11.8\nE13          14.8      8.2         17.4   10.3\nE14          14.2     18.5         13.3   10.0\nE15          12.9     15.6         13.3    8.1\nE16          17.6      8.6         17.5   10.9\nE17          15.0     10.8         14.3    8.6\nE18          10.1     10.6         12.1    6.0\nE19          15.4     14.3         15.4    9.9\nE20          13.1     11.7         14.7   13.8\nE21          11.9     12.8         15.0    9.3\nE22          13.6     11.9         15.9   12.8\nE23          11.9     11.9         14.1    6.1\nE24          12.5     16.1         16.5   10.8\nE25          12.7     11.3         14.5   12.6\nE26          10.6     16.5         15.8   11.9\nE27          15.7      7.4         17.6   11.3\nE28          14.3     13.8         16.0    9.1\nE29          11.7     12.4         14.2    8.5\nE30          16.5     12.6         17.8    7.9\n\n\n\n\n\nIci, on centre et réduit chaque variable comme la dispersion des notes de chaque matière est différente.Ainsi, les notes des matières pourront être comparables\n\n\nCode\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\n\n\nCode\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carrés intra-clusters\",\n     main = \"Méthode du coude pour choisir k\")\n\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\n\nCode\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\n\nK-means clustering with 3 clusters of sizes 9, 9, 12\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.2211544 -0.0946250   -0.4048354  1.1962307\n2    -0.8619349  0.7183503   -0.4582437 -0.6801298\n3     0.8123169 -0.4677940    0.6473093 -0.3870757\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  1   1   3   2   1   3   1   1   3   3   3   1   3   2   2   3   3   2   3   1 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  2   1   2   2   1   2   3   3   2   3 \n\nWithin cluster sum of squares by cluster:\n[1] 16.22915 20.59663 29.54569\n (between_SS / total_SS =  42.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\n\nCode\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\n\n\nCode\nsuppressMessages(suppressWarnings(\n  capture.output(install.packages(\"ggplot2\"), file = \"/dev/null\")\n))\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$Étudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = Étudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils étudiants\",\n       subtitle = \"Projection sur les deux premières composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\npca$rotation[, 1:2] \n\n\n                      PC1         PC2\nMathematiques  0.49868636 -0.34021342\nPhysique      -0.61219046 -0.21342747\nInformatique   0.61106222 -0.02010495\nChimie        -0.05601533 -0.91558688\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxième axe factoriel sépare les étudiants bons en chimie (en haut) des plus faibles en chimie (en bas)\n\n\n\nLes clusters issus de k-means se retrouvent bien séparés :\n\ncluster (rouge) : Ces étudiants sont bons en physique.\ncluster (vert) : Ces étudiants sont forts en Maths et informatique\ncluster (bleu) : Ces étudiants ont des performances en chimie."
  }
]