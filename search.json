[
  {
    "objectID": "presentation/definition.html",
    "href": "presentation/definition.html",
    "title": "Définition",
    "section": "",
    "text": "Classification « supervisée » signifie une classification guidée par des étiquettes ou “superviseur” qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d’un magasin en “fidèle” et “occasionnel”.\nAinsi, une classification non supervisée est une classification non guidée et où l’algorithme découvre lui-même la structure des données et détermine lui-même le regroupement, sans « superviseur ». Exemple : établir un classification des clients (ou clustering) selon les montants dépensés dans un magasin sans préciser à l’algorithme quels sont les clients fidèles ou non fidèles."
  },
  {
    "objectID": "presentation/definition.html#introduction",
    "href": "presentation/definition.html#introduction",
    "title": "Définition",
    "section": "",
    "text": "Classification « supervisée » signifie une classification guidée par des étiquettes ou “superviseur” qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d’un magasin en “fidèle” et “occasionnel”.\nAinsi, une classification non supervisée est une classification non guidée et où l’algorithme découvre lui-même la structure des données et détermine lui-même le regroupement, sans « superviseur ». Exemple : établir un classification des clients (ou clustering) selon les montants dépensés dans un magasin sans préciser à l’algorithme quels sont les clients fidèles ou non fidèles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classification non supervisée",
    "section": "",
    "text": "External link to Quarto website\nR example\nPython example 🐍"
  },
  {
    "objectID": "index.html#des-liens-intéressants",
    "href": "index.html#des-liens-intéressants",
    "title": "Classification non supervisée",
    "section": "",
    "text": "External link to Quarto website\nR example\nPython example 🐍"
  },
  {
    "objectID": "modeles/mod_kmeans.html",
    "href": "modeles/mod_kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L’algorithme des K-means appelées aussi méthode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On considère que les observations x= {𝑥₁, .., 𝑥ₙ} sont décrites par p variables (𝑥ₙ appartient à ℝᵖ) et que l’espace est munie est de la métrique (généralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le même poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#introduction",
    "href": "modeles/mod_kmeans.html#introduction",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L’algorithme des K-means appelées aussi méthode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On considère que les observations x= {𝑥₁, .., 𝑥ₙ} sont décrites par p variables (𝑥ₙ appartient à ℝᵖ) et que l’espace est munie est de la métrique (généralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le même poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "href": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "title": "Algorithme des K-means",
    "section": "Algorithme de Lloyd (k-means)",
    "text": "Algorithme de Lloyd (k-means)\n\nAlgorithme des k-means\nLa méthode des centres mobiles a pour but de fournir une partition de x= {𝑥₁, .., 𝑥ₙ}.\n\nPrincipe : Pour l’ensemble d’observations 𝑥 = {𝑥₁, .., 𝑥ₙ}, on suppose qu’il existe\n𝐶ᴷ = {𝐶₁, .., 𝐶ₖ} tel que 𝑥 = ⋃ₖ₌₁ᴷ 𝐶ₖ est disjoint\n\n\nEtape 1. On commence par choisir K observations différentes : {μ₁, .., μₖ}\nEtape 2. On réalise itérativement une succession ces actions:\n\nPour chaque observation x= {𝑥₁, .., 𝑥ₙ} on trouve son centre (barycentre) le plus proche pour créer 𝐶ₖ = {ensemble d’observations les plus proches du centre μₖ}\nDans chaque nouvelle classe 𝐶ₖ on définit le nouveau centre de classe μₖ comme étant le barycentre de 𝐶ₖ.\n\nEtape 3. on réitère l’étape 1.\n\n\n\nRemarques\n\nL’algorithme s’arrête lorsque la partition des classes ne change plus (critère de convergence)\nl’algorithme ne s’applique que sur des données quantitatives\nsi les barycentres sont donnés, on commence l’algorithme avec ces barycentres."
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-1",
    "href": "modeles/mod_kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "Exemple 1",
    "text": "Exemple 1\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ₁=1 et μ₂=7\nItération 1\nEtape 1 : µ₁=1 et μ₂=7 sont donnés par hypothèse\nEtape 2 : Commençons par calculer la distance de chaque élément de x par rapport à µ₁=1 et μ₂=7\n\n# Fonction calculant les distances à un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numérique\n  # mu : valeur numérique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu1 &lt;- 1\nmu2 &lt;- 7\ndistances1 &lt;- distance_au_centre(x, mu1)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu2)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu1\", \"_mu2\"))\nprint(df_merged)\n\n  points distance_mu1 distance_mu2\n1      1            0            6\n2      2            1            5\n3      9            8            2\n4     12           11            5\n5     20           19           13\n\n\nEtape 2 : les classes sont telles que la distance d(x,µ₁) d(x,μ₂) soit minimale. Donc les classes sont {1,2} et {9,12,20}.\nItération 2  :\nEtape 1 : on détermine les centres de chaque classe\n\nmu3 &lt;- mean(c(1, 2))\nmu4 &lt;- mean(c(9, 12, 20))\nprint(mu3)\n\n[1] 1.5\n\nprint(mu4)\n\n[1] 13.66667\n\n\nEtape 2 : Commençons par calculer la distance de chaque élément de x par rapport à µ₁=1,5 et μ₂=13,67\n\n# Fonction calculant les distances à un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur numérique\n  # mu : valeur numérique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu3 &lt;-1.5\nmu4 &lt;-13.67\n\ndistances1 &lt;- distance_au_centre(x, mu3)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu4)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu3\", \"_mu4\"))\nprint(df_merged)\n\n  points distance_mu3 distance_mu4\n1      1          0.5        12.67\n2      2          0.5        11.67\n3      9          7.5         4.67\n4     12         10.5         1.67\n5     20         18.5         6.33\n\n\nEtape 2 : les classes sont telles que la distance d(x,µ3) d(x,μ4) soit minimale. Le regroupement ne change pas de la 1ère à la 2ème itération (critère de convergence) donc les classes sont {1,2} et {9,12,20}.\nSous R, la fonction kmeans permet directement de réaliser le regroupement selon l’algorithme des k-means.\n\n# données\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des données par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-2",
    "href": "modeles/mod_kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =12 et µ3 =20\n\n# données\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des données par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res2$centers)\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "modeles/mod_kmeans.html#inertie-intra-classe",
    "href": "modeles/mod_kmeans.html#inertie-intra-classe",
    "title": "Algorithme des K-means",
    "section": "Inertie intra-classe",
    "text": "Inertie intra-classe\n\nDéfinition\n\nL’inertie intra-classe de x = {x1,..,xn} par rapport à la partition \\[C_K = \\{\\,C_1,\\,C_2,\\,\\dots,\\,C_K\\} \\] et les centres des classes dans µ = {µ1,..,µK} s’écrit: IW =\\(\\sum_{k=1}^K d²(xi, µk)\\) Notons-la IW(C,µ) à partir de maintenant.\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n[1] 5\n\n\n\n\nPropriétés\n\nL’algorithme de Lloyd permet de diminuer l’inertie intra-classe IW à chaque itération. Ainsi : \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)}) \\ge \\mathrm{IW}(C^{(m+1)}, \\mu^{(m)})\\)\nLa suite numérique \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)})\\) est stationnaire(i.e.,à partir d’un certain nombre d’itérations,elle prend toujours la même valeur).\n\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =9, µ3 =12 et µ4 =20.\n\n# données\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des données par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res3$centers)\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n[1] 0.5"
  },
  {
    "objectID": "modeles/mod_kmeans.html#méthode-du-coude",
    "href": "modeles/mod_kmeans.html#méthode-du-coude",
    "title": "Algorithme des K-means",
    "section": "Méthode du coude",
    "text": "Méthode du coude\nLe critère du coude est une méthode visuelle consistant à tracer l’inertie intra de k-means en fonction de k et à choisir le nombre de clusters correspondant au point d’inflexion de la courbe.\n\n# vecteur de valeurs de k à tester\nks    &lt;- 2:4\n\n# pré-allocation d’un vecteur pour stocker l’inertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# tracé de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reliés par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"Méthode du coude pour le choix de k\")\n\n\n\n\n\n\n\n\nConclusion : La méthode du coude privilégie donc 3 clusters qui permettent de capturer la plus grande partie de l’inertie intra-classe. En effet, ajouter une 4ème classe, n’apporte qu’un faible gain d’inertie intra-classe."
  },
  {
    "objectID": "exemples/kmeans.html",
    "href": "exemples/kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ₁=1 et μ₂=7\n\n# données\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des données par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "exemples/kmeans.html#exemple-1",
    "href": "exemples/kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ₁=1 et μ₂=7\n\n# données\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des données par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "exemples/kmeans.html#exemple-2",
    "href": "exemples/kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =12 et µ3 =20\n\n# données\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des données par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res2$centers)\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "exemples/kmeans.html#propriétés",
    "href": "exemples/kmeans.html#propriétés",
    "title": "Algorithme des K-means",
    "section": "Propriétés",
    "text": "Propriétés\n\nInertie intra-classe\n\nL’inertie intra-classe de x = {x1,..,xn} par rapport à la partition C ={C1,..,CK} et les centres des classes dans µ = {µ1,..,µK} s’écrit: IW = d2(xi, µk) k=1xi∈Ck Notons-la IW(C,µ) à partir de maintenant.\n\nNotons-la IW(C,µ) à partir de maintenant.\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n[1] 5"
  },
  {
    "objectID": "exemples/kmeans.html#exemple-3",
    "href": "exemples/kmeans.html#exemple-3",
    "title": "Algorithme des K-means",
    "section": "Exemple 3",
    "text": "Exemple 3\nVoici un exemple de script R utilisant l’algorithme k-means pour trouver les classes (les clusters) des données x={1, 2, 9, 12, 20} avec les barycentres µ1 =1, µ2 =9, µ3 =12 et µ4 =20.\n\n# données\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des données par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs éléments\ncat(\"\\nÉléments par classe :\\n\")\n\n\nÉléments par classe :\n\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res3$centers)\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nl’inertie intra classe est égale à :\n\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n[1] 0.5"
  },
  {
    "objectID": "exemples/kmeans.html#méthode-du-coude",
    "href": "exemples/kmeans.html#méthode-du-coude",
    "title": "Algorithme des K-means",
    "section": "Méthode du coude",
    "text": "Méthode du coude\nLa méthode du coude sert à déterminer le nombre optimal de clusters à retenir. Précisément, la méthode du “coude”, détermine le “k” à partir duquel l’inertie cesse de beaucoup diminuer significativement quand on augmente k.\n\n# vecteur de valeurs de k à tester\nks    &lt;- 2:4\n\n# pré-allocation d’un vecteur pour stocker l’inertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# tracé de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reliés par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"Méthode du coude pour le choix de k\")"
  },
  {
    "objectID": "presentation/historique.html",
    "href": "presentation/historique.html",
    "title": "Définition",
    "section": "",
    "text": "Voici un bref historique des grandes étapes de la classification non supervisée :\n\nAnnées 1930 – Naissance de la classification hiérarchique John Tukey et d’autres statisticiens proposent des méthodes de regroupement basées sur la similarité (dendrogrammes) pour organiser les données sans labels.\n1965 – Algorithme des nuées dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisé par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s – Modèles à mélanges et l’algorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) généralisent les modèles de mélange gaussien, entraînés via l’Expectation–Maximization, pour estimer des sous-populations au sein d’un jeu de données.\nAnnées 1980 – Cartes auto-organisatrices (SOM) Teuvo Kohonen développe les Self-Organizing Maps, réseaux de neurones non supervisés qui projettent des données de haute dimension en cartes topologiques 2D.\nAnnées 1990 – Clustering spectral et plus de flexibilité Des méthodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarité) permettent de détecter des structures non sphériques. Parallèlement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnées 2000 – Big Data et densité L’algorithme DBSCAN (1996) gagne en popularité ; en 2002, Ester et al. le précisent pour détecter les zones de forte densité dans de grands volumes de données.\nAnnées 2010 – Deep clustering L’émergence du deep learning entraîne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanément des représentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette évolution illustre comment la classification non supervisée est passée de méthodes statistiques simples (hiérarchique, k-means) à des approches sophistiquées mêlant représentations profondes et techniques de graphe."
  },
  {
    "objectID": "presentation/historique.html#dates-clés",
    "href": "presentation/historique.html#dates-clés",
    "title": "Définition",
    "section": "",
    "text": "Voici un bref historique des grandes étapes de la classification non supervisée :\n\nAnnées 1930 – Naissance de la classification hiérarchique John Tukey et d’autres statisticiens proposent des méthodes de regroupement basées sur la similarité (dendrogrammes) pour organiser les données sans labels.\n1965 – Algorithme des nuées dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularisé par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s – Modèles à mélanges et l’algorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) généralisent les modèles de mélange gaussien, entraînés via l’Expectation–Maximization, pour estimer des sous-populations au sein d’un jeu de données.\nAnnées 1980 – Cartes auto-organisatrices (SOM) Teuvo Kohonen développe les Self-Organizing Maps, réseaux de neurones non supervisés qui projettent des données de haute dimension en cartes topologiques 2D.\nAnnées 1990 – Clustering spectral et plus de flexibilité Des méthodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarité) permettent de détecter des structures non sphériques. Parallèlement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnnées 2000 – Big Data et densité L’algorithme DBSCAN (1996) gagne en popularité ; en 2002, Ester et al. le précisent pour détecter les zones de forte densité dans de grands volumes de données.\nAnnées 2010 – Deep clustering L’émergence du deep learning entraîne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultanément des représentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette évolution illustre comment la classification non supervisée est passée de méthodes statistiques simples (hiérarchique, k-means) à des approches sophistiquées mêlant représentations profondes et techniques de graphe."
  }
]