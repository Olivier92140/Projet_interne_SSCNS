[
  {
    "objectID": "presentation/historique.html",
    "href": "presentation/historique.html",
    "title": "D√©finition",
    "section": "",
    "text": "Voici un bref historique des grandes √©tapes de la classification non supervis√©e :\n\nAnn√©es 1930 ‚Äì Naissance de la classification hi√©rarchique John Tukey et d‚Äôautres statisticiens proposent des m√©thodes de regroupement bas√©es sur la similarit√© (dendrogrammes) pour organiser les donn√©es sans labels.\nApparue de la classification ascendante hi√©rarchique dans les ann√©es 1950-60 (notamment grace √† Sokal & Michener en 1958).\n1965 ‚Äì Algorithme des nu√©es dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularis√© par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s ‚Äì Mod√®les √† m√©langes et l‚Äôalgorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) g√©n√©ralisent les mod√®les de m√©lange gaussien, entra√Æn√©s via l‚ÄôExpectation‚ÄìMaximization, pour estimer des sous-populations au sein d‚Äôun jeu de donn√©es.\nAnn√©es 1980 ‚Äì Cartes auto-organisatrices (SOM) Teuvo Kohonen d√©veloppe les Self-Organizing Maps, r√©seaux de neurones non supervis√©s qui projettent des donn√©es de haute dimension en cartes topologiques 2D.\nAnn√©es 1990 ‚Äì Clustering spectral et plus de flexibilit√© Des m√©thodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarit√©) permettent de d√©tecter des structures non sph√©riques. Parall√®lement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnn√©es 2000 ‚Äì Big Data et densit√© L‚Äôalgorithme DBSCAN (1996) gagne en popularit√© ; en 2002, Ester et al.¬†le pr√©cisent pour d√©tecter les zones de forte densit√© dans de grands volumes de donn√©es.\nAnn√©es 2010 ‚Äì Deep clustering L‚Äô√©mergence du deep learning entra√Æne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultan√©ment des repr√©sentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette √©volution illustre comment la classification non supervis√©e est pass√©e de m√©thodes statistiques simples (hi√©rarchique, k-means) √† des approches sophistiqu√©es m√™lant repr√©sentations profondes et techniques de graphe."
  },
  {
    "objectID": "presentation/historique.html#dates-cl√©s",
    "href": "presentation/historique.html#dates-cl√©s",
    "title": "D√©finition",
    "section": "",
    "text": "Voici un bref historique des grandes √©tapes de la classification non supervis√©e :\n\nAnn√©es 1930 ‚Äì Naissance de la classification hi√©rarchique John Tukey et d‚Äôautres statisticiens proposent des m√©thodes de regroupement bas√©es sur la similarit√© (dendrogrammes) pour organiser les donn√©es sans labels.\nApparue de la classification ascendante hi√©rarchique dans les ann√©es 1950-60 (notamment grace √† Sokal & Michener en 1958).\n1965 ‚Äì Algorithme des nu√©es dynamiques Stuart Lloyd formalise ce qui deviendra plus tard le k-means, popularis√© par James MacQueen en 1967 : partitionner un jeu de points en k groupes en minimisant la variance intra-groupe.\n1970s ‚Äì Mod√®les √† m√©langes et l‚Äôalgorithme EM Geoffrey J. McLachlan, Thriyambakam Krishnan, et surtout Jerry Dempster, Nan Laird et Donald Rubin (1977) g√©n√©ralisent les mod√®les de m√©lange gaussien, entra√Æn√©s via l‚ÄôExpectation‚ÄìMaximization, pour estimer des sous-populations au sein d‚Äôun jeu de donn√©es.\nAnn√©es 1980 ‚Äì Cartes auto-organisatrices (SOM) Teuvo Kohonen d√©veloppe les Self-Organizing Maps, r√©seaux de neurones non supervis√©s qui projettent des donn√©es de haute dimension en cartes topologiques 2D.\nAnn√©es 1990 ‚Äì Clustering spectral et plus de flexibilit√© Des m√©thodes comme le clustering spectral (utilisant les valeurs propres du graphe de similarit√©) permettent de d√©tecter des structures non sph√©riques. Parall√®lement, on affine les distances et noyaux pour mieux capturer la forme des clusters.\nAnn√©es 2000 ‚Äì Big Data et densit√© L‚Äôalgorithme DBSCAN (1996) gagne en popularit√© ; en 2002, Ester et al.¬†le pr√©cisent pour d√©tecter les zones de forte densit√© dans de grands volumes de donn√©es.\nAnn√©es 2010 ‚Äì Deep clustering L‚Äô√©mergence du deep learning entra√Æne des approches hybrides (ex. Deep Embedded Clustering) : on apprend simultan√©ment des repr√©sentations (via auto-encodeurs) et des partitions.\n\nConclusion :\nCette √©volution illustre comment la classification non supervis√©e est pass√©e de m√©thodes statistiques simples (hi√©rarchique, k-means) √† des approches sophistiqu√©es m√™lant repr√©sentations profondes et techniques de graphe."
  },
  {
    "objectID": "exemples/kmeans.html",
    "href": "exemples/kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de donn√©es simulant les notes de 30 √©tudiants en 4 mati√®res d‚Äôune fili√®re ing√©nieur, applique le k-means pour segmenter les profils.\n\n\n\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.5), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\n\n\n\n\nIci, on centre et r√©duit chaque variable comme la dispersion des notes de chaque mati√®re est diff√©rente.Ainsi, les notes des mati√®res pourront √™tre comparables\n\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carr√©s intra-clusters\",\n     main = \"M√©thode du coude pour choisir k\")\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\nset.seed(42)\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\nK-means clustering with 3 clusters of sizes 9, 12, 9\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.8619349  0.7183503   -0.4564989 -0.6801298\n2     0.8123169 -0.4677940    0.6541109 -0.3870757\n3    -0.2211544 -0.0946250   -0.4156489  1.1962307\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  3   3   2   1   3   2   3   3   2   2   2   3   2   1   1   2   2   1   2   3 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  1   3   1   1   3   1   2   2   1   2 \n\nWithin cluster sum of squares by cluster:\n[1] 20.63932 29.27492 16.28552\n (between_SS / total_SS =  42.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$√âtudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = √âtudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils √©tudiants\",\n       subtitle = \"Projection sur les deux premi√®res composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\npca$rotation[, 1:2] \n\n                      PC1         PC2\nMathematiques  0.50381044 -0.34672112\nPhysique      -0.60778171 -0.20884255\nInformatique   0.61054070 -0.01669262\nChimie        -0.06337573 -0.91427053\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxi√®me axe factoriel s√©pare les √©tudiants bons en chimie (en haut) des plus faibles en chimie (en bas)"
  },
  {
    "objectID": "exemples/kmeans.html#exemple",
    "href": "exemples/kmeans.html#exemple",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "Voici un exemple complet qui prend un jeu de donn√©es simulant les notes de 30 √©tudiants en 4 mati√®res d‚Äôune fili√®re ing√©nieur, applique le k-means pour segmenter les profils.\n\n\n\nset.seed(123)\nn_students &lt;- 30\nnotes &lt;- data.frame(\n  Etudiant = paste0(\"E\", sprintf(\"%02d\", 1:n_students)),\n  Mathematiques   = round(rnorm(n_students, mean = 14, sd = 2), 1),\n  Physique        = round(rnorm(n_students, mean = 12, sd = 3), 1),\n  Informatique    = round(rnorm(n_students, mean = 15, sd = 2.5), 1),\n  Chimie          = round(rnorm(n_students, mean = 11, sd = 3), 1)\n)\nrownames(notes) &lt;- notes$Etudiant\nnotes$Etudiant &lt;- NULL\n\n\n\n\nIci, on centre et r√©duit chaque variable comme la dispersion des notes de chaque mati√®re est diff√©rente.Ainsi, les notes des mati√®res pourront √™tre comparables\n\nnotes_scaled &lt;- scale(notes)\n\n\n\n\n\nwss &lt;- numeric(10)\nfor (k in 1:10) {\n  km &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\n  wss[k] &lt;- km$tot.withinss\n}\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Nombre de clusters k\",\n     ylab = \"Somme des carr√©s intra-clusters\",\n     main = \"M√©thode du coude pour choisir k\")\n\n\n\n\n\n\n\n\nOn choisit ainsi k=3 donc 3 clusters.\n\n\n\n\nset.seed(42)\nk &lt;- 3\nkm_res &lt;- kmeans(notes_scaled, centers = k, nstart = 25)\nprint(km_res)\n\nK-means clustering with 3 clusters of sizes 9, 12, 9\n\nCluster means:\n  Mathematiques   Physique Informatique     Chimie\n1    -0.8619349  0.7183503   -0.4564989 -0.6801298\n2     0.8123169 -0.4677940    0.6541109 -0.3870757\n3    -0.2211544 -0.0946250   -0.4156489  1.1962307\n\nClustering vector:\nE01 E02 E03 E04 E05 E06 E07 E08 E09 E10 E11 E12 E13 E14 E15 E16 E17 E18 E19 E20 \n  3   3   2   1   3   2   3   3   2   2   2   3   2   1   1   2   2   1   2   3 \nE21 E22 E23 E24 E25 E26 E27 E28 E29 E30 \n  1   3   1   1   3   1   2   2   1   2 \n\nWithin cluster sum of squares by cluster:\n[1] 20.63932 29.27492 16.28552\n (between_SS / total_SS =  42.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n\n\nnotes_cluster &lt;- cbind(notes, Cluster = factor(km_res$cluster))\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n\n# Calcul des composantes principales\npca &lt;- prcomp(notes_scaled)\nscores &lt;- as.data.frame(pca$x[, 1:2])\nscores$Cluster &lt;- factor(km_res$cluster)\nscores$√âtudiant &lt;- rownames(scores)\n\nggplot(scores, aes(x = PC1, y = PC2, color = Cluster, label = √âtudiant)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.5, size = 3) +\n  labs(title = \"Clustering k-means des profils √©tudiants\",\n       subtitle = \"Projection sur les deux premi√®res composantes principales\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\npca$rotation[, 1:2] \n\n                      PC1         PC2\nMathematiques  0.50381044 -0.34672112\nPhysique      -0.60778171 -0.20884255\nInformatique   0.61054070 -0.01669262\nChimie        -0.06337573 -0.91427053\n\n\nLe premier axe factoriel oppose profils maths/info vs physique et le deuxi√®me axe factoriel s√©pare les √©tudiants bons en chimie (en haut) des plus faibles en chimie (en bas)"
  },
  {
    "objectID": "exemples/kmeans.html#conclusion",
    "href": "exemples/kmeans.html#conclusion",
    "title": "Algorithme des K-means",
    "section": "Conclusion",
    "text": "Conclusion\nLes clusters issus de k-means se retrouvent bien s√©par√©s :\n\ncluster (rouge) : Ces √©tudiants sont bons en physique.\ncluster (vert) : Ces √©tudiants sont forts en Maths et informatique\ncluster (bleu) : Ces √©tudiants ont des performances en chimie."
  },
  {
    "objectID": "exemples/CAH.html",
    "href": "exemples/CAH.html",
    "title": "Algorithme CAH",
    "section": "",
    "text": "Les donn√©es repr√©sentent les d√©penses de clients dans un magasin selon 3 cat√©gories: alimentaire, habillement et √©lectronique.\nEnjeu : On souhaite identifier des typologies de clients pr√©sentant des comportements similaires pour adapter l‚Äôoffre √† chaque poste de d√©penses identifi√©.\nProbl√©matique : Comment regrouper de mani√®re homog√®ne une client√®le selon ses habitudes de consommation dans trois domaines : l‚Äôalimentaire, l‚Äôhabillement, et l‚Äô√©lectronique ?\n\n\n\ndata &lt;- read.delim2(\"clients_depenses.csv\", sep = \"\\t\", stringsAsFactors = FALSE)\ndata\n\n      Client Alimentaire Habillement √âlectronique\n1   Client_1      539.73      373.28      1110.77\n2   Client_2      488.93      288.71      1025.71\n3   Client_3      551.81      303.37       982.65\n4   Client_4      621.84      228.76       954.83\n5   Client_5      481.26      272.78       778.22\n6   Client_6      481.26      305.54       892.02\n7   Client_7      626.33      242.45       930.90\n8   Client_8      561.39      318.78      1158.57\n9   Client_9      462.44      269.96      1051.54\n10 Client_10      543.40      285.41       735.54\n11 Client_11      462.92      269.91      1048.61\n12 Client_12      462.74      392.61       942.24\n13 Client_13      519.35      299.32       898.46\n14 Client_14      346.93      247.11      1091.75\n15 Client_15      362.14      341.12      1154.65\n16 Client_16      455.01      238.95      1139.69\n17 Client_17      418.97      310.44       874.12\n18 Client_18      525.13      202.01       953.62\n19 Client_19      427.35      233.59      1049.69\n20 Client_20      387.01      309.84      1146.33\n\n\n\n\n\n\n# Extraction des donn√©es num√©riques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la m√©thode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n# D√©coupage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de d√©part\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n# √âtape 1 : R√©aliser l'ACP sur les colonnes num√©riques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# √âtape 2 : Extraire les coordonn√©es sur les deux premi√®res composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n                    PC1        PC2\nAlimentaire  -0.6615989  0.1989097\nHabillement   0.3955003  0.9117238\n√âlectronique  0.6370765 -0.3594365"
  },
  {
    "objectID": "exemples/CAH.html#exemple",
    "href": "exemples/CAH.html#exemple",
    "title": "Algorithme CAH",
    "section": "",
    "text": "Les donn√©es repr√©sentent les d√©penses de clients dans un magasin selon 3 cat√©gories: alimentaire, habillement et √©lectronique.\nEnjeu : On souhaite identifier des typologies de clients pr√©sentant des comportements similaires pour adapter l‚Äôoffre √† chaque poste de d√©penses identifi√©.\nProbl√©matique : Comment regrouper de mani√®re homog√®ne une client√®le selon ses habitudes de consommation dans trois domaines : l‚Äôalimentaire, l‚Äôhabillement, et l‚Äô√©lectronique ?\n\n\n\ndata &lt;- read.delim2(\"clients_depenses.csv\", sep = \"\\t\", stringsAsFactors = FALSE)\ndata\n\n      Client Alimentaire Habillement √âlectronique\n1   Client_1      539.73      373.28      1110.77\n2   Client_2      488.93      288.71      1025.71\n3   Client_3      551.81      303.37       982.65\n4   Client_4      621.84      228.76       954.83\n5   Client_5      481.26      272.78       778.22\n6   Client_6      481.26      305.54       892.02\n7   Client_7      626.33      242.45       930.90\n8   Client_8      561.39      318.78      1158.57\n9   Client_9      462.44      269.96      1051.54\n10 Client_10      543.40      285.41       735.54\n11 Client_11      462.92      269.91      1048.61\n12 Client_12      462.74      392.61       942.24\n13 Client_13      519.35      299.32       898.46\n14 Client_14      346.93      247.11      1091.75\n15 Client_15      362.14      341.12      1154.65\n16 Client_16      455.01      238.95      1139.69\n17 Client_17      418.97      310.44       874.12\n18 Client_18      525.13      202.01       953.62\n19 Client_19      427.35      233.59      1049.69\n20 Client_20      387.01      309.84      1146.33\n\n\n\n\n\n\n# Extraction des donn√©es num√©riques uniquement\nX &lt;- data[, -1]\n\n\n\n\n\n# Matrice des distances\nd &lt;- dist(X) #distance = plus les individus sont proches plus la distance est faible\n\n# CAH avec la m√©thode de Ward\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n\n\n\n\n# Dendrogramme avec ligne rouge pour visualiser la coupure\nplot(hc, labels = data$Client, main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Client\", ylab = \"Distance\")\nabline(h = 300, col = \"red\", lty = 2) \n\n\n\n\n\n\n\n\nla ligne rouge coupe la plus grande distance verticale en 3 branches. Donc le nombre optimal de clusters est 3.\n\n\n\n\n# D√©coupage en 3 groupes (selon la plus grande distance verticale)\nclusters &lt;- cutree(hc, k = 3)\n\n# Ajouter les clusters au tableau de d√©part\ndata$Cluster &lt;- factor(clusters)\n\n\n\n\n\ninstall.packages(\"ggplot2\")\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n# √âtape 1 : R√©aliser l'ACP sur les colonnes num√©riques (sans Client ni Cluster)\nacp &lt;- prcomp(X, scale. = TRUE)\n\n# √âtape 2 : Extraire les coordonn√©es sur les deux premi√®res composantes\ncoord &lt;- as.data.frame(acp$x[, 1:2])\ncolnames(coord) &lt;- c(\"PC1\", \"PC2\")\ncoord$Client &lt;- data$Client\ncoord$Cluster &lt;- factor(data$Cluster, labels = c(\"1\", \"2\", \"3\"))\n\nggplot(coord, aes(x = PC1, y = PC2, color = Cluster, label = Client)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -0.6, size = 3) +\n  labs(title = \"ACP - Visualisation des clusters (CAH)\",\n       x = \"Composante principale 1\", y = \"Composante principale 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nacp$rotation[, 1:2]  # colonnes : PC1 et PC2, lignes : variables\n\n                    PC1        PC2\nAlimentaire  -0.6615989  0.1989097\nHabillement   0.3955003  0.9117238\n√âlectronique  0.6370765 -0.3594365"
  },
  {
    "objectID": "exemples/CAH.html#conclusion",
    "href": "exemples/CAH.html#conclusion",
    "title": "Algorithme CAH",
    "section": "Conclusion",
    "text": "Conclusion\nLa CAH permet de regrouper les 20 clients en 3 segments de consommation distincts. Ces clusters peuvent correspondre √† :\n\nCluster 1 (rouge) : Ces clients correspondent √† des gros consommateurs en √©lectronique. Pour une entreprise, il est int√©ressant de cibler de type de client pour leur proposer des offres int√©ressantes ou des nouveaut√©s.\nCluster 2 (vert) : Ce sont des clients orient√©s alimentaire et l‚Äôhabillement, peu consommateurs d‚Äô√©lectronique. Pour une entreprise, il est int√©ressant de cibler de type de client pour leur proposer par exemple des cartes de fid√©lit√©.\nCluster 3 (bleu) : Ce sont des clients √† fort int√©r√™t pour l‚Äôhabillement, avec des comportements mixtes sur les autres postes. Pour une entreprise, il est int√©ressant de cibler de type de client pour leur proposer des promotions ou des nouvelles gammes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Classification non supervis√©e",
    "section": "",
    "text": "Clustering K-Means dans R\nClassification ascendante hi√©rarchique avec R"
  },
  {
    "objectID": "index.html#des-liens-int√©ressants",
    "href": "index.html#des-liens-int√©ressants",
    "title": "Classification non supervis√©e",
    "section": "",
    "text": "Clustering K-Means dans R\nClassification ascendante hi√©rarchique avec R"
  },
  {
    "objectID": "modeles/mod_CAH.html",
    "href": "modeles/mod_CAH.html",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "",
    "text": "La classification hi√©rarchique ascendante est une m√©thode de partitionnement dont l‚Äôobjectif est de construire une hi√©rarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant it√©rativement les groupes deux √† deux."
  },
  {
    "objectID": "modeles/mod_CAH.html#introduction",
    "href": "modeles/mod_CAH.html#introduction",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "",
    "text": "La classification hi√©rarchique ascendante est une m√©thode de partitionnement dont l‚Äôobjectif est de construire une hi√©rarchie sur un ensemble de n observations, en partant de n singletons puis en fusionnant it√©rativement les groupes deux √† deux."
  },
  {
    "objectID": "modeles/mod_CAH.html#principe-de-la-m√©thode",
    "href": "modeles/mod_CAH.html#principe-de-la-m√©thode",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Principe de la m√©thode",
    "text": "Principe de la m√©thode\nSoit x = {x‚ÇÅ, ‚Ä¶, x‚Çô} l‚Äôensemble d‚Äôobservations √† classer :\n\n√âtape initiale\nLes n individus x constituent chacun une classe singleton.\nFusion des plus proches\nOn calcule la distance deux √† deux entre tous les individus, puis on r√©unit dans la m√™me classe les deux individus les plus proches.\nIt√©ration\nOn recalcule la distance entre la nouvelle classe form√©e et les n‚Äì2 individus restants.\nOn fusionne √† nouveau les deux √©l√©ments (classes ou individus) les plus proches.\nCe processus est it√©r√© jusqu‚Äô√† ce qu‚Äôil ne reste plus qu‚Äôune unique classe: x.\n\nRemarque :\nCrit√®res d‚Äôagr√©gation\nPour pouvoir regrouper des parties de ‚Ñ¶ :\n\nDissimilarit√© entre observations\nd(xi,xj) ‚àà R+, xi,xj ‚àà x,\nDissimilarit√© entre classes\nD(A,B) ‚àà R+, o√π A,B ‚àà P(x) avec A‚à©B =‚àÖ."
  },
  {
    "objectID": "modeles/mod_CAH.html#algorithme-de-cah",
    "href": "modeles/mod_CAH.html#algorithme-de-cah",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Algorithme de CAH",
    "text": "Algorithme de CAH\nData : x‚ÇÅ, ‚Ä¶, x‚Çô\nInitialisation : d√©finir la partition C‚ÅΩ‚Å∞‚Åæ constitu√©e des singletons\nfor m = 1, ‚Ä¶, K‚Äì1 do\n- Calculer les distances deux √† deux entre les classes de la partition C‚ÅΩ·µê‚Åª¬π‚Åæ √† l‚Äôaide de D\n- Former la partition C‚ÅΩ·µê‚Åæ en regroupant les deux classes les plus proches selon D\nend\nResult : Hi√©rarchie indic√©e\nL‚Äôalgorithme de la CAH fournit une hi√©rarchie indic√©e o√π D est l‚Äôindice si D est croissante."
  },
  {
    "objectID": "modeles/mod_CAH.html#distance-ou-dissimilarit√©-entre-les-classes",
    "href": "modeles/mod_CAH.html#distance-ou-dissimilarit√©-entre-les-classes",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Distance (ou dissimilarit√©) entre les classes",
    "text": "Distance (ou dissimilarit√©) entre les classes\nSoit A et B deux classes de ‚Ñ¶, on a les crit√®res d‚Äôagr√©gation (linkages) suivants :\n\nCrit√®re du saut minimum (single linkage)\n\\(D_m\\)(A,B) = min{d(x,y); x ‚àà A, y ‚àà B}.\nCrit√®re du saut maximum (complete linkage)\n\\(D_M\\)(A,B) = max{d(x,y); x ‚àà A, y ‚àà B}.\nCrit√®re de la distance moyenne (average linkage)\n\\(D_a(A,B) = \\frac{\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)}{n_A\\,n_B}\\)\no√π \\(n_A\\)=card(A) et \\(n_B\\)=card(B)\nCrit√®re de Ward (Ward linkage)\n\\(D_W(A, B) = \\frac{n_A\\,n_B}{n_A + n_B}\\,d^2(\\mu_A, \\mu_B)\\)\no√π \\(¬µ_A=\\frac{\\sum_{x‚ààA}x}{n_A}\\) et \\(¬µ_B=\\frac{\\sum_{y‚ààB}y}{n_B}\\)"
  },
  {
    "objectID": "modeles/mod_CAH.html#dendrogramme",
    "href": "modeles/mod_CAH.html#dendrogramme",
    "title": "Classification Ascendante Hi√©rarchique (CAH)",
    "section": "Dendrogramme",
    "text": "Dendrogramme\nUn dendrogramme est un type de diagramme arborescent utilis√© pour repr√©senter visuellement le r√©sultat d‚Äôune analyse de regroupement hi√©rarchique (clustering hi√©rarchique).\nLes branches repr√©sentent les fusions successives de groupes d‚Äôobjets selon leur similarit√© ou leur distance.\n\nR√®gle de la plus grande distance verticale\nLa r√®gle de la plus grande distance verticale (aussi appel√©e ‚Äúr√®gle de l‚Äô√©lagage maximal‚Äù) est une m√©thode visuelle simple pour d√©terminer le nombre optimal de clusters √† partir d‚Äôun dendrogramme.\nEtapes de la r√®gle :\n\nTracer le dendrogramme.\nRep√©rer les hauteurs des lignes verticales.\nIdentifier la plus grande distance verticale sans ligne horizontale qui la coupe : C‚Äôest-√†-dire la plus grande ‚Äúbarre verticale libre‚Äù.\nTracer une ligne horizontale qui coupe cette barre.\nLe nombre optimal de clusters est alors √©gal au nombre de branches coup√©es par cette ligne horizontale.\n\n\n\navantages, limites et sp√©cificit√© de l‚Äôalgorithme CAH\n\navantages : pas besoin de sp√©cifier K √† l‚Äôavance, visualisation hi√©rarchique.\nlimites: complexit√© quadratique, sensible √† l‚Äô√©chelle des variables.\nsp√©cificit√© : Structure arborescente\n\n\n\nApplication directe\n\n# √âtape 1 : Donn√©es originales\ndata &lt;- data.frame(\n  Employ√©    = c(\"E1\", \"E2\", \"E3\", \"E4\", \"E5\"),\n  Anciennet√© = c(2, 3, 5, 6, 8),\n  Salaire    = c(2000, 2100, 3500, 4100, 10000)\n)\n\n# √âtape 2 : Matrice des caract√©ristiques\nX &lt;- data[, c(\"Anciennet√©\", \"Salaire\")]\n\n# √âtape 3 : Clustering hi√©rarchique (Ward)\nd &lt;- dist(X)\nhc &lt;- hclust(d, method = \"ward.D2\")\n\n# √âtape 4 : Affichage du dendrogramme avec ylim plus grand\nplot(hc, labels = data$Employ√©,\n     main = \"Dendrogramme - CAH (Ward)\",\n     xlab = \"Employ√©\", ylab = \"Distance\",\n     ylim = c(0, 10000))  # Limite sup√©rieure augment√©e\n\n# Hauteur de coupure\nn_clusters &lt;- 2\nheight_cut &lt;- (sort(hc$height, decreasing = TRUE)[n_clusters - 1] +\n               sort(hc$height, decreasing = TRUE)[n_clusters]) / 1.35\nabline(h = height_cut, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n# √âtape 5 : Clusters\nclusters &lt;- cutree(hc, k = n_clusters)\ndata$Cluster &lt;- as.factor(clusters)\n\n# Affichage final\nprint(data)\n\n  Employ√© Anciennet√© Salaire Cluster\n1      E1          2    2000       1\n2      E2          3    2100       1\n3      E3          5    3500       1\n4      E4          6    4100       1\n5      E5          8   10000       2"
  },
  {
    "objectID": "modeles/mod_kmeans.html",
    "href": "modeles/mod_kmeans.html",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L‚Äôalgorithme des K-means appel√©es aussi m√©thode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On consid√®re que les observations x= {ùë•‚ÇÅ, .., ùë•‚Çô} sont d√©crites par p variables (ùë•‚Çô appartient √† ‚Ñù·µñ) et que l‚Äôespace est munie est de la m√©trique (g√©n√©ralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le m√™me poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#introduction",
    "href": "modeles/mod_kmeans.html#introduction",
    "title": "Algorithme des K-means",
    "section": "",
    "text": "L‚Äôalgorithme des K-means appel√©es aussi m√©thode des centres mobiles (ou algorithme de Lloyd) a pour but de fournir une partition. On consid√®re que les observations x= {ùë•‚ÇÅ, .., ùë•‚Çô} sont d√©crites par p variables (ùë•‚Çô appartient √† ‚Ñù·µñ) et que l‚Äôespace est munie est de la m√©trique (g√©n√©ralement distance euclidienne).Pour simplifier les notations,on supposera que chaque observation a le m√™me poids."
  },
  {
    "objectID": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "href": "modeles/mod_kmeans.html#algorithme-de-lloyd-k-means",
    "title": "Algorithme des K-means",
    "section": "Algorithme de Lloyd (k-means)",
    "text": "Algorithme de Lloyd (k-means)\n\nAlgorithme des k-means\nLa m√©thode des centres mobiles a pour but de fournir une partition de x= {ùë•‚ÇÅ, .., ùë•‚Çô}.\n\nPrincipe : Pour l‚Äôensemble d‚Äôobservations ùë• = {ùë•‚ÇÅ, .., ùë•‚Çô}, on suppose qu‚Äôil existe\nùê∂·¥∑ = {ùê∂‚ÇÅ, .., ùê∂‚Çñ} tel que ùë• = ‚ãÉ‚Çñ‚Çå‚ÇÅ·¥∑ ùê∂‚Çñ est disjoint\n\n\nEtape 1. On commence par choisir K observations diff√©rentes : {Œº‚ÇÅ, .., Œº‚Çñ}\nEtape 2. On r√©alise it√©rativement une succession ces actions:\n\nPour chaque observation x= {ùë•‚ÇÅ, .., ùë•‚Çô} on trouve son centre (barycentre) le plus proche pour cr√©er ùê∂‚Çñ = {ensemble d‚Äôobservations les plus proches du centre Œº‚Çñ}\nDans chaque nouvelle classe ùê∂‚Çñ on d√©finit le nouveau centre de classe Œº‚Çñ comme √©tant le barycentre de ùê∂‚Çñ.\n\nEtape 3. on r√©it√®re l‚Äô√©tape 1.\n\n\n\nRemarques\n\nL‚Äôalgorithme s‚Äôarr√™te lorsque la partition des classes ne change plus (crit√®re de convergence)\nl‚Äôalgorithme ne s‚Äôapplique que sur des donn√©es quantitatives\nsi les barycentres sont donn√©s, on commence l‚Äôalgorithme avec ces barycentres.\n\n\n\navantage, limite et sp√©cificit√© de l‚Äôalgorithme\n\navantage : algorithme rapide\nlimites: besoin de sp√©cifier le nombre de classes K\nsp√©cificit√© : minimisation de l‚Äôinertie intra-classe"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-1",
    "href": "modeles/mod_kmeans.html#exemple-1",
    "title": "Algorithme des K-means",
    "section": "Exemple 1",
    "text": "Exemple 1\nVoici un exemple de script R utilisant l‚Äôalgorithme k-means pour trouver les classes (les clusters) des donn√©es x={1, 2, 9, 12, 20} avec les barycentres ¬µ‚ÇÅ=1 et Œº‚ÇÇ=7\nIt√©ration 1\nEtape 1 : ¬µ‚ÇÅ=1 et Œº‚ÇÇ=7 sont donn√©s par hypoth√®se\nEtape 2 : Commen√ßons par calculer la distance de chaque √©l√©ment de x par rapport √† ¬µ‚ÇÅ=1 et Œº‚ÇÇ=7\n\n# Fonction calculant les distances √† un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur num√©rique\n  # mu : valeur num√©rique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu1 &lt;- 1\nmu2 &lt;- 7\ndistances1 &lt;- distance_au_centre(x, mu1)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu2)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu1\", \"_mu2\"))\nprint(df_merged)\n\n  points distance_mu1 distance_mu2\n1      1            0            6\n2      2            1            5\n3      9            8            2\n4     12           11            5\n5     20           19           13\n\n\nEtape 2 : les classes sont telles que la distance d(x,¬µ‚ÇÅ) d(x,Œº‚ÇÇ) soit minimale. Donc les classes sont {1,2} et {9,12,20}.\nIt√©ration 2  :\nEtape 1 : on d√©termine les centres de chaque classe\n\nmu3 &lt;- mean(c(1, 2))\nmu4 &lt;- mean(c(9, 12, 20))\nprint(mu3)\n\n[1] 1.5\n\nprint(mu4)\n\n[1] 13.66667\n\n\nEtape 2 : Commen√ßons par calculer la distance de chaque √©l√©ment de x par rapport √† ¬µ‚ÇÅ=1,5 et Œº‚ÇÇ=13,67\n\n# Fonction calculant les distances √† un barycentre\ndistance_au_centre &lt;- function(x, mu) {\n  # x  : vecteur num√©rique\n  # mu : valeur num√©rique du barycentre\n  abs(x - mu)\n}\nx   &lt;- c(1, 2, 9, 12, 20)\nmu3 &lt;-1.5\nmu4 &lt;-13.67\n\ndistances1 &lt;- distance_au_centre(x, mu3)\ndf1&lt;- data.frame(points = x, distance = distances1)\ndistances2 &lt;- distance_au_centre(x, mu4)\ndf2&lt;- data.frame(points = x, distance = distances2)\ndf_merged &lt;- merge(df1, df2, by= \"points\", suffixes = c(\"_mu3\", \"_mu4\"))\nprint(df_merged)\n\n  points distance_mu3 distance_mu4\n1      1          0.5        12.67\n2      2          0.5        11.67\n3      9          7.5         4.67\n4     12         10.5         1.67\n5     20         18.5         6.33\n\n\nEtape 2 : les classes sont telles que la distance d(x,¬µ3) d(x,Œº4) soit minimale. Le regroupement ne change pas de la 1√®re √† la 2√®me it√©ration (crit√®re de convergence) donc les classes sont {1,2} et {9,12,20}.\nSous R, la fonction kmeans permet directement de r√©aliser le regroupement selon l‚Äôalgorithme des k-means.\n\n# donn√©es\nk1 &lt;- 2\nx1    &lt;- c(1, 2, 9, 12, 20)\ninit1 &lt;- matrix(c(1, 7), ncol = 1)  # centres initiaux\n\n# k-means\nres1  &lt;- kmeans(x1, centers = init1, iter.max = 10)\n\n# regroupement des donn√©es par classe\nclasses1 &lt;- split(x1, res1$cluster)\n\n# affichage des classes et de leurs √©l√©ments\ncat(\"\\n√âl√©ments par classe :\\n\")\n\n\n√âl√©ments par classe :\n\nfor (i in seq_along(classes1)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes1[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12, 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res1$centers)\n\n      [,1]\n1  1.50000\n2 13.66667\n\n\nl‚Äôinertie intra classe est √©gale √† :\n\ntotal_inertia_alt1 &lt;- sum(res1$withinss)\nprint(total_inertia_alt1)\n\n[1] 65.16667"
  },
  {
    "objectID": "modeles/mod_kmeans.html#exemple-2",
    "href": "modeles/mod_kmeans.html#exemple-2",
    "title": "Algorithme des K-means",
    "section": "Exemple 2",
    "text": "Exemple 2\nVoici un exemple de script R utilisant l‚Äôalgorithme k-means pour trouver les classes (les clusters) des donn√©es x={1, 2, 9, 12, 20} avec les barycentres ¬µ1 =1, ¬µ2 =12 et ¬µ3 =20\n\n# donn√©es\nk2 &lt;- 3\nx2 &lt;- c(1, 2, 9, 12, 20)\ninit2 &lt;- matrix(c(1, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres2  &lt;- kmeans(x2, centers = init2, iter.max = 10)\n\n# regroupement des donn√©es par classe\nclasses2 &lt;- split(x2, res2$cluster)\n\n# affichage des classes et de leurs √©l√©ments\ncat(\"\\n√âl√©ments par classe :\\n\")\n\n\n√âl√©ments par classe :\n\nfor (i in seq_along(classes2)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes2[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9, 12\nClasse 3 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res2$centers)\n\n  [,1]\n1  1.5\n2 10.5\n3 20.0"
  },
  {
    "objectID": "modeles/mod_kmeans.html#inertie-intra-classe",
    "href": "modeles/mod_kmeans.html#inertie-intra-classe",
    "title": "Algorithme des K-means",
    "section": "Inertie intra-classe",
    "text": "Inertie intra-classe\n\nD√©finition\n\nL‚Äôinertie intra-classe de x = {x1,..,xn} par rapport √† la partition \\[C_K = \\{\\,C_1,\\,C_2,\\,\\dots,\\,C_K\\} \\] et les centres des classes dans ¬µ = {¬µ1,..,¬µK} s‚Äô√©crit: IW =\\(\\sum_{k=1}^K d¬≤(xi, ¬µk)\\) Notons-la IW(C,¬µ) √† partir de maintenant.\n\nl‚Äôinertie intra classe est √©gale √† :\n\ntotal_inertia_alt2 &lt;- sum(res2$withinss)\nprint(total_inertia_alt2)\n\n[1] 5\n\n\n\n\nPropri√©t√©s\n\nL‚Äôalgorithme de Lloyd permet de diminuer l‚Äôinertie intra-classe IW √† chaque it√©ration. Ainsi : \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)}) \\ge \\mathrm{IW}(C^{(m+1)}, \\mu^{(m)})\\)\nLa suite num√©rique \\(\\mathrm{IW}(C^{(m)}, \\mu^{(m)})\\) est stationnaire(i.e.,√† partir d‚Äôun certain nombre d‚Äôit√©rations,elle prend toujours la m√™me valeur).\n\nVoici un exemple de script R utilisant l‚Äôalgorithme k-means pour trouver les classes (les clusters) des donn√©es x={1, 2, 9, 12, 20} avec les barycentres ¬µ1 =1, ¬µ2 =9, ¬µ3 =12 et ¬µ4 =20.\n\n# donn√©es\nx3 &lt;- c(1, 2, 9, 12, 20)\ninit3 &lt;- matrix(c(1, 9, 12, 20), ncol = 1)  # centres initiaux\n\n# k-means\nres3 &lt;- kmeans(x3, centers = init3, iter.max = 10)\n\n# regroupement des donn√©es par classe\nclasses3 &lt;- split(x3, res3$cluster)\n\n# affichage des classes et de leurs √©l√©ments\ncat(\"\\n√âl√©ments par classe :\\n\")\n\n\n√âl√©ments par classe :\n\nfor (i in seq_along(classes3)) {\n  cat(sprintf(\"Classe %d : %s\\n\", i, paste(classes3[[i]], collapse = \", \")))\n}\n\nClasse 1 : 1, 2\nClasse 2 : 9\nClasse 3 : 12\nClasse 4 : 20\n\n# affichage des centres finaux\ncat(\"Centres finaux :\\n\")\n\nCentres finaux :\n\nprint(res3$centers)\n\n  [,1]\n1  1.5\n2  9.0\n3 12.0\n4 20.0\n\n\nl‚Äôinertie intra classe est √©gale √† :\n\ntotal_inertia_alt3 &lt;- sum(res3$withinss)\nprint(total_inertia_alt3)\n\n[1] 0.5"
  },
  {
    "objectID": "modeles/mod_kmeans.html#m√©thode-du-coude",
    "href": "modeles/mod_kmeans.html#m√©thode-du-coude",
    "title": "Algorithme des K-means",
    "section": "M√©thode du coude",
    "text": "M√©thode du coude\nLe crit√®re du coude est une m√©thode visuelle consistant √† tracer l‚Äôinertie intra de k-means en fonction de k et √† choisir le nombre de clusters correspondant au point d‚Äôinflexion de la courbe.\n\n# vecteur de valeurs de k √† tester\nks    &lt;- 2:4\n\n# pr√©-allocation d‚Äôun vecteur pour stocker l‚Äôinertie totale\ninertias &lt;- c(total_inertia_alt1, total_inertia_alt2, total_inertia_alt3)\n\n# trac√© de la courbe inertie vs k\nplot(ks, inertias,\n     type = \"b\",                             # points reli√©s par des segments\n     pch  = 19,                              # symbole plein pour les points\n     xlab = \"Nombre de clusters (k)\",\n     ylab = \"Inertie intra-classe totale\",\n     main = \"M√©thode du coude pour le choix de k\")\n\n\n\n\n\n\n\n\nConclusion : La m√©thode du coude privil√©gie donc 3 clusters qui permettent de capturer la plus grande partie de l‚Äôinertie intra-classe. En effet, ajouter une 4√®me classe, n‚Äôapporte qu‚Äôun faible gain d‚Äôinertie intra-classe."
  },
  {
    "objectID": "presentation/definition.html",
    "href": "presentation/definition.html",
    "title": "D√©finition",
    "section": "",
    "text": "Classification ¬´ supervis√©e ¬ª signifie une classification guid√©e par des √©tiquettes (ou ‚Äúsuperviseur‚Äù) qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d‚Äôun magasin en clients ‚Äúfid√®les‚Äù ou ‚Äúoccasionnels‚Äù.\nAinsi, une classification non supervis√©e est une classification non guid√©e et o√π l‚Äôalgorithme d√©couvre lui-m√™me la structure des donn√©es et d√©termine lui-m√™me le regroupement, sans utiliser de ‚Äúsuperviseur‚Äù. Exemple : √©tablir une classification des clients (ou clustering) selon les montants d√©pens√©s dans un magasin sans pr√©ciser √† l‚Äôalgorithme si les clients fid√®les ou occasionnels."
  },
  {
    "objectID": "presentation/definition.html#introduction",
    "href": "presentation/definition.html#introduction",
    "title": "D√©finition",
    "section": "",
    "text": "Classification ¬´ supervis√©e ¬ª signifie une classification guid√©e par des √©tiquettes (ou ‚Äúsuperviseur‚Äù) qui indiquent selon quel groupe la classification doit se faire. Exemple : classer les clients d‚Äôun magasin en clients ‚Äúfid√®les‚Äù ou ‚Äúoccasionnels‚Äù.\nAinsi, une classification non supervis√©e est une classification non guid√©e et o√π l‚Äôalgorithme d√©couvre lui-m√™me la structure des donn√©es et d√©termine lui-m√™me le regroupement, sans utiliser de ‚Äúsuperviseur‚Äù. Exemple : √©tablir une classification des clients (ou clustering) selon les montants d√©pens√©s dans un magasin sans pr√©ciser √† l‚Äôalgorithme si les clients fid√®les ou occasionnels."
  }
]